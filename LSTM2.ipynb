{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YmBVDrCpw2CL",
        "outputId": "7a83ccb8-eee8-41eb-a503-a2bf89b26782"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EXPÉRIENCE FEDERATED LEARNING AVEC APPROCHE EN DEUX PHASES\n",
            "================================================================================\n",
            "=== Lancement du Federated Learning en deux phases avec clusterisation locale ===\n",
            "Génération des données synthétiques...\n",
            "\n",
            "=== Cycle de communication 1/3 ===\n",
            "Phase 1: Initialisation avec FedAvg et création des centres de clusters\n",
            "Entraînement du client 1/3\n",
            "Époque 1/50\n",
            "Loss: 1.6304\n",
            "Époque 2/50\n",
            "Loss: 1.6121\n",
            "Époque 3/50\n",
            "Loss: 1.5962\n",
            "Époque 4/50\n",
            "Loss: 1.5820\n",
            "Époque 5/50\n",
            "Loss: 1.5691\n",
            "Époque 6/50\n",
            "Loss: 1.5570\n",
            "Époque 7/50\n",
            "Loss: 1.5455\n",
            "Époque 8/50\n",
            "Loss: 1.5343\n",
            "Époque 9/50\n",
            "Loss: 1.5230\n",
            "Époque 10/50\n",
            "Loss: 1.5113\n",
            "Époque 11/50\n",
            "Loss: 1.4991\n",
            "Époque 12/50\n",
            "Loss: 1.4860\n",
            "Époque 13/50\n",
            "Loss: 1.4721\n",
            "Époque 14/50\n",
            "Loss: 1.4569\n",
            "Époque 15/50\n",
            "Loss: 1.4403\n",
            "Époque 16/50\n",
            "Loss: 1.4224\n",
            "Époque 17/50\n",
            "Loss: 1.4033\n",
            "Époque 18/50\n",
            "Loss: 1.3832\n",
            "Époque 19/50\n",
            "Loss: 1.3620\n",
            "Époque 20/50\n",
            "Loss: 1.3397\n",
            "Époque 21/50\n",
            "Loss: 1.3161\n",
            "Époque 22/50\n",
            "Loss: 1.2912\n",
            "Époque 23/50\n",
            "Loss: 1.2652\n",
            "Époque 24/50\n",
            "Loss: 1.2380\n",
            "Époque 25/50\n",
            "Loss: 1.2097\n",
            "Époque 26/50\n",
            "Loss: 1.1804\n",
            "Époque 27/50\n",
            "Loss: 1.1503\n",
            "Époque 28/50\n",
            "Loss: 1.1197\n",
            "Époque 29/50\n",
            "Loss: 1.0888\n",
            "Époque 30/50\n",
            "Loss: 1.0573\n",
            "Époque 31/50\n",
            "Loss: 1.0251\n",
            "Époque 32/50\n",
            "Loss: 0.9922\n",
            "Époque 33/50\n",
            "Loss: 0.9592\n",
            "Époque 34/50\n",
            "Loss: 0.9261\n",
            "Époque 35/50\n",
            "Loss: 0.8928\n",
            "Époque 36/50\n",
            "Loss: 0.8594\n",
            "Époque 37/50\n",
            "Loss: 0.8262\n",
            "Époque 38/50\n",
            "Loss: 0.7935\n",
            "Époque 39/50\n",
            "Loss: 0.7612\n",
            "Époque 40/50\n",
            "Loss: 0.7296\n",
            "Époque 41/50\n",
            "Loss: 0.6983\n",
            "Époque 42/50\n",
            "Loss: 0.6672\n",
            "Époque 43/50\n",
            "Loss: 0.6365\n",
            "Époque 44/50\n",
            "Loss: 0.6064\n",
            "Époque 45/50\n",
            "Loss: 0.5768\n",
            "Époque 46/50\n",
            "Loss: 0.5480\n",
            "Époque 47/50\n",
            "Loss: 0.5200\n",
            "Époque 48/50\n",
            "Loss: 0.4928\n",
            "Époque 49/50\n",
            "Loss: 0.4666\n",
            "Époque 50/50\n",
            "Loss: 0.4414\n",
            "Époque 1/50\n",
            "Loss: 1.6304\n",
            "Époque 2/50\n",
            "Loss: 1.6121\n",
            "Époque 3/50\n",
            "Loss: 1.5962\n",
            "Époque 4/50\n",
            "Loss: 1.5820\n",
            "Époque 5/50\n",
            "Loss: 1.5691\n",
            "Époque 6/50\n",
            "Loss: 1.5570\n",
            "Époque 7/50\n",
            "Loss: 1.5455\n",
            "Époque 8/50\n",
            "Loss: 1.5343\n",
            "Époque 9/50\n",
            "Loss: 1.5230\n",
            "Époque 10/50\n",
            "Loss: 1.5113\n",
            "Époque 11/50\n",
            "Loss: 1.4991\n",
            "Époque 12/50\n",
            "Loss: 1.4860\n",
            "Époque 13/50\n",
            "Loss: 1.4721\n",
            "Époque 14/50\n",
            "Loss: 1.4569\n",
            "Époque 15/50\n",
            "Loss: 1.4403\n",
            "Époque 16/50\n",
            "Loss: 1.4224\n",
            "Époque 17/50\n",
            "Loss: 1.4033\n",
            "Époque 18/50\n",
            "Loss: 1.3832\n",
            "Époque 19/50\n",
            "Loss: 1.3620\n",
            "Époque 20/50\n",
            "Loss: 1.3397\n",
            "Époque 21/50\n",
            "Loss: 1.3161\n",
            "Époque 22/50\n",
            "Loss: 1.2912\n",
            "Époque 23/50\n",
            "Loss: 1.2652\n",
            "Époque 24/50\n",
            "Loss: 1.2380\n",
            "Époque 25/50\n",
            "Loss: 1.2097\n",
            "Époque 26/50\n",
            "Loss: 1.1804\n",
            "Époque 27/50\n",
            "Loss: 1.1503\n",
            "Époque 28/50\n",
            "Loss: 1.1197\n",
            "Époque 29/50\n",
            "Loss: 1.0888\n",
            "Époque 30/50\n",
            "Loss: 1.0573\n",
            "Époque 31/50\n",
            "Loss: 1.0251\n",
            "Époque 32/50\n",
            "Loss: 0.9922\n",
            "Époque 33/50\n",
            "Loss: 0.9592\n",
            "Époque 34/50\n",
            "Loss: 0.9261\n",
            "Époque 35/50\n",
            "Loss: 0.8928\n",
            "Époque 36/50\n",
            "Loss: 0.8594\n",
            "Époque 37/50\n",
            "Loss: 0.8262\n",
            "Époque 38/50\n",
            "Loss: 0.7935\n",
            "Époque 39/50\n",
            "Loss: 0.7612\n",
            "Époque 40/50\n",
            "Loss: 0.7296\n",
            "Époque 41/50\n",
            "Loss: 0.6983\n",
            "Époque 42/50\n",
            "Loss: 0.6672\n",
            "Époque 43/50\n",
            "Loss: 0.6365\n",
            "Époque 44/50\n",
            "Loss: 0.6064\n",
            "Époque 45/50\n",
            "Loss: 0.5768\n",
            "Époque 46/50\n",
            "Loss: 0.5480\n",
            "Époque 47/50\n",
            "Loss: 0.5200\n",
            "Époque 48/50\n",
            "Loss: 0.4928\n",
            "Époque 49/50\n",
            "Loss: 0.4666\n",
            "Époque 50/50\n",
            "Loss: 0.4414\n",
            "Époque 1/50\n",
            "Loss: 1.6304\n",
            "Époque 2/50\n",
            "Loss: 1.6121\n",
            "Époque 3/50\n",
            "Loss: 1.5962\n",
            "Époque 4/50\n",
            "Loss: 1.5820\n",
            "Époque 5/50\n",
            "Loss: 1.5691\n",
            "Époque 6/50\n",
            "Loss: 1.5570\n",
            "Époque 7/50\n",
            "Loss: 1.5455\n",
            "Époque 8/50\n",
            "Loss: 1.5343\n",
            "Époque 9/50\n",
            "Loss: 1.5230\n",
            "Époque 10/50\n",
            "Loss: 1.5113\n",
            "Époque 11/50\n",
            "Loss: 1.4991\n",
            "Époque 12/50\n",
            "Loss: 1.4860\n",
            "Époque 13/50\n",
            "Loss: 1.4721\n",
            "Époque 14/50\n",
            "Loss: 1.4569\n",
            "Époque 15/50\n",
            "Loss: 1.4403\n",
            "Époque 16/50\n",
            "Loss: 1.4224\n",
            "Époque 17/50\n",
            "Loss: 1.4033\n",
            "Époque 18/50\n",
            "Loss: 1.3832\n",
            "Époque 19/50\n",
            "Loss: 1.3620\n",
            "Époque 20/50\n",
            "Loss: 1.3397\n",
            "Époque 21/50\n",
            "Loss: 1.3161\n",
            "Époque 22/50\n",
            "Loss: 1.2912\n",
            "Époque 23/50\n",
            "Loss: 1.2652\n",
            "Époque 24/50\n",
            "Loss: 1.2380\n",
            "Époque 25/50\n",
            "Loss: 1.2097\n",
            "Époque 26/50\n",
            "Loss: 1.1804\n",
            "Époque 27/50\n",
            "Loss: 1.1503\n",
            "Époque 28/50\n",
            "Loss: 1.1197\n",
            "Époque 29/50\n",
            "Loss: 1.0888\n",
            "Époque 30/50\n",
            "Loss: 1.0573\n",
            "Époque 31/50\n",
            "Loss: 1.0251\n",
            "Époque 32/50\n",
            "Loss: 0.9922\n",
            "Époque 33/50\n",
            "Loss: 0.9592\n",
            "Époque 34/50\n",
            "Loss: 0.9261\n",
            "Époque 35/50\n",
            "Loss: 0.8928\n",
            "Époque 36/50\n",
            "Loss: 0.8594\n",
            "Époque 37/50\n",
            "Loss: 0.8262\n",
            "Époque 38/50\n",
            "Loss: 0.7935\n",
            "Époque 39/50\n",
            "Loss: 0.7612\n",
            "Époque 40/50\n",
            "Loss: 0.7296\n",
            "Époque 41/50\n",
            "Loss: 0.6983\n",
            "Époque 42/50\n",
            "Loss: 0.6672\n",
            "Époque 43/50\n",
            "Loss: 0.6365\n",
            "Époque 44/50\n",
            "Loss: 0.6064\n",
            "Époque 45/50\n",
            "Loss: 0.5768\n",
            "Époque 46/50\n",
            "Loss: 0.5480\n",
            "Époque 47/50\n",
            "Loss: 0.5200\n",
            "Époque 48/50\n",
            "Loss: 0.4928\n",
            "Époque 49/50\n",
            "Loss: 0.4666\n",
            "Époque 50/50\n",
            "Loss: 0.4414\n",
            "Époque 1/50\n",
            "Loss: 1.6304\n",
            "Époque 2/50\n",
            "Loss: 1.6121\n",
            "Époque 3/50\n",
            "Loss: 1.5962\n",
            "Époque 4/50\n",
            "Loss: 1.5820\n",
            "Époque 5/50\n",
            "Loss: 1.5691\n",
            "Époque 6/50\n",
            "Loss: 1.5570\n",
            "Époque 7/50\n",
            "Loss: 1.5455\n",
            "Époque 8/50\n",
            "Loss: 1.5343\n",
            "Époque 9/50\n",
            "Loss: 1.5230\n",
            "Époque 10/50\n",
            "Loss: 1.5113\n",
            "Époque 11/50\n",
            "Loss: 1.4991\n",
            "Époque 12/50\n",
            "Loss: 1.4860\n",
            "Époque 13/50\n",
            "Loss: 1.4721\n",
            "Époque 14/50\n",
            "Loss: 1.4569\n",
            "Époque 15/50\n",
            "Loss: 1.4403\n",
            "Époque 16/50\n",
            "Loss: 1.4224\n",
            "Époque 17/50\n",
            "Loss: 1.4033\n",
            "Époque 18/50\n",
            "Loss: 1.3832\n",
            "Époque 19/50\n",
            "Loss: 1.3620\n",
            "Époque 20/50\n",
            "Loss: 1.3397\n",
            "Époque 21/50\n",
            "Loss: 1.3161\n",
            "Époque 22/50\n",
            "Loss: 1.2912\n",
            "Époque 23/50\n",
            "Loss: 1.2652\n",
            "Époque 24/50\n",
            "Loss: 1.2380\n",
            "Époque 25/50\n",
            "Loss: 1.2097\n",
            "Époque 26/50\n",
            "Loss: 1.1804\n",
            "Époque 27/50\n",
            "Loss: 1.1503\n",
            "Époque 28/50\n",
            "Loss: 1.1197\n",
            "Époque 29/50\n",
            "Loss: 1.0888\n",
            "Époque 30/50\n",
            "Loss: 1.0573\n",
            "Époque 31/50\n",
            "Loss: 1.0251\n",
            "Époque 32/50\n",
            "Loss: 0.9922\n",
            "Époque 33/50\n",
            "Loss: 0.9592\n",
            "Époque 34/50\n",
            "Loss: 0.9261\n",
            "Époque 35/50\n",
            "Loss: 0.8928\n",
            "Époque 36/50\n",
            "Loss: 0.8594\n",
            "Époque 37/50\n",
            "Loss: 0.8262\n",
            "Époque 38/50\n",
            "Loss: 0.7935\n",
            "Époque 39/50\n",
            "Loss: 0.7612\n",
            "Époque 40/50\n",
            "Loss: 0.7296\n",
            "Époque 41/50\n",
            "Loss: 0.6983\n",
            "Époque 42/50\n",
            "Loss: 0.6672\n",
            "Époque 43/50\n",
            "Loss: 0.6365\n",
            "Époque 44/50\n",
            "Loss: 0.6064\n",
            "Époque 45/50\n",
            "Loss: 0.5768\n",
            "Époque 46/50\n",
            "Loss: 0.5480\n",
            "Époque 47/50\n",
            "Loss: 0.5200\n",
            "Époque 48/50\n",
            "Loss: 0.4928\n",
            "Époque 49/50\n",
            "Loss: 0.4666\n",
            "Époque 50/50\n",
            "Loss: 0.4414\n",
            "Époque 1/50\n",
            "Loss: 1.6304\n",
            "Époque 2/50\n",
            "Loss: 1.6121\n",
            "Époque 3/50\n",
            "Loss: 1.5962\n",
            "Époque 4/50\n",
            "Loss: 1.5820\n",
            "Époque 5/50\n",
            "Loss: 1.5691\n",
            "Époque 6/50\n",
            "Loss: 1.5570\n",
            "Époque 7/50\n",
            "Loss: 1.5455\n",
            "Époque 8/50\n",
            "Loss: 1.5343\n",
            "Époque 9/50\n",
            "Loss: 1.5230\n",
            "Époque 10/50\n",
            "Loss: 1.5113\n",
            "Époque 11/50\n",
            "Loss: 1.4991\n",
            "Époque 12/50\n",
            "Loss: 1.4860\n",
            "Époque 13/50\n",
            "Loss: 1.4721\n",
            "Époque 14/50\n",
            "Loss: 1.4569\n",
            "Époque 15/50\n",
            "Loss: 1.4403\n",
            "Époque 16/50\n",
            "Loss: 1.4224\n",
            "Époque 17/50\n",
            "Loss: 1.4033\n",
            "Époque 18/50\n",
            "Loss: 1.3832\n",
            "Époque 19/50\n",
            "Loss: 1.3620\n",
            "Époque 20/50\n",
            "Loss: 1.3397\n",
            "Époque 21/50\n",
            "Loss: 1.3161\n",
            "Époque 22/50\n",
            "Loss: 1.2912\n",
            "Époque 23/50\n",
            "Loss: 1.2652\n",
            "Époque 24/50\n",
            "Loss: 1.2380\n",
            "Époque 25/50\n",
            "Loss: 1.2097\n",
            "Époque 26/50\n",
            "Loss: 1.1804\n",
            "Époque 27/50\n",
            "Loss: 1.1503\n",
            "Époque 28/50\n",
            "Loss: 1.1197\n",
            "Époque 29/50\n",
            "Loss: 1.0888\n",
            "Époque 30/50\n",
            "Loss: 1.0573\n",
            "Époque 31/50\n",
            "Loss: 1.0251\n",
            "Époque 32/50\n",
            "Loss: 0.9922\n",
            "Époque 33/50\n",
            "Loss: 0.9592\n",
            "Époque 34/50\n",
            "Loss: 0.9261\n",
            "Époque 35/50\n",
            "Loss: 0.8928\n",
            "Époque 36/50\n",
            "Loss: 0.8594\n",
            "Époque 37/50\n",
            "Loss: 0.8262\n",
            "Époque 38/50\n",
            "Loss: 0.7935\n",
            "Époque 39/50\n",
            "Loss: 0.7612\n",
            "Époque 40/50\n",
            "Loss: 0.7296\n",
            "Époque 41/50\n",
            "Loss: 0.6983\n",
            "Époque 42/50\n",
            "Loss: 0.6672\n",
            "Époque 43/50\n",
            "Loss: 0.6365\n",
            "Époque 44/50\n",
            "Loss: 0.6064\n",
            "Époque 45/50\n",
            "Loss: 0.5768\n",
            "Époque 46/50\n",
            "Loss: 0.5480\n",
            "Époque 47/50\n",
            "Loss: 0.5200\n",
            "Époque 48/50\n",
            "Loss: 0.4928\n",
            "Époque 49/50\n",
            "Loss: 0.4666\n",
            "Époque 50/50\n",
            "Loss: 0.4414\n",
            "Entraînement du client 2/3\n",
            "Époque 1/50\n",
            "Loss: 1.6115\n",
            "Époque 2/50\n",
            "Loss: 1.5972\n",
            "Époque 3/50\n",
            "Loss: 1.5840\n",
            "Époque 4/50\n",
            "Loss: 1.5714\n",
            "Époque 5/50\n",
            "Loss: 1.5590\n",
            "Époque 6/50\n",
            "Loss: 1.5466\n",
            "Époque 7/50\n",
            "Loss: 1.5341\n",
            "Époque 8/50\n",
            "Loss: 1.5212\n",
            "Époque 9/50\n",
            "Loss: 1.5079\n",
            "Époque 10/50\n",
            "Loss: 1.4940\n",
            "Époque 11/50\n",
            "Loss: 1.4794\n",
            "Époque 12/50\n",
            "Loss: 1.4641\n",
            "Époque 13/50\n",
            "Loss: 1.4479\n",
            "Époque 14/50\n",
            "Loss: 1.4305\n",
            "Époque 15/50\n",
            "Loss: 1.4115\n",
            "Époque 16/50\n",
            "Loss: 1.3908\n",
            "Époque 17/50\n",
            "Loss: 1.3685\n",
            "Époque 18/50\n",
            "Loss: 1.3447\n",
            "Époque 19/50\n",
            "Loss: 1.3199\n",
            "Époque 20/50\n",
            "Loss: 1.2942\n",
            "Époque 21/50\n",
            "Loss: 1.2677\n",
            "Époque 22/50\n",
            "Loss: 1.2404\n",
            "Époque 23/50\n",
            "Loss: 1.2121\n",
            "Époque 24/50\n",
            "Loss: 1.1828\n",
            "Époque 25/50\n",
            "Loss: 1.1524\n",
            "Époque 26/50\n",
            "Loss: 1.1212\n",
            "Époque 27/50\n",
            "Loss: 1.0893\n",
            "Époque 28/50\n",
            "Loss: 1.0565\n",
            "Époque 29/50\n",
            "Loss: 1.0229\n",
            "Époque 30/50\n",
            "Loss: 0.9888\n",
            "Époque 31/50\n",
            "Loss: 0.9540\n",
            "Époque 32/50\n",
            "Loss: 0.9185\n",
            "Époque 33/50\n",
            "Loss: 0.8828\n",
            "Époque 34/50\n",
            "Loss: 0.8471\n",
            "Époque 35/50\n",
            "Loss: 0.8114\n",
            "Époque 36/50\n",
            "Loss: 0.7754\n",
            "Époque 37/50\n",
            "Loss: 0.7394\n",
            "Époque 38/50\n",
            "Loss: 0.7042\n",
            "Époque 39/50\n",
            "Loss: 0.6701\n",
            "Époque 40/50\n",
            "Loss: 0.6364\n",
            "Époque 41/50\n",
            "Loss: 0.6035\n",
            "Époque 42/50\n",
            "Loss: 0.5717\n",
            "Époque 43/50\n",
            "Loss: 0.5409\n",
            "Époque 44/50\n",
            "Loss: 0.5113\n",
            "Époque 45/50\n",
            "Loss: 0.4835\n",
            "Époque 46/50\n",
            "Loss: 0.4572\n",
            "Époque 47/50\n",
            "Loss: 0.4319\n",
            "Époque 48/50\n",
            "Loss: 0.4081\n",
            "Époque 49/50\n",
            "Loss: 0.3855\n",
            "Époque 50/50\n",
            "Loss: 0.3641\n",
            "Époque 1/50\n",
            "Loss: 1.6115\n",
            "Époque 2/50\n",
            "Loss: 1.5972\n",
            "Époque 3/50\n",
            "Loss: 1.5840\n",
            "Époque 4/50\n",
            "Loss: 1.5714\n",
            "Époque 5/50\n",
            "Loss: 1.5590\n",
            "Époque 6/50\n",
            "Loss: 1.5466\n",
            "Époque 7/50\n",
            "Loss: 1.5341\n",
            "Époque 8/50\n",
            "Loss: 1.5212\n",
            "Époque 9/50\n",
            "Loss: 1.5079\n",
            "Époque 10/50\n",
            "Loss: 1.4940\n",
            "Époque 11/50\n",
            "Loss: 1.4794\n",
            "Époque 12/50\n",
            "Loss: 1.4641\n",
            "Époque 13/50\n",
            "Loss: 1.4479\n",
            "Époque 14/50\n",
            "Loss: 1.4305\n",
            "Époque 15/50\n",
            "Loss: 1.4115\n",
            "Époque 16/50\n",
            "Loss: 1.3908\n",
            "Époque 17/50\n",
            "Loss: 1.3685\n",
            "Époque 18/50\n",
            "Loss: 1.3447\n",
            "Époque 19/50\n",
            "Loss: 1.3199\n",
            "Époque 20/50\n",
            "Loss: 1.2942\n",
            "Époque 21/50\n",
            "Loss: 1.2677\n",
            "Époque 22/50\n",
            "Loss: 1.2404\n",
            "Époque 23/50\n",
            "Loss: 1.2121\n",
            "Époque 24/50\n",
            "Loss: 1.1828\n",
            "Époque 25/50\n",
            "Loss: 1.1524\n",
            "Époque 26/50\n",
            "Loss: 1.1212\n",
            "Époque 27/50\n",
            "Loss: 1.0893\n",
            "Époque 28/50\n",
            "Loss: 1.0565\n",
            "Époque 29/50\n",
            "Loss: 1.0229\n",
            "Époque 30/50\n",
            "Loss: 0.9888\n",
            "Époque 31/50\n",
            "Loss: 0.9540\n",
            "Époque 32/50\n",
            "Loss: 0.9185\n",
            "Époque 33/50\n",
            "Loss: 0.8828\n",
            "Époque 34/50\n",
            "Loss: 0.8471\n",
            "Époque 35/50\n",
            "Loss: 0.8114\n",
            "Époque 36/50\n",
            "Loss: 0.7754\n",
            "Époque 37/50\n",
            "Loss: 0.7394\n",
            "Époque 38/50\n",
            "Loss: 0.7042\n",
            "Époque 39/50\n",
            "Loss: 0.6701\n",
            "Époque 40/50\n",
            "Loss: 0.6364\n",
            "Époque 41/50\n",
            "Loss: 0.6035\n",
            "Époque 42/50\n",
            "Loss: 0.5717\n",
            "Époque 43/50\n",
            "Loss: 0.5409\n",
            "Époque 44/50\n",
            "Loss: 0.5113\n",
            "Époque 45/50\n",
            "Loss: 0.4835\n",
            "Époque 46/50\n",
            "Loss: 0.4572\n",
            "Époque 47/50\n",
            "Loss: 0.4319\n",
            "Époque 48/50\n",
            "Loss: 0.4081\n",
            "Époque 49/50\n",
            "Loss: 0.3855\n",
            "Époque 50/50\n",
            "Loss: 0.3641\n",
            "Époque 1/50\n",
            "Loss: 1.6115\n",
            "Époque 2/50\n",
            "Loss: 1.5972\n",
            "Époque 3/50\n",
            "Loss: 1.5840\n",
            "Époque 4/50\n",
            "Loss: 1.5714\n",
            "Époque 5/50\n",
            "Loss: 1.5590\n",
            "Époque 6/50\n",
            "Loss: 1.5466\n",
            "Époque 7/50\n",
            "Loss: 1.5341\n",
            "Époque 8/50\n",
            "Loss: 1.5212\n",
            "Époque 9/50\n",
            "Loss: 1.5079\n",
            "Époque 10/50\n",
            "Loss: 1.4940\n",
            "Époque 11/50\n",
            "Loss: 1.4794\n",
            "Époque 12/50\n",
            "Loss: 1.4641\n",
            "Époque 13/50\n",
            "Loss: 1.4479\n",
            "Époque 14/50\n",
            "Loss: 1.4305\n",
            "Époque 15/50\n",
            "Loss: 1.4115\n",
            "Époque 16/50\n",
            "Loss: 1.3908\n",
            "Époque 17/50\n",
            "Loss: 1.3685\n",
            "Époque 18/50\n",
            "Loss: 1.3447\n",
            "Époque 19/50\n",
            "Loss: 1.3199\n",
            "Époque 20/50\n",
            "Loss: 1.2942\n",
            "Époque 21/50\n",
            "Loss: 1.2677\n",
            "Époque 22/50\n",
            "Loss: 1.2404\n",
            "Époque 23/50\n",
            "Loss: 1.2121\n",
            "Époque 24/50\n",
            "Loss: 1.1828\n",
            "Époque 25/50\n",
            "Loss: 1.1524\n",
            "Époque 26/50\n",
            "Loss: 1.1212\n",
            "Époque 27/50\n",
            "Loss: 1.0893\n",
            "Époque 28/50\n",
            "Loss: 1.0565\n",
            "Époque 29/50\n",
            "Loss: 1.0229\n",
            "Époque 30/50\n",
            "Loss: 0.9888\n",
            "Époque 31/50\n",
            "Loss: 0.9540\n",
            "Époque 32/50\n",
            "Loss: 0.9185\n",
            "Époque 33/50\n",
            "Loss: 0.8828\n",
            "Époque 34/50\n",
            "Loss: 0.8471\n",
            "Époque 35/50\n",
            "Loss: 0.8114\n",
            "Époque 36/50\n",
            "Loss: 0.7754\n",
            "Époque 37/50\n",
            "Loss: 0.7394\n",
            "Époque 38/50\n",
            "Loss: 0.7042\n",
            "Époque 39/50\n",
            "Loss: 0.6701\n",
            "Époque 40/50\n",
            "Loss: 0.6364\n",
            "Époque 41/50\n",
            "Loss: 0.6035\n",
            "Époque 42/50\n",
            "Loss: 0.5717\n",
            "Époque 43/50\n",
            "Loss: 0.5409\n",
            "Époque 44/50\n",
            "Loss: 0.5113\n",
            "Époque 45/50\n",
            "Loss: 0.4835\n",
            "Époque 46/50\n",
            "Loss: 0.4572\n",
            "Époque 47/50\n",
            "Loss: 0.4319\n",
            "Époque 48/50\n",
            "Loss: 0.4081\n",
            "Époque 49/50\n",
            "Loss: 0.3855\n",
            "Époque 50/50\n",
            "Loss: 0.3641\n",
            "Époque 1/50\n",
            "Loss: 1.6115\n",
            "Époque 2/50\n",
            "Loss: 1.5972\n",
            "Époque 3/50\n",
            "Loss: 1.5840\n",
            "Époque 4/50\n",
            "Loss: 1.5714\n",
            "Époque 5/50\n",
            "Loss: 1.5590\n",
            "Époque 6/50\n",
            "Loss: 1.5466\n",
            "Époque 7/50\n",
            "Loss: 1.5341\n",
            "Époque 8/50\n",
            "Loss: 1.5212\n",
            "Époque 9/50\n",
            "Loss: 1.5079\n",
            "Époque 10/50\n",
            "Loss: 1.4940\n",
            "Époque 11/50\n",
            "Loss: 1.4794\n",
            "Époque 12/50\n",
            "Loss: 1.4641\n",
            "Époque 13/50\n",
            "Loss: 1.4479\n",
            "Époque 14/50\n",
            "Loss: 1.4305\n",
            "Époque 15/50\n",
            "Loss: 1.4115\n",
            "Époque 16/50\n",
            "Loss: 1.3908\n",
            "Époque 17/50\n",
            "Loss: 1.3685\n",
            "Époque 18/50\n",
            "Loss: 1.3447\n",
            "Époque 19/50\n",
            "Loss: 1.3199\n",
            "Époque 20/50\n",
            "Loss: 1.2942\n",
            "Époque 21/50\n",
            "Loss: 1.2677\n",
            "Époque 22/50\n",
            "Loss: 1.2404\n",
            "Époque 23/50\n",
            "Loss: 1.2121\n",
            "Époque 24/50\n",
            "Loss: 1.1828\n",
            "Époque 25/50\n",
            "Loss: 1.1524\n",
            "Époque 26/50\n",
            "Loss: 1.1212\n",
            "Époque 27/50\n",
            "Loss: 1.0893\n",
            "Époque 28/50\n",
            "Loss: 1.0565\n",
            "Époque 29/50\n",
            "Loss: 1.0229\n",
            "Époque 30/50\n",
            "Loss: 0.9888\n",
            "Époque 31/50\n",
            "Loss: 0.9540\n",
            "Époque 32/50\n",
            "Loss: 0.9185\n",
            "Époque 33/50\n",
            "Loss: 0.8828\n",
            "Époque 34/50\n",
            "Loss: 0.8471\n",
            "Époque 35/50\n",
            "Loss: 0.8114\n",
            "Époque 36/50\n",
            "Loss: 0.7754\n",
            "Époque 37/50\n",
            "Loss: 0.7394\n",
            "Époque 38/50\n",
            "Loss: 0.7042\n",
            "Époque 39/50\n",
            "Loss: 0.6701\n",
            "Époque 40/50\n",
            "Loss: 0.6364\n",
            "Époque 41/50\n",
            "Loss: 0.6035\n",
            "Époque 42/50\n",
            "Loss: 0.5717\n",
            "Époque 43/50\n",
            "Loss: 0.5409\n",
            "Époque 44/50\n",
            "Loss: 0.5113\n",
            "Époque 45/50\n",
            "Loss: 0.4835\n",
            "Époque 46/50\n",
            "Loss: 0.4572\n",
            "Époque 47/50\n",
            "Loss: 0.4319\n",
            "Époque 48/50\n",
            "Loss: 0.4081\n",
            "Époque 49/50\n",
            "Loss: 0.3855\n",
            "Époque 50/50\n",
            "Loss: 0.3641\n",
            "Époque 1/50\n",
            "Loss: 1.6115\n",
            "Époque 2/50\n",
            "Loss: 1.5972\n",
            "Époque 3/50\n",
            "Loss: 1.5840\n",
            "Époque 4/50\n",
            "Loss: 1.5714\n",
            "Époque 5/50\n",
            "Loss: 1.5590\n",
            "Époque 6/50\n",
            "Loss: 1.5466\n",
            "Époque 7/50\n",
            "Loss: 1.5341\n",
            "Époque 8/50\n",
            "Loss: 1.5212\n",
            "Époque 9/50\n",
            "Loss: 1.5079\n",
            "Époque 10/50\n",
            "Loss: 1.4940\n",
            "Époque 11/50\n",
            "Loss: 1.4794\n",
            "Époque 12/50\n",
            "Loss: 1.4641\n",
            "Époque 13/50\n",
            "Loss: 1.4479\n",
            "Époque 14/50\n",
            "Loss: 1.4305\n",
            "Époque 15/50\n",
            "Loss: 1.4115\n",
            "Époque 16/50\n",
            "Loss: 1.3908\n",
            "Époque 17/50\n",
            "Loss: 1.3685\n",
            "Époque 18/50\n",
            "Loss: 1.3447\n",
            "Époque 19/50\n",
            "Loss: 1.3199\n",
            "Époque 20/50\n",
            "Loss: 1.2942\n",
            "Époque 21/50\n",
            "Loss: 1.2677\n",
            "Époque 22/50\n",
            "Loss: 1.2404\n",
            "Époque 23/50\n",
            "Loss: 1.2121\n",
            "Époque 24/50\n",
            "Loss: 1.1828\n",
            "Époque 25/50\n",
            "Loss: 1.1524\n",
            "Époque 26/50\n",
            "Loss: 1.1212\n",
            "Époque 27/50\n",
            "Loss: 1.0893\n",
            "Époque 28/50\n",
            "Loss: 1.0565\n",
            "Époque 29/50\n",
            "Loss: 1.0229\n",
            "Époque 30/50\n",
            "Loss: 0.9888\n",
            "Époque 31/50\n",
            "Loss: 0.9540\n",
            "Époque 32/50\n",
            "Loss: 0.9185\n",
            "Époque 33/50\n",
            "Loss: 0.8828\n",
            "Époque 34/50\n",
            "Loss: 0.8471\n",
            "Époque 35/50\n",
            "Loss: 0.8114\n",
            "Époque 36/50\n",
            "Loss: 0.7754\n",
            "Époque 37/50\n",
            "Loss: 0.7394\n",
            "Époque 38/50\n",
            "Loss: 0.7042\n",
            "Époque 39/50\n",
            "Loss: 0.6701\n",
            "Époque 40/50\n",
            "Loss: 0.6364\n",
            "Époque 41/50\n",
            "Loss: 0.6035\n",
            "Époque 42/50\n",
            "Loss: 0.5717\n",
            "Époque 43/50\n",
            "Loss: 0.5409\n",
            "Époque 44/50\n",
            "Loss: 0.5113\n",
            "Époque 45/50\n",
            "Loss: 0.4835\n",
            "Époque 46/50\n",
            "Loss: 0.4572\n",
            "Époque 47/50\n",
            "Loss: 0.4319\n",
            "Époque 48/50\n",
            "Loss: 0.4081\n",
            "Époque 49/50\n",
            "Loss: 0.3855\n",
            "Époque 50/50\n",
            "Loss: 0.3641\n",
            "Entraînement du client 3/3\n",
            "Époque 1/50\n",
            "Loss: 1.6199\n",
            "Époque 2/50\n",
            "Loss: 1.6059\n",
            "Époque 3/50\n",
            "Loss: 1.5936\n",
            "Époque 4/50\n",
            "Loss: 1.5822\n",
            "Époque 5/50\n",
            "Loss: 1.5713\n",
            "Époque 6/50\n",
            "Loss: 1.5604\n",
            "Époque 7/50\n",
            "Loss: 1.5491\n",
            "Époque 8/50\n",
            "Loss: 1.5372\n",
            "Époque 9/50\n",
            "Loss: 1.5243\n",
            "Époque 10/50\n",
            "Loss: 1.5103\n",
            "Époque 11/50\n",
            "Loss: 1.4951\n",
            "Époque 12/50\n",
            "Loss: 1.4786\n",
            "Époque 13/50\n",
            "Loss: 1.4609\n",
            "Époque 14/50\n",
            "Loss: 1.4421\n",
            "Époque 15/50\n",
            "Loss: 1.4223\n",
            "Époque 16/50\n",
            "Loss: 1.4016\n",
            "Époque 17/50\n",
            "Loss: 1.3802\n",
            "Époque 18/50\n",
            "Loss: 1.3576\n",
            "Époque 19/50\n",
            "Loss: 1.3338\n",
            "Époque 20/50\n",
            "Loss: 1.3086\n",
            "Époque 21/50\n",
            "Loss: 1.2821\n",
            "Époque 22/50\n",
            "Loss: 1.2541\n",
            "Époque 23/50\n",
            "Loss: 1.2247\n",
            "Époque 24/50\n",
            "Loss: 1.1941\n",
            "Époque 25/50\n",
            "Loss: 1.1623\n",
            "Époque 26/50\n",
            "Loss: 1.1299\n",
            "Époque 27/50\n",
            "Loss: 1.0971\n",
            "Époque 28/50\n",
            "Loss: 1.0638\n",
            "Époque 29/50\n",
            "Loss: 1.0296\n",
            "Époque 30/50\n",
            "Loss: 0.9945\n",
            "Époque 31/50\n",
            "Loss: 0.9592\n",
            "Époque 32/50\n",
            "Loss: 0.9238\n",
            "Époque 33/50\n",
            "Loss: 0.8883\n",
            "Époque 34/50\n",
            "Loss: 0.8526\n",
            "Époque 35/50\n",
            "Loss: 0.8168\n",
            "Époque 36/50\n",
            "Loss: 0.7813\n",
            "Époque 37/50\n",
            "Loss: 0.7467\n",
            "Époque 38/50\n",
            "Loss: 0.7126\n",
            "Époque 39/50\n",
            "Loss: 0.6792\n",
            "Époque 40/50\n",
            "Loss: 0.6463\n",
            "Époque 41/50\n",
            "Loss: 0.6140\n",
            "Époque 42/50\n",
            "Loss: 0.5825\n",
            "Époque 43/50\n",
            "Loss: 0.5520\n",
            "Époque 44/50\n",
            "Loss: 0.5227\n",
            "Époque 45/50\n",
            "Loss: 0.4946\n",
            "Époque 46/50\n",
            "Loss: 0.4676\n",
            "Époque 47/50\n",
            "Loss: 0.4418\n",
            "Époque 48/50\n",
            "Loss: 0.4172\n",
            "Époque 49/50\n",
            "Loss: 0.3939\n",
            "Époque 50/50\n",
            "Loss: 0.3713\n",
            "Époque 1/50\n",
            "Loss: 1.6199\n",
            "Époque 2/50\n",
            "Loss: 1.6059\n",
            "Époque 3/50\n",
            "Loss: 1.5936\n",
            "Époque 4/50\n",
            "Loss: 1.5822\n",
            "Époque 5/50\n",
            "Loss: 1.5713\n",
            "Époque 6/50\n",
            "Loss: 1.5604\n",
            "Époque 7/50\n",
            "Loss: 1.5491\n",
            "Époque 8/50\n",
            "Loss: 1.5372\n",
            "Époque 9/50\n",
            "Loss: 1.5243\n",
            "Époque 10/50\n",
            "Loss: 1.5103\n",
            "Époque 11/50\n",
            "Loss: 1.4951\n",
            "Époque 12/50\n",
            "Loss: 1.4786\n",
            "Époque 13/50\n",
            "Loss: 1.4609\n",
            "Époque 14/50\n",
            "Loss: 1.4421\n",
            "Époque 15/50\n",
            "Loss: 1.4223\n",
            "Époque 16/50\n",
            "Loss: 1.4016\n",
            "Époque 17/50\n",
            "Loss: 1.3802\n",
            "Époque 18/50\n",
            "Loss: 1.3576\n",
            "Époque 19/50\n",
            "Loss: 1.3338\n",
            "Époque 20/50\n",
            "Loss: 1.3086\n",
            "Époque 21/50\n",
            "Loss: 1.2821\n",
            "Époque 22/50\n",
            "Loss: 1.2541\n",
            "Époque 23/50\n",
            "Loss: 1.2247\n",
            "Époque 24/50\n",
            "Loss: 1.1941\n",
            "Époque 25/50\n",
            "Loss: 1.1623\n",
            "Époque 26/50\n",
            "Loss: 1.1299\n",
            "Époque 27/50\n",
            "Loss: 1.0971\n",
            "Époque 28/50\n",
            "Loss: 1.0638\n",
            "Époque 29/50\n",
            "Loss: 1.0296\n",
            "Époque 30/50\n",
            "Loss: 0.9945\n",
            "Époque 31/50\n",
            "Loss: 0.9592\n",
            "Époque 32/50\n",
            "Loss: 0.9238\n",
            "Époque 33/50\n",
            "Loss: 0.8883\n",
            "Époque 34/50\n",
            "Loss: 0.8526\n",
            "Époque 35/50\n",
            "Loss: 0.8168\n",
            "Époque 36/50\n",
            "Loss: 0.7813\n",
            "Époque 37/50\n",
            "Loss: 0.7467\n",
            "Époque 38/50\n",
            "Loss: 0.7126\n",
            "Époque 39/50\n",
            "Loss: 0.6792\n",
            "Époque 40/50\n",
            "Loss: 0.6463\n",
            "Époque 41/50\n",
            "Loss: 0.6140\n",
            "Époque 42/50\n",
            "Loss: 0.5825\n",
            "Époque 43/50\n",
            "Loss: 0.5520\n",
            "Époque 44/50\n",
            "Loss: 0.5227\n",
            "Époque 45/50\n",
            "Loss: 0.4946\n",
            "Époque 46/50\n",
            "Loss: 0.4676\n",
            "Époque 47/50\n",
            "Loss: 0.4418\n",
            "Époque 48/50\n",
            "Loss: 0.4172\n",
            "Époque 49/50\n",
            "Loss: 0.3939\n",
            "Époque 50/50\n",
            "Loss: 0.3713\n",
            "Époque 1/50\n",
            "Loss: 1.6199\n",
            "Époque 2/50\n",
            "Loss: 1.6059\n",
            "Époque 3/50\n",
            "Loss: 1.5936\n",
            "Époque 4/50\n",
            "Loss: 1.5822\n",
            "Époque 5/50\n",
            "Loss: 1.5713\n",
            "Époque 6/50\n",
            "Loss: 1.5604\n",
            "Époque 7/50\n",
            "Loss: 1.5491\n",
            "Époque 8/50\n",
            "Loss: 1.5372\n",
            "Époque 9/50\n",
            "Loss: 1.5243\n",
            "Époque 10/50\n",
            "Loss: 1.5103\n",
            "Époque 11/50\n",
            "Loss: 1.4951\n",
            "Époque 12/50\n",
            "Loss: 1.4786\n",
            "Époque 13/50\n",
            "Loss: 1.4609\n",
            "Époque 14/50\n",
            "Loss: 1.4421\n",
            "Époque 15/50\n",
            "Loss: 1.4223\n",
            "Époque 16/50\n",
            "Loss: 1.4016\n",
            "Époque 17/50\n",
            "Loss: 1.3802\n",
            "Époque 18/50\n",
            "Loss: 1.3576\n",
            "Époque 19/50\n",
            "Loss: 1.3338\n",
            "Époque 20/50\n",
            "Loss: 1.3086\n",
            "Époque 21/50\n",
            "Loss: 1.2821\n",
            "Époque 22/50\n",
            "Loss: 1.2541\n",
            "Époque 23/50\n",
            "Loss: 1.2247\n",
            "Époque 24/50\n",
            "Loss: 1.1941\n",
            "Époque 25/50\n",
            "Loss: 1.1623\n",
            "Époque 26/50\n",
            "Loss: 1.1299\n",
            "Époque 27/50\n",
            "Loss: 1.0971\n",
            "Époque 28/50\n",
            "Loss: 1.0638\n",
            "Époque 29/50\n",
            "Loss: 1.0296\n",
            "Époque 30/50\n",
            "Loss: 0.9945\n",
            "Époque 31/50\n",
            "Loss: 0.9592\n",
            "Époque 32/50\n",
            "Loss: 0.9238\n",
            "Époque 33/50\n",
            "Loss: 0.8883\n",
            "Époque 34/50\n",
            "Loss: 0.8526\n",
            "Époque 35/50\n",
            "Loss: 0.8168\n",
            "Époque 36/50\n",
            "Loss: 0.7813\n",
            "Époque 37/50\n",
            "Loss: 0.7467\n",
            "Époque 38/50\n",
            "Loss: 0.7126\n",
            "Époque 39/50\n",
            "Loss: 0.6792\n",
            "Époque 40/50\n",
            "Loss: 0.6463\n",
            "Époque 41/50\n",
            "Loss: 0.6140\n",
            "Époque 42/50\n",
            "Loss: 0.5825\n",
            "Époque 43/50\n",
            "Loss: 0.5520\n",
            "Époque 44/50\n",
            "Loss: 0.5227\n",
            "Époque 45/50\n",
            "Loss: 0.4946\n",
            "Époque 46/50\n",
            "Loss: 0.4676\n",
            "Époque 47/50\n",
            "Loss: 0.4418\n",
            "Époque 48/50\n",
            "Loss: 0.4172\n",
            "Époque 49/50\n",
            "Loss: 0.3939\n",
            "Époque 50/50\n",
            "Loss: 0.3713\n",
            "Époque 1/50\n",
            "Loss: 1.6199\n",
            "Époque 2/50\n",
            "Loss: 1.6059\n",
            "Époque 3/50\n",
            "Loss: 1.5936\n",
            "Époque 4/50\n",
            "Loss: 1.5822\n",
            "Époque 5/50\n",
            "Loss: 1.5713\n",
            "Époque 6/50\n",
            "Loss: 1.5604\n",
            "Époque 7/50\n",
            "Loss: 1.5491\n",
            "Époque 8/50\n",
            "Loss: 1.5372\n",
            "Époque 9/50\n",
            "Loss: 1.5243\n",
            "Époque 10/50\n",
            "Loss: 1.5103\n",
            "Époque 11/50\n",
            "Loss: 1.4951\n",
            "Époque 12/50\n",
            "Loss: 1.4786\n",
            "Époque 13/50\n",
            "Loss: 1.4609\n",
            "Époque 14/50\n",
            "Loss: 1.4421\n",
            "Époque 15/50\n",
            "Loss: 1.4223\n",
            "Époque 16/50\n",
            "Loss: 1.4016\n",
            "Époque 17/50\n",
            "Loss: 1.3802\n",
            "Époque 18/50\n",
            "Loss: 1.3576\n",
            "Époque 19/50\n",
            "Loss: 1.3338\n",
            "Époque 20/50\n",
            "Loss: 1.3086\n",
            "Époque 21/50\n",
            "Loss: 1.2821\n",
            "Époque 22/50\n",
            "Loss: 1.2541\n",
            "Époque 23/50\n",
            "Loss: 1.2247\n",
            "Époque 24/50\n",
            "Loss: 1.1941\n",
            "Époque 25/50\n",
            "Loss: 1.1623\n",
            "Époque 26/50\n",
            "Loss: 1.1299\n",
            "Époque 27/50\n",
            "Loss: 1.0971\n",
            "Époque 28/50\n",
            "Loss: 1.0638\n",
            "Époque 29/50\n",
            "Loss: 1.0296\n",
            "Époque 30/50\n",
            "Loss: 0.9945\n",
            "Époque 31/50\n",
            "Loss: 0.9592\n",
            "Époque 32/50\n",
            "Loss: 0.9238\n",
            "Époque 33/50\n",
            "Loss: 0.8883\n",
            "Époque 34/50\n",
            "Loss: 0.8526\n",
            "Époque 35/50\n",
            "Loss: 0.8168\n",
            "Époque 36/50\n",
            "Loss: 0.7813\n",
            "Époque 37/50\n",
            "Loss: 0.7467\n",
            "Époque 38/50\n",
            "Loss: 0.7126\n",
            "Époque 39/50\n",
            "Loss: 0.6792\n",
            "Époque 40/50\n",
            "Loss: 0.6463\n",
            "Époque 41/50\n",
            "Loss: 0.6140\n",
            "Époque 42/50\n",
            "Loss: 0.5825\n",
            "Époque 43/50\n",
            "Loss: 0.5520\n",
            "Époque 44/50\n",
            "Loss: 0.5227\n",
            "Époque 45/50\n",
            "Loss: 0.4946\n",
            "Époque 46/50\n",
            "Loss: 0.4676\n",
            "Époque 47/50\n",
            "Loss: 0.4418\n",
            "Époque 48/50\n",
            "Loss: 0.4172\n",
            "Époque 49/50\n",
            "Loss: 0.3939\n",
            "Époque 50/50\n",
            "Loss: 0.3713\n",
            "Époque 1/50\n",
            "Loss: 1.6199\n",
            "Époque 2/50\n",
            "Loss: 1.6059\n",
            "Époque 3/50\n",
            "Loss: 1.5936\n",
            "Époque 4/50\n",
            "Loss: 1.5822\n",
            "Époque 5/50\n",
            "Loss: 1.5713\n",
            "Époque 6/50\n",
            "Loss: 1.5604\n",
            "Époque 7/50\n",
            "Loss: 1.5491\n",
            "Époque 8/50\n",
            "Loss: 1.5372\n",
            "Époque 9/50\n",
            "Loss: 1.5243\n",
            "Époque 10/50\n",
            "Loss: 1.5103\n",
            "Époque 11/50\n",
            "Loss: 1.4951\n",
            "Époque 12/50\n",
            "Loss: 1.4786\n",
            "Époque 13/50\n",
            "Loss: 1.4609\n",
            "Époque 14/50\n",
            "Loss: 1.4421\n",
            "Époque 15/50\n",
            "Loss: 1.4223\n",
            "Époque 16/50\n",
            "Loss: 1.4016\n",
            "Époque 17/50\n",
            "Loss: 1.3802\n",
            "Époque 18/50\n",
            "Loss: 1.3576\n",
            "Époque 19/50\n",
            "Loss: 1.3338\n",
            "Époque 20/50\n",
            "Loss: 1.3086\n",
            "Époque 21/50\n",
            "Loss: 1.2821\n",
            "Époque 22/50\n",
            "Loss: 1.2541\n",
            "Époque 23/50\n",
            "Loss: 1.2247\n",
            "Époque 24/50\n",
            "Loss: 1.1941\n",
            "Époque 25/50\n",
            "Loss: 1.1623\n",
            "Époque 26/50\n",
            "Loss: 1.1299\n",
            "Époque 27/50\n",
            "Loss: 1.0971\n",
            "Époque 28/50\n",
            "Loss: 1.0638\n",
            "Époque 29/50\n",
            "Loss: 1.0296\n",
            "Époque 30/50\n",
            "Loss: 0.9945\n",
            "Époque 31/50\n",
            "Loss: 0.9592\n",
            "Époque 32/50\n",
            "Loss: 0.9238\n",
            "Époque 33/50\n",
            "Loss: 0.8883\n",
            "Époque 34/50\n",
            "Loss: 0.8526\n",
            "Époque 35/50\n",
            "Loss: 0.8168\n",
            "Époque 36/50\n",
            "Loss: 0.7813\n",
            "Époque 37/50\n",
            "Loss: 0.7467\n",
            "Époque 38/50\n",
            "Loss: 0.7126\n",
            "Époque 39/50\n",
            "Loss: 0.6792\n",
            "Époque 40/50\n",
            "Loss: 0.6463\n",
            "Époque 41/50\n",
            "Loss: 0.6140\n",
            "Époque 42/50\n",
            "Loss: 0.5825\n",
            "Époque 43/50\n",
            "Loss: 0.5520\n",
            "Époque 44/50\n",
            "Loss: 0.5227\n",
            "Époque 45/50\n",
            "Loss: 0.4946\n",
            "Époque 46/50\n",
            "Loss: 0.4676\n",
            "Époque 47/50\n",
            "Loss: 0.4418\n",
            "Époque 48/50\n",
            "Loss: 0.4172\n",
            "Époque 49/50\n",
            "Loss: 0.3939\n",
            "Époque 50/50\n",
            "Loss: 0.3713\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
            "[WinError 2] Le fichier spécifié est introuvable\n",
            "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
            "  warnings.warn(\n",
            "  File \"C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
            "    cpu_info = subprocess.run(\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 548, in run\n",
            "    with Popen(*popenargs, **kwargs) as process:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 1026, in __init__\n",
            "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
            "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\ikram\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Résultats du cycle 1:\n",
            "  Méthode proposée - Accuracy: 0.4389, Loss: 0.1926\n",
            "  FedAvg traditionnel - Accuracy: 0.4389, Loss: 0.1926\n",
            "  Bande passante traditionnelle: 63.43 KB\n",
            "  Bande passante proposée: 253.84 KB\n",
            "\n",
            "=== Cycle de communication 2/3 ===\n",
            "Phase 2: Mise à jour avec matrices de transition uniquement\n",
            "Client 1/3\n",
            "Époque 1/50\n",
            "Loss: 1.5322\n",
            "Époque 2/50\n",
            "Loss: 1.3779\n",
            "Époque 3/50\n",
            "Loss: 1.2809\n",
            "Époque 4/50\n",
            "Loss: 1.2111\n",
            "Époque 5/50\n",
            "Loss: 1.1562\n",
            "Époque 6/50\n",
            "Loss: 1.1098\n",
            "Époque 7/50\n",
            "Loss: 1.0672\n",
            "Époque 8/50\n",
            "Loss: 1.0259\n",
            "Époque 9/50\n",
            "Loss: 0.9855\n",
            "Époque 10/50\n",
            "Loss: 0.9469\n",
            "Époque 11/50\n",
            "Loss: 0.9108\n",
            "Époque 12/50\n",
            "Loss: 0.8773\n",
            "Époque 13/50\n",
            "Loss: 0.8456\n",
            "Époque 14/50\n",
            "Loss: 0.8151\n",
            "Époque 15/50\n",
            "Loss: 0.7854\n",
            "Époque 16/50\n",
            "Loss: 0.7565\n",
            "Époque 17/50\n",
            "Loss: 0.7280\n",
            "Époque 18/50\n",
            "Loss: 0.6997\n",
            "Époque 19/50\n",
            "Loss: 0.6719\n",
            "Époque 20/50\n",
            "Loss: 0.6451\n",
            "Époque 21/50\n",
            "Loss: 0.6196\n",
            "Époque 22/50\n",
            "Loss: 0.5951\n",
            "Époque 23/50\n",
            "Loss: 0.5713\n",
            "Époque 24/50\n",
            "Loss: 0.5484\n",
            "Époque 25/50\n",
            "Loss: 0.5262\n",
            "Époque 26/50\n",
            "Loss: 0.5047\n",
            "Époque 27/50\n",
            "Loss: 0.4839\n",
            "Époque 28/50\n",
            "Loss: 0.4638\n",
            "Époque 29/50\n",
            "Loss: 0.4443\n",
            "Époque 30/50\n",
            "Loss: 0.4253\n",
            "Époque 31/50\n",
            "Loss: 0.4068\n",
            "Époque 32/50\n",
            "Loss: 0.3890\n",
            "Époque 33/50\n",
            "Loss: 0.3721\n",
            "Époque 34/50\n",
            "Loss: 0.3560\n",
            "Époque 35/50\n",
            "Loss: 0.3406\n",
            "Époque 36/50\n",
            "Loss: 0.3259\n",
            "Époque 37/50\n",
            "Loss: 0.3118\n",
            "Époque 38/50\n",
            "Loss: 0.2983\n",
            "Époque 39/50\n",
            "Loss: 0.2853\n",
            "Époque 40/50\n",
            "Loss: 0.2729\n",
            "Époque 41/50\n",
            "Loss: 0.2610\n",
            "Époque 42/50\n",
            "Loss: 0.2496\n",
            "Époque 43/50\n",
            "Loss: 0.2387\n",
            "Époque 44/50\n",
            "Loss: 0.2284\n",
            "Époque 45/50\n",
            "Loss: 0.2185\n",
            "Époque 46/50\n",
            "Loss: 0.2092\n",
            "Époque 47/50\n",
            "Loss: 0.2002\n",
            "Époque 48/50\n",
            "Loss: 0.1917\n",
            "Époque 49/50\n",
            "Loss: 0.1837\n",
            "Époque 50/50\n",
            "Loss: 0.1760\n",
            "Époque 1/50\n",
            "Loss: 1.5322\n",
            "Époque 2/50\n",
            "Loss: 1.3779\n",
            "Époque 3/50\n",
            "Loss: 1.2809\n",
            "Époque 4/50\n",
            "Loss: 1.2111\n",
            "Époque 5/50\n",
            "Loss: 1.1562\n",
            "Époque 6/50\n",
            "Loss: 1.1098\n",
            "Époque 7/50\n",
            "Loss: 1.0672\n",
            "Époque 8/50\n",
            "Loss: 1.0259\n",
            "Époque 9/50\n",
            "Loss: 0.9855\n",
            "Époque 10/50\n",
            "Loss: 0.9469\n",
            "Époque 11/50\n",
            "Loss: 0.9108\n",
            "Époque 12/50\n",
            "Loss: 0.8773\n",
            "Époque 13/50\n",
            "Loss: 0.8456\n",
            "Époque 14/50\n",
            "Loss: 0.8151\n",
            "Époque 15/50\n",
            "Loss: 0.7854\n",
            "Époque 16/50\n",
            "Loss: 0.7565\n",
            "Époque 17/50\n",
            "Loss: 0.7280\n",
            "Époque 18/50\n",
            "Loss: 0.6997\n",
            "Époque 19/50\n",
            "Loss: 0.6719\n",
            "Époque 20/50\n",
            "Loss: 0.6451\n",
            "Époque 21/50\n",
            "Loss: 0.6196\n",
            "Époque 22/50\n",
            "Loss: 0.5951\n",
            "Époque 23/50\n",
            "Loss: 0.5713\n",
            "Époque 24/50\n",
            "Loss: 0.5484\n",
            "Époque 25/50\n",
            "Loss: 0.5262\n",
            "Époque 26/50\n",
            "Loss: 0.5047\n",
            "Époque 27/50\n",
            "Loss: 0.4839\n",
            "Époque 28/50\n",
            "Loss: 0.4638\n",
            "Époque 29/50\n",
            "Loss: 0.4443\n",
            "Époque 30/50\n",
            "Loss: 0.4253\n",
            "Époque 31/50\n",
            "Loss: 0.4068\n",
            "Époque 32/50\n",
            "Loss: 0.3890\n",
            "Époque 33/50\n",
            "Loss: 0.3721\n",
            "Époque 34/50\n",
            "Loss: 0.3560\n",
            "Époque 35/50\n",
            "Loss: 0.3406\n",
            "Époque 36/50\n",
            "Loss: 0.3259\n",
            "Époque 37/50\n",
            "Loss: 0.3118\n",
            "Époque 38/50\n",
            "Loss: 0.2983\n",
            "Époque 39/50\n",
            "Loss: 0.2853\n",
            "Époque 40/50\n",
            "Loss: 0.2729\n",
            "Époque 41/50\n",
            "Loss: 0.2610\n",
            "Époque 42/50\n",
            "Loss: 0.2496\n",
            "Époque 43/50\n",
            "Loss: 0.2387\n",
            "Époque 44/50\n",
            "Loss: 0.2284\n",
            "Époque 45/50\n",
            "Loss: 0.2185\n",
            "Époque 46/50\n",
            "Loss: 0.2092\n",
            "Époque 47/50\n",
            "Loss: 0.2002\n",
            "Époque 48/50\n",
            "Loss: 0.1917\n",
            "Époque 49/50\n",
            "Loss: 0.1837\n",
            "Époque 50/50\n",
            "Loss: 0.1760\n",
            "Époque 1/50\n",
            "Loss: 1.5322\n",
            "Époque 2/50\n",
            "Loss: 1.3779\n",
            "Époque 3/50\n",
            "Loss: 1.2809\n",
            "Époque 4/50\n",
            "Loss: 1.2111\n",
            "Époque 5/50\n",
            "Loss: 1.1562\n",
            "Époque 6/50\n",
            "Loss: 1.1098\n",
            "Époque 7/50\n",
            "Loss: 1.0672\n",
            "Époque 8/50\n",
            "Loss: 1.0259\n",
            "Époque 9/50\n",
            "Loss: 0.9855\n",
            "Époque 10/50\n",
            "Loss: 0.9469\n",
            "Époque 11/50\n",
            "Loss: 0.9108\n",
            "Époque 12/50\n",
            "Loss: 0.8773\n",
            "Époque 13/50\n",
            "Loss: 0.8456\n",
            "Époque 14/50\n",
            "Loss: 0.8151\n",
            "Époque 15/50\n",
            "Loss: 0.7854\n",
            "Époque 16/50\n",
            "Loss: 0.7565\n",
            "Époque 17/50\n",
            "Loss: 0.7280\n",
            "Époque 18/50\n",
            "Loss: 0.6997\n",
            "Époque 19/50\n",
            "Loss: 0.6719\n",
            "Époque 20/50\n",
            "Loss: 0.6451\n",
            "Époque 21/50\n",
            "Loss: 0.6196\n",
            "Époque 22/50\n",
            "Loss: 0.5951\n",
            "Époque 23/50\n",
            "Loss: 0.5713\n",
            "Époque 24/50\n",
            "Loss: 0.5484\n",
            "Époque 25/50\n",
            "Loss: 0.5262\n",
            "Époque 26/50\n",
            "Loss: 0.5047\n",
            "Époque 27/50\n",
            "Loss: 0.4839\n",
            "Époque 28/50\n",
            "Loss: 0.4638\n",
            "Époque 29/50\n",
            "Loss: 0.4443\n",
            "Époque 30/50\n",
            "Loss: 0.4253\n",
            "Époque 31/50\n",
            "Loss: 0.4068\n",
            "Époque 32/50\n",
            "Loss: 0.3890\n",
            "Époque 33/50\n",
            "Loss: 0.3721\n",
            "Époque 34/50\n",
            "Loss: 0.3560\n",
            "Époque 35/50\n",
            "Loss: 0.3406\n",
            "Époque 36/50\n",
            "Loss: 0.3259\n",
            "Époque 37/50\n",
            "Loss: 0.3118\n",
            "Époque 38/50\n",
            "Loss: 0.2983\n",
            "Époque 39/50\n",
            "Loss: 0.2853\n",
            "Époque 40/50\n",
            "Loss: 0.2729\n",
            "Époque 41/50\n",
            "Loss: 0.2610\n",
            "Époque 42/50\n",
            "Loss: 0.2496\n",
            "Époque 43/50\n",
            "Loss: 0.2387\n",
            "Époque 44/50\n",
            "Loss: 0.2284\n",
            "Époque 45/50\n",
            "Loss: 0.2185\n",
            "Époque 46/50\n",
            "Loss: 0.2092\n",
            "Époque 47/50\n",
            "Loss: 0.2002\n",
            "Époque 48/50\n",
            "Loss: 0.1917\n",
            "Époque 49/50\n",
            "Loss: 0.1837\n",
            "Époque 50/50\n",
            "Loss: 0.1760\n",
            "Époque 1/50\n",
            "Loss: 1.5322\n",
            "Époque 2/50\n",
            "Loss: 1.3779\n",
            "Époque 3/50\n",
            "Loss: 1.2809\n",
            "Époque 4/50\n",
            "Loss: 1.2111\n",
            "Époque 5/50\n",
            "Loss: 1.1562\n",
            "Époque 6/50\n",
            "Loss: 1.1098\n",
            "Époque 7/50\n",
            "Loss: 1.0672\n",
            "Époque 8/50\n",
            "Loss: 1.0259\n",
            "Époque 9/50\n",
            "Loss: 0.9855\n",
            "Époque 10/50\n",
            "Loss: 0.9469\n",
            "Époque 11/50\n",
            "Loss: 0.9108\n",
            "Époque 12/50\n",
            "Loss: 0.8773\n",
            "Époque 13/50\n",
            "Loss: 0.8456\n",
            "Époque 14/50\n",
            "Loss: 0.8151\n",
            "Époque 15/50\n",
            "Loss: 0.7854\n",
            "Époque 16/50\n",
            "Loss: 0.7565\n",
            "Époque 17/50\n",
            "Loss: 0.7280\n",
            "Époque 18/50\n",
            "Loss: 0.6997\n",
            "Époque 19/50\n",
            "Loss: 0.6719\n",
            "Époque 20/50\n",
            "Loss: 0.6451\n",
            "Époque 21/50\n",
            "Loss: 0.6196\n",
            "Époque 22/50\n",
            "Loss: 0.5951\n",
            "Époque 23/50\n",
            "Loss: 0.5713\n",
            "Époque 24/50\n",
            "Loss: 0.5484\n",
            "Époque 25/50\n",
            "Loss: 0.5262\n",
            "Époque 26/50\n",
            "Loss: 0.5047\n",
            "Époque 27/50\n",
            "Loss: 0.4839\n",
            "Époque 28/50\n",
            "Loss: 0.4638\n",
            "Époque 29/50\n",
            "Loss: 0.4443\n",
            "Époque 30/50\n",
            "Loss: 0.4253\n",
            "Époque 31/50\n",
            "Loss: 0.4068\n",
            "Époque 32/50\n",
            "Loss: 0.3890\n",
            "Époque 33/50\n",
            "Loss: 0.3721\n",
            "Époque 34/50\n",
            "Loss: 0.3560\n",
            "Époque 35/50\n",
            "Loss: 0.3406\n",
            "Époque 36/50\n",
            "Loss: 0.3259\n",
            "Époque 37/50\n",
            "Loss: 0.3118\n",
            "Époque 38/50\n",
            "Loss: 0.2983\n",
            "Époque 39/50\n",
            "Loss: 0.2853\n",
            "Époque 40/50\n",
            "Loss: 0.2729\n",
            "Époque 41/50\n",
            "Loss: 0.2610\n",
            "Époque 42/50\n",
            "Loss: 0.2496\n",
            "Époque 43/50\n",
            "Loss: 0.2387\n",
            "Époque 44/50\n",
            "Loss: 0.2284\n",
            "Époque 45/50\n",
            "Loss: 0.2185\n",
            "Époque 46/50\n",
            "Loss: 0.2092\n",
            "Époque 47/50\n",
            "Loss: 0.2002\n",
            "Époque 48/50\n",
            "Loss: 0.1917\n",
            "Époque 49/50\n",
            "Loss: 0.1837\n",
            "Époque 50/50\n",
            "Loss: 0.1760\n",
            "Époque 1/50\n",
            "Loss: 1.5322\n",
            "Époque 2/50\n",
            "Loss: 1.3779\n",
            "Époque 3/50\n",
            "Loss: 1.2809\n",
            "Époque 4/50\n",
            "Loss: 1.2111\n",
            "Époque 5/50\n",
            "Loss: 1.1562\n",
            "Époque 6/50\n",
            "Loss: 1.1098\n",
            "Époque 7/50\n",
            "Loss: 1.0672\n",
            "Époque 8/50\n",
            "Loss: 1.0259\n",
            "Époque 9/50\n",
            "Loss: 0.9855\n",
            "Époque 10/50\n",
            "Loss: 0.9469\n",
            "Époque 11/50\n",
            "Loss: 0.9108\n",
            "Époque 12/50\n",
            "Loss: 0.8773\n",
            "Époque 13/50\n",
            "Loss: 0.8456\n",
            "Époque 14/50\n",
            "Loss: 0.8151\n",
            "Époque 15/50\n",
            "Loss: 0.7854\n",
            "Époque 16/50\n",
            "Loss: 0.7565\n",
            "Époque 17/50\n",
            "Loss: 0.7280\n",
            "Époque 18/50\n",
            "Loss: 0.6997\n",
            "Époque 19/50\n",
            "Loss: 0.6719\n",
            "Époque 20/50\n",
            "Loss: 0.6451\n",
            "Époque 21/50\n",
            "Loss: 0.6196\n",
            "Époque 22/50\n",
            "Loss: 0.5951\n",
            "Époque 23/50\n",
            "Loss: 0.5713\n",
            "Époque 24/50\n",
            "Loss: 0.5484\n",
            "Époque 25/50\n",
            "Loss: 0.5262\n",
            "Époque 26/50\n",
            "Loss: 0.5047\n",
            "Époque 27/50\n",
            "Loss: 0.4839\n",
            "Époque 28/50\n",
            "Loss: 0.4638\n",
            "Époque 29/50\n",
            "Loss: 0.4443\n",
            "Époque 30/50\n",
            "Loss: 0.4253\n",
            "Époque 31/50\n",
            "Loss: 0.4068\n",
            "Époque 32/50\n",
            "Loss: 0.3890\n",
            "Époque 33/50\n",
            "Loss: 0.3721\n",
            "Époque 34/50\n",
            "Loss: 0.3560\n",
            "Époque 35/50\n",
            "Loss: 0.3406\n",
            "Époque 36/50\n",
            "Loss: 0.3259\n",
            "Époque 37/50\n",
            "Loss: 0.3118\n",
            "Époque 38/50\n",
            "Loss: 0.2983\n",
            "Époque 39/50\n",
            "Loss: 0.2853\n",
            "Époque 40/50\n",
            "Loss: 0.2729\n",
            "Époque 41/50\n",
            "Loss: 0.2610\n",
            "Époque 42/50\n",
            "Loss: 0.2496\n",
            "Époque 43/50\n",
            "Loss: 0.2387\n",
            "Époque 44/50\n",
            "Loss: 0.2284\n",
            "Époque 45/50\n",
            "Loss: 0.2185\n",
            "Époque 46/50\n",
            "Loss: 0.2092\n",
            "Époque 47/50\n",
            "Loss: 0.2002\n",
            "Époque 48/50\n",
            "Loss: 0.1917\n",
            "Époque 49/50\n",
            "Loss: 0.1837\n",
            "Époque 50/50\n",
            "Loss: 0.1760\n",
            "Époque 1/50\n",
            "Loss: 1.5322\n",
            "Époque 2/50\n",
            "Loss: 1.3779\n",
            "Époque 3/50\n",
            "Loss: 1.2809\n",
            "Époque 4/50\n",
            "Loss: 1.2111\n",
            "Époque 5/50\n",
            "Loss: 1.1562\n",
            "Époque 6/50\n",
            "Loss: 1.1098\n",
            "Époque 7/50\n",
            "Loss: 1.0672\n",
            "Époque 8/50\n",
            "Loss: 1.0259\n",
            "Époque 9/50\n",
            "Loss: 0.9855\n",
            "Époque 10/50\n",
            "Loss: 0.9469\n",
            "Époque 11/50\n",
            "Loss: 0.9108\n",
            "Époque 12/50\n",
            "Loss: 0.8773\n",
            "Époque 13/50\n",
            "Loss: 0.8456\n",
            "Époque 14/50\n",
            "Loss: 0.8151\n",
            "Époque 15/50\n",
            "Loss: 0.7854\n",
            "Époque 16/50\n",
            "Loss: 0.7565\n",
            "Époque 17/50\n",
            "Loss: 0.7280\n",
            "Époque 18/50\n",
            "Loss: 0.6997\n",
            "Époque 19/50\n",
            "Loss: 0.6719\n",
            "Époque 20/50\n",
            "Loss: 0.6451\n",
            "Époque 21/50\n",
            "Loss: 0.6196\n",
            "Époque 22/50\n",
            "Loss: 0.5951\n",
            "Époque 23/50\n",
            "Loss: 0.5713\n",
            "Époque 24/50\n",
            "Loss: 0.5484\n",
            "Époque 25/50\n",
            "Loss: 0.5262\n",
            "Époque 26/50\n",
            "Loss: 0.5047\n",
            "Époque 27/50\n",
            "Loss: 0.4839\n",
            "Époque 28/50\n",
            "Loss: 0.4638\n",
            "Époque 29/50\n",
            "Loss: 0.4443\n",
            "Époque 30/50\n",
            "Loss: 0.4253\n",
            "Époque 31/50\n",
            "Loss: 0.4068\n",
            "Époque 32/50\n",
            "Loss: 0.3890\n",
            "Époque 33/50\n",
            "Loss: 0.3721\n",
            "Époque 34/50\n",
            "Loss: 0.3560\n",
            "Époque 35/50\n",
            "Loss: 0.3406\n",
            "Époque 36/50\n",
            "Loss: 0.3259\n",
            "Époque 37/50\n",
            "Loss: 0.3118\n",
            "Époque 38/50\n",
            "Loss: 0.2983\n",
            "Époque 39/50\n",
            "Loss: 0.2853\n",
            "Époque 40/50\n",
            "Loss: 0.2729\n",
            "Époque 41/50\n",
            "Loss: 0.2610\n",
            "Époque 42/50\n",
            "Loss: 0.2496\n",
            "Époque 43/50\n",
            "Loss: 0.2387\n",
            "Époque 44/50\n",
            "Loss: 0.2284\n",
            "Époque 45/50\n",
            "Loss: 0.2185\n",
            "Époque 46/50\n",
            "Loss: 0.2092\n",
            "Époque 47/50\n",
            "Loss: 0.2002\n",
            "Époque 48/50\n",
            "Loss: 0.1917\n",
            "Époque 49/50\n",
            "Loss: 0.1837\n",
            "Époque 50/50\n",
            "Loss: 0.1760\n",
            "Client 2/3\n",
            "Époque 1/50\n",
            "Loss: 1.5859\n",
            "Époque 2/50\n",
            "Loss: 1.4071\n",
            "Époque 3/50\n",
            "Loss: 1.3138\n",
            "Époque 4/50\n",
            "Loss: 1.2503\n",
            "Époque 5/50\n",
            "Loss: 1.1971\n",
            "Époque 6/50\n",
            "Loss: 1.1485\n",
            "Époque 7/50\n",
            "Loss: 1.1035\n",
            "Époque 8/50\n",
            "Loss: 1.0620\n",
            "Époque 9/50\n",
            "Loss: 1.0243\n",
            "Époque 10/50\n",
            "Loss: 0.9898\n",
            "Époque 11/50\n",
            "Loss: 0.9581\n",
            "Époque 12/50\n",
            "Loss: 0.9279\n",
            "Époque 13/50\n",
            "Loss: 0.8982\n",
            "Époque 14/50\n",
            "Loss: 0.8683\n",
            "Époque 15/50\n",
            "Loss: 0.8379\n",
            "Époque 16/50\n",
            "Loss: 0.8078\n",
            "Époque 17/50\n",
            "Loss: 0.7786\n",
            "Époque 18/50\n",
            "Loss: 0.7507\n",
            "Époque 19/50\n",
            "Loss: 0.7241\n",
            "Époque 20/50\n",
            "Loss: 0.6983\n",
            "Époque 21/50\n",
            "Loss: 0.6730\n",
            "Époque 22/50\n",
            "Loss: 0.6480\n",
            "Époque 23/50\n",
            "Loss: 0.6234\n",
            "Époque 24/50\n",
            "Loss: 0.5994\n",
            "Époque 25/50\n",
            "Loss: 0.5761\n",
            "Époque 26/50\n",
            "Loss: 0.5534\n",
            "Époque 27/50\n",
            "Loss: 0.5312\n",
            "Époque 28/50\n",
            "Loss: 0.5097\n",
            "Époque 29/50\n",
            "Loss: 0.4890\n",
            "Époque 30/50\n",
            "Loss: 0.4690\n",
            "Époque 31/50\n",
            "Loss: 0.4496\n",
            "Époque 32/50\n",
            "Loss: 0.4309\n",
            "Époque 33/50\n",
            "Loss: 0.4130\n",
            "Époque 34/50\n",
            "Loss: 0.3958\n",
            "Époque 35/50\n",
            "Loss: 0.3794\n",
            "Époque 36/50\n",
            "Loss: 0.3638\n",
            "Époque 37/50\n",
            "Loss: 0.3487\n",
            "Époque 38/50\n",
            "Loss: 0.3342\n",
            "Époque 39/50\n",
            "Loss: 0.3205\n",
            "Époque 40/50\n",
            "Loss: 0.3073\n",
            "Époque 41/50\n",
            "Loss: 0.2948\n",
            "Époque 42/50\n",
            "Loss: 0.2830\n",
            "Époque 43/50\n",
            "Loss: 0.2716\n",
            "Époque 44/50\n",
            "Loss: 0.2609\n",
            "Époque 45/50\n",
            "Loss: 0.2508\n",
            "Époque 46/50\n",
            "Loss: 0.2411\n",
            "Époque 47/50\n",
            "Loss: 0.2319\n",
            "Époque 48/50\n",
            "Loss: 0.2232\n",
            "Époque 49/50\n",
            "Loss: 0.2148\n",
            "Époque 50/50\n",
            "Loss: 0.2069\n",
            "Époque 1/50\n",
            "Loss: 1.5859\n",
            "Époque 2/50\n",
            "Loss: 1.4071\n",
            "Époque 3/50\n",
            "Loss: 1.3138\n",
            "Époque 4/50\n",
            "Loss: 1.2503\n",
            "Époque 5/50\n",
            "Loss: 1.1971\n",
            "Époque 6/50\n",
            "Loss: 1.1485\n",
            "Époque 7/50\n",
            "Loss: 1.1035\n",
            "Époque 8/50\n",
            "Loss: 1.0620\n",
            "Époque 9/50\n",
            "Loss: 1.0243\n",
            "Époque 10/50\n",
            "Loss: 0.9898\n",
            "Époque 11/50\n",
            "Loss: 0.9581\n",
            "Époque 12/50\n",
            "Loss: 0.9279\n",
            "Époque 13/50\n",
            "Loss: 0.8982\n",
            "Époque 14/50\n",
            "Loss: 0.8683\n",
            "Époque 15/50\n",
            "Loss: 0.8379\n",
            "Époque 16/50\n",
            "Loss: 0.8078\n",
            "Époque 17/50\n",
            "Loss: 0.7786\n",
            "Époque 18/50\n",
            "Loss: 0.7507\n",
            "Époque 19/50\n",
            "Loss: 0.7241\n",
            "Époque 20/50\n",
            "Loss: 0.6983\n",
            "Époque 21/50\n",
            "Loss: 0.6730\n",
            "Époque 22/50\n",
            "Loss: 0.6480\n",
            "Époque 23/50\n",
            "Loss: 0.6234\n",
            "Époque 24/50\n",
            "Loss: 0.5994\n",
            "Époque 25/50\n",
            "Loss: 0.5761\n",
            "Époque 26/50\n",
            "Loss: 0.5534\n",
            "Époque 27/50\n",
            "Loss: 0.5312\n",
            "Époque 28/50\n",
            "Loss: 0.5097\n",
            "Époque 29/50\n",
            "Loss: 0.4890\n",
            "Époque 30/50\n",
            "Loss: 0.4690\n",
            "Époque 31/50\n",
            "Loss: 0.4496\n",
            "Époque 32/50\n",
            "Loss: 0.4309\n",
            "Époque 33/50\n",
            "Loss: 0.4130\n",
            "Époque 34/50\n",
            "Loss: 0.3958\n",
            "Époque 35/50\n",
            "Loss: 0.3794\n",
            "Époque 36/50\n",
            "Loss: 0.3638\n",
            "Époque 37/50\n",
            "Loss: 0.3487\n",
            "Époque 38/50\n",
            "Loss: 0.3342\n",
            "Époque 39/50\n",
            "Loss: 0.3205\n",
            "Époque 40/50\n",
            "Loss: 0.3073\n",
            "Époque 41/50\n",
            "Loss: 0.2948\n",
            "Époque 42/50\n",
            "Loss: 0.2830\n",
            "Époque 43/50\n",
            "Loss: 0.2716\n",
            "Époque 44/50\n",
            "Loss: 0.2609\n",
            "Époque 45/50\n",
            "Loss: 0.2508\n",
            "Époque 46/50\n",
            "Loss: 0.2411\n",
            "Époque 47/50\n",
            "Loss: 0.2319\n",
            "Époque 48/50\n",
            "Loss: 0.2232\n",
            "Époque 49/50\n",
            "Loss: 0.2148\n",
            "Époque 50/50\n",
            "Loss: 0.2069\n",
            "Époque 1/50\n",
            "Loss: 1.5859\n",
            "Époque 2/50\n",
            "Loss: 1.4071\n",
            "Époque 3/50\n",
            "Loss: 1.3138\n",
            "Époque 4/50\n",
            "Loss: 1.2503\n",
            "Époque 5/50\n",
            "Loss: 1.1971\n",
            "Époque 6/50\n",
            "Loss: 1.1485\n",
            "Époque 7/50\n",
            "Loss: 1.1035\n",
            "Époque 8/50\n",
            "Loss: 1.0620\n",
            "Époque 9/50\n",
            "Loss: 1.0243\n",
            "Époque 10/50\n",
            "Loss: 0.9898\n",
            "Époque 11/50\n",
            "Loss: 0.9581\n",
            "Époque 12/50\n",
            "Loss: 0.9279\n",
            "Époque 13/50\n",
            "Loss: 0.8982\n",
            "Époque 14/50\n",
            "Loss: 0.8683\n",
            "Époque 15/50\n",
            "Loss: 0.8379\n",
            "Époque 16/50\n",
            "Loss: 0.8078\n",
            "Époque 17/50\n",
            "Loss: 0.7786\n",
            "Époque 18/50\n",
            "Loss: 0.7507\n",
            "Époque 19/50\n",
            "Loss: 0.7241\n",
            "Époque 20/50\n",
            "Loss: 0.6983\n",
            "Époque 21/50\n",
            "Loss: 0.6730\n",
            "Époque 22/50\n",
            "Loss: 0.6480\n",
            "Époque 23/50\n",
            "Loss: 0.6234\n",
            "Époque 24/50\n",
            "Loss: 0.5994\n",
            "Époque 25/50\n",
            "Loss: 0.5761\n",
            "Époque 26/50\n",
            "Loss: 0.5534\n",
            "Époque 27/50\n",
            "Loss: 0.5312\n",
            "Époque 28/50\n",
            "Loss: 0.5097\n",
            "Époque 29/50\n",
            "Loss: 0.4890\n",
            "Époque 30/50\n",
            "Loss: 0.4690\n",
            "Époque 31/50\n",
            "Loss: 0.4496\n",
            "Époque 32/50\n",
            "Loss: 0.4309\n",
            "Époque 33/50\n",
            "Loss: 0.4130\n",
            "Époque 34/50\n",
            "Loss: 0.3958\n",
            "Époque 35/50\n",
            "Loss: 0.3794\n",
            "Époque 36/50\n",
            "Loss: 0.3638\n",
            "Époque 37/50\n",
            "Loss: 0.3487\n",
            "Époque 38/50\n",
            "Loss: 0.3342\n",
            "Époque 39/50\n",
            "Loss: 0.3205\n",
            "Époque 40/50\n",
            "Loss: 0.3073\n",
            "Époque 41/50\n",
            "Loss: 0.2948\n",
            "Époque 42/50\n",
            "Loss: 0.2830\n",
            "Époque 43/50\n",
            "Loss: 0.2716\n",
            "Époque 44/50\n",
            "Loss: 0.2609\n",
            "Époque 45/50\n",
            "Loss: 0.2508\n",
            "Époque 46/50\n",
            "Loss: 0.2411\n",
            "Époque 47/50\n",
            "Loss: 0.2319\n",
            "Époque 48/50\n",
            "Loss: 0.2232\n",
            "Époque 49/50\n",
            "Loss: 0.2148\n",
            "Époque 50/50\n",
            "Loss: 0.2069\n",
            "Époque 1/50\n",
            "Loss: 1.5859\n",
            "Époque 2/50\n",
            "Loss: 1.4071\n",
            "Époque 3/50\n",
            "Loss: 1.3138\n",
            "Époque 4/50\n",
            "Loss: 1.2503\n",
            "Époque 5/50\n",
            "Loss: 1.1971\n",
            "Époque 6/50\n",
            "Loss: 1.1485\n",
            "Époque 7/50\n",
            "Loss: 1.1035\n",
            "Époque 8/50\n",
            "Loss: 1.0620\n",
            "Époque 9/50\n",
            "Loss: 1.0243\n",
            "Époque 10/50\n",
            "Loss: 0.9898\n",
            "Époque 11/50\n",
            "Loss: 0.9581\n",
            "Époque 12/50\n",
            "Loss: 0.9279\n",
            "Époque 13/50\n",
            "Loss: 0.8982\n",
            "Époque 14/50\n",
            "Loss: 0.8683\n",
            "Époque 15/50\n",
            "Loss: 0.8379\n",
            "Époque 16/50\n",
            "Loss: 0.8078\n",
            "Époque 17/50\n",
            "Loss: 0.7786\n",
            "Époque 18/50\n",
            "Loss: 0.7507\n",
            "Époque 19/50\n",
            "Loss: 0.7241\n",
            "Époque 20/50\n",
            "Loss: 0.6983\n",
            "Époque 21/50\n",
            "Loss: 0.6730\n",
            "Époque 22/50\n",
            "Loss: 0.6480\n",
            "Époque 23/50\n",
            "Loss: 0.6234\n",
            "Époque 24/50\n",
            "Loss: 0.5994\n",
            "Époque 25/50\n",
            "Loss: 0.5761\n",
            "Époque 26/50\n",
            "Loss: 0.5534\n",
            "Époque 27/50\n",
            "Loss: 0.5312\n",
            "Époque 28/50\n",
            "Loss: 0.5097\n",
            "Époque 29/50\n",
            "Loss: 0.4890\n",
            "Époque 30/50\n",
            "Loss: 0.4690\n",
            "Époque 31/50\n",
            "Loss: 0.4496\n",
            "Époque 32/50\n",
            "Loss: 0.4309\n",
            "Époque 33/50\n",
            "Loss: 0.4130\n",
            "Époque 34/50\n",
            "Loss: 0.3958\n",
            "Époque 35/50\n",
            "Loss: 0.3794\n",
            "Époque 36/50\n",
            "Loss: 0.3638\n",
            "Époque 37/50\n",
            "Loss: 0.3487\n",
            "Époque 38/50\n",
            "Loss: 0.3342\n",
            "Époque 39/50\n",
            "Loss: 0.3205\n",
            "Époque 40/50\n",
            "Loss: 0.3073\n",
            "Époque 41/50\n",
            "Loss: 0.2948\n",
            "Époque 42/50\n",
            "Loss: 0.2830\n",
            "Époque 43/50\n",
            "Loss: 0.2716\n",
            "Époque 44/50\n",
            "Loss: 0.2609\n",
            "Époque 45/50\n",
            "Loss: 0.2508\n",
            "Époque 46/50\n",
            "Loss: 0.2411\n",
            "Époque 47/50\n",
            "Loss: 0.2319\n",
            "Époque 48/50\n",
            "Loss: 0.2232\n",
            "Époque 49/50\n",
            "Loss: 0.2148\n",
            "Époque 50/50\n",
            "Loss: 0.2069\n",
            "Époque 1/50\n",
            "Loss: 1.5859\n",
            "Époque 2/50\n",
            "Loss: 1.4071\n",
            "Époque 3/50\n",
            "Loss: 1.3138\n",
            "Époque 4/50\n",
            "Loss: 1.2503\n",
            "Époque 5/50\n",
            "Loss: 1.1971\n",
            "Époque 6/50\n",
            "Loss: 1.1485\n",
            "Époque 7/50\n",
            "Loss: 1.1035\n",
            "Époque 8/50\n",
            "Loss: 1.0620\n",
            "Époque 9/50\n",
            "Loss: 1.0243\n",
            "Époque 10/50\n",
            "Loss: 0.9898\n",
            "Époque 11/50\n",
            "Loss: 0.9581\n",
            "Époque 12/50\n",
            "Loss: 0.9279\n",
            "Époque 13/50\n",
            "Loss: 0.8982\n",
            "Époque 14/50\n",
            "Loss: 0.8683\n",
            "Époque 15/50\n",
            "Loss: 0.8379\n",
            "Époque 16/50\n",
            "Loss: 0.8078\n",
            "Époque 17/50\n",
            "Loss: 0.7786\n",
            "Époque 18/50\n",
            "Loss: 0.7507\n",
            "Époque 19/50\n",
            "Loss: 0.7241\n",
            "Époque 20/50\n",
            "Loss: 0.6983\n",
            "Époque 21/50\n",
            "Loss: 0.6730\n",
            "Époque 22/50\n",
            "Loss: 0.6480\n",
            "Époque 23/50\n",
            "Loss: 0.6234\n",
            "Époque 24/50\n",
            "Loss: 0.5994\n",
            "Époque 25/50\n",
            "Loss: 0.5761\n",
            "Époque 26/50\n",
            "Loss: 0.5534\n",
            "Époque 27/50\n",
            "Loss: 0.5312\n",
            "Époque 28/50\n",
            "Loss: 0.5097\n",
            "Époque 29/50\n",
            "Loss: 0.4890\n",
            "Époque 30/50\n",
            "Loss: 0.4690\n",
            "Époque 31/50\n",
            "Loss: 0.4496\n",
            "Époque 32/50\n",
            "Loss: 0.4309\n",
            "Époque 33/50\n",
            "Loss: 0.4130\n",
            "Époque 34/50\n",
            "Loss: 0.3958\n",
            "Époque 35/50\n",
            "Loss: 0.3794\n",
            "Époque 36/50\n",
            "Loss: 0.3638\n",
            "Époque 37/50\n",
            "Loss: 0.3487\n",
            "Époque 38/50\n",
            "Loss: 0.3342\n",
            "Époque 39/50\n",
            "Loss: 0.3205\n",
            "Époque 40/50\n",
            "Loss: 0.3073\n",
            "Époque 41/50\n",
            "Loss: 0.2948\n",
            "Époque 42/50\n",
            "Loss: 0.2830\n",
            "Époque 43/50\n",
            "Loss: 0.2716\n",
            "Époque 44/50\n",
            "Loss: 0.2609\n",
            "Époque 45/50\n",
            "Loss: 0.2508\n",
            "Époque 46/50\n",
            "Loss: 0.2411\n",
            "Époque 47/50\n",
            "Loss: 0.2319\n",
            "Époque 48/50\n",
            "Loss: 0.2232\n",
            "Époque 49/50\n",
            "Loss: 0.2148\n",
            "Époque 50/50\n",
            "Loss: 0.2069\n",
            "Époque 1/50\n",
            "Loss: 1.5859\n",
            "Époque 2/50\n",
            "Loss: 1.4071\n",
            "Époque 3/50\n",
            "Loss: 1.3138\n",
            "Époque 4/50\n",
            "Loss: 1.2503\n",
            "Époque 5/50\n",
            "Loss: 1.1971\n",
            "Époque 6/50\n",
            "Loss: 1.1485\n",
            "Époque 7/50\n",
            "Loss: 1.1035\n",
            "Époque 8/50\n",
            "Loss: 1.0620\n",
            "Époque 9/50\n",
            "Loss: 1.0243\n",
            "Époque 10/50\n",
            "Loss: 0.9898\n",
            "Époque 11/50\n",
            "Loss: 0.9581\n",
            "Époque 12/50\n",
            "Loss: 0.9279\n",
            "Époque 13/50\n",
            "Loss: 0.8982\n",
            "Époque 14/50\n",
            "Loss: 0.8683\n",
            "Époque 15/50\n",
            "Loss: 0.8379\n",
            "Époque 16/50\n",
            "Loss: 0.8078\n",
            "Époque 17/50\n",
            "Loss: 0.7786\n",
            "Époque 18/50\n",
            "Loss: 0.7507\n",
            "Époque 19/50\n",
            "Loss: 0.7241\n",
            "Époque 20/50\n",
            "Loss: 0.6983\n",
            "Époque 21/50\n",
            "Loss: 0.6730\n",
            "Époque 22/50\n",
            "Loss: 0.6480\n",
            "Époque 23/50\n",
            "Loss: 0.6234\n",
            "Époque 24/50\n",
            "Loss: 0.5994\n",
            "Époque 25/50\n",
            "Loss: 0.5761\n",
            "Époque 26/50\n",
            "Loss: 0.5534\n",
            "Époque 27/50\n",
            "Loss: 0.5312\n",
            "Époque 28/50\n",
            "Loss: 0.5097\n",
            "Époque 29/50\n",
            "Loss: 0.4890\n",
            "Époque 30/50\n",
            "Loss: 0.4690\n",
            "Époque 31/50\n",
            "Loss: 0.4496\n",
            "Époque 32/50\n",
            "Loss: 0.4309\n",
            "Époque 33/50\n",
            "Loss: 0.4130\n",
            "Époque 34/50\n",
            "Loss: 0.3958\n",
            "Époque 35/50\n",
            "Loss: 0.3794\n",
            "Époque 36/50\n",
            "Loss: 0.3638\n",
            "Époque 37/50\n",
            "Loss: 0.3487\n",
            "Époque 38/50\n",
            "Loss: 0.3342\n",
            "Époque 39/50\n",
            "Loss: 0.3205\n",
            "Époque 40/50\n",
            "Loss: 0.3073\n",
            "Époque 41/50\n",
            "Loss: 0.2948\n",
            "Époque 42/50\n",
            "Loss: 0.2830\n",
            "Époque 43/50\n",
            "Loss: 0.2716\n",
            "Époque 44/50\n",
            "Loss: 0.2609\n",
            "Époque 45/50\n",
            "Loss: 0.2508\n",
            "Époque 46/50\n",
            "Loss: 0.2411\n",
            "Époque 47/50\n",
            "Loss: 0.2319\n",
            "Époque 48/50\n",
            "Loss: 0.2232\n",
            "Époque 49/50\n",
            "Loss: 0.2148\n",
            "Époque 50/50\n",
            "Loss: 0.2069\n",
            "Client 3/3\n",
            "Époque 1/50\n",
            "Loss: 1.6214\n",
            "Époque 2/50\n",
            "Loss: 1.4265\n",
            "Époque 3/50\n",
            "Loss: 1.3200\n",
            "Époque 4/50\n",
            "Loss: 1.2502\n",
            "Époque 5/50\n",
            "Loss: 1.1956\n",
            "Époque 6/50\n",
            "Loss: 1.1477\n",
            "Époque 7/50\n",
            "Loss: 1.1037\n",
            "Époque 8/50\n",
            "Loss: 1.0624\n",
            "Époque 9/50\n",
            "Loss: 1.0231\n",
            "Époque 10/50\n",
            "Loss: 0.9857\n",
            "Époque 11/50\n",
            "Loss: 0.9501\n",
            "Époque 12/50\n",
            "Loss: 0.9161\n",
            "Époque 13/50\n",
            "Loss: 0.8834\n",
            "Époque 14/50\n",
            "Loss: 0.8516\n",
            "Époque 15/50\n",
            "Loss: 0.8203\n",
            "Époque 16/50\n",
            "Loss: 0.7895\n",
            "Époque 17/50\n",
            "Loss: 0.7590\n",
            "Époque 18/50\n",
            "Loss: 0.7291\n",
            "Époque 19/50\n",
            "Loss: 0.7000\n",
            "Époque 20/50\n",
            "Loss: 0.6718\n",
            "Époque 21/50\n",
            "Loss: 0.6443\n",
            "Époque 22/50\n",
            "Loss: 0.6175\n",
            "Époque 23/50\n",
            "Loss: 0.5917\n",
            "Époque 24/50\n",
            "Loss: 0.5669\n",
            "Époque 25/50\n",
            "Loss: 0.5432\n",
            "Époque 26/50\n",
            "Loss: 0.5203\n",
            "Époque 27/50\n",
            "Loss: 0.4982\n",
            "Époque 28/50\n",
            "Loss: 0.4769\n",
            "Époque 29/50\n",
            "Loss: 0.4563\n",
            "Époque 30/50\n",
            "Loss: 0.4365\n",
            "Époque 31/50\n",
            "Loss: 0.4175\n",
            "Époque 32/50\n",
            "Loss: 0.3992\n",
            "Époque 33/50\n",
            "Loss: 0.3816\n",
            "Époque 34/50\n",
            "Loss: 0.3649\n",
            "Époque 35/50\n",
            "Loss: 0.3488\n",
            "Époque 36/50\n",
            "Loss: 0.3333\n",
            "Époque 37/50\n",
            "Loss: 0.3185\n",
            "Époque 38/50\n",
            "Loss: 0.3042\n",
            "Époque 39/50\n",
            "Loss: 0.2906\n",
            "Époque 40/50\n",
            "Loss: 0.2777\n",
            "Époque 41/50\n",
            "Loss: 0.2654\n",
            "Époque 42/50\n",
            "Loss: 0.2536\n",
            "Époque 43/50\n",
            "Loss: 0.2424\n",
            "Époque 44/50\n",
            "Loss: 0.2317\n",
            "Époque 45/50\n",
            "Loss: 0.2215\n",
            "Époque 46/50\n",
            "Loss: 0.2118\n",
            "Époque 47/50\n",
            "Loss: 0.2025\n",
            "Époque 48/50\n",
            "Loss: 0.1937\n",
            "Époque 49/50\n",
            "Loss: 0.1854\n",
            "Époque 50/50\n",
            "Loss: 0.1774\n",
            "Époque 1/50\n",
            "Loss: 1.6214\n",
            "Époque 2/50\n",
            "Loss: 1.4265\n",
            "Époque 3/50\n",
            "Loss: 1.3200\n",
            "Époque 4/50\n",
            "Loss: 1.2502\n",
            "Époque 5/50\n",
            "Loss: 1.1956\n",
            "Époque 6/50\n",
            "Loss: 1.1477\n",
            "Époque 7/50\n",
            "Loss: 1.1037\n",
            "Époque 8/50\n",
            "Loss: 1.0624\n",
            "Époque 9/50\n",
            "Loss: 1.0231\n",
            "Époque 10/50\n",
            "Loss: 0.9857\n",
            "Époque 11/50\n",
            "Loss: 0.9501\n",
            "Époque 12/50\n",
            "Loss: 0.9161\n",
            "Époque 13/50\n",
            "Loss: 0.8834\n",
            "Époque 14/50\n",
            "Loss: 0.8516\n",
            "Époque 15/50\n",
            "Loss: 0.8203\n",
            "Époque 16/50\n",
            "Loss: 0.7895\n",
            "Époque 17/50\n",
            "Loss: 0.7590\n",
            "Époque 18/50\n",
            "Loss: 0.7291\n",
            "Époque 19/50\n",
            "Loss: 0.7000\n",
            "Époque 20/50\n",
            "Loss: 0.6718\n",
            "Époque 21/50\n",
            "Loss: 0.6443\n",
            "Époque 22/50\n",
            "Loss: 0.6175\n",
            "Époque 23/50\n",
            "Loss: 0.5917\n",
            "Époque 24/50\n",
            "Loss: 0.5669\n",
            "Époque 25/50\n",
            "Loss: 0.5432\n",
            "Époque 26/50\n",
            "Loss: 0.5203\n",
            "Époque 27/50\n",
            "Loss: 0.4982\n",
            "Époque 28/50\n",
            "Loss: 0.4769\n",
            "Époque 29/50\n",
            "Loss: 0.4563\n",
            "Époque 30/50\n",
            "Loss: 0.4365\n",
            "Époque 31/50\n",
            "Loss: 0.4175\n",
            "Époque 32/50\n",
            "Loss: 0.3992\n",
            "Époque 33/50\n",
            "Loss: 0.3816\n",
            "Époque 34/50\n",
            "Loss: 0.3649\n",
            "Époque 35/50\n",
            "Loss: 0.3488\n",
            "Époque 36/50\n",
            "Loss: 0.3333\n",
            "Époque 37/50\n",
            "Loss: 0.3185\n",
            "Époque 38/50\n",
            "Loss: 0.3042\n",
            "Époque 39/50\n",
            "Loss: 0.2906\n",
            "Époque 40/50\n",
            "Loss: 0.2777\n",
            "Époque 41/50\n",
            "Loss: 0.2654\n",
            "Époque 42/50\n",
            "Loss: 0.2536\n",
            "Époque 43/50\n",
            "Loss: 0.2424\n",
            "Époque 44/50\n",
            "Loss: 0.2317\n",
            "Époque 45/50\n",
            "Loss: 0.2215\n",
            "Époque 46/50\n",
            "Loss: 0.2118\n",
            "Époque 47/50\n",
            "Loss: 0.2025\n",
            "Époque 48/50\n",
            "Loss: 0.1937\n",
            "Époque 49/50\n",
            "Loss: 0.1854\n",
            "Époque 50/50\n",
            "Loss: 0.1774\n",
            "Époque 1/50\n",
            "Loss: 1.6214\n",
            "Époque 2/50\n",
            "Loss: 1.4265\n",
            "Époque 3/50\n",
            "Loss: 1.3200\n",
            "Époque 4/50\n",
            "Loss: 1.2502\n",
            "Époque 5/50\n",
            "Loss: 1.1956\n",
            "Époque 6/50\n",
            "Loss: 1.1477\n",
            "Époque 7/50\n",
            "Loss: 1.1037\n",
            "Époque 8/50\n",
            "Loss: 1.0624\n",
            "Époque 9/50\n",
            "Loss: 1.0231\n",
            "Époque 10/50\n",
            "Loss: 0.9857\n",
            "Époque 11/50\n",
            "Loss: 0.9501\n",
            "Époque 12/50\n",
            "Loss: 0.9161\n",
            "Époque 13/50\n",
            "Loss: 0.8834\n",
            "Époque 14/50\n",
            "Loss: 0.8516\n",
            "Époque 15/50\n",
            "Loss: 0.8203\n",
            "Époque 16/50\n",
            "Loss: 0.7895\n",
            "Époque 17/50\n",
            "Loss: 0.7590\n",
            "Époque 18/50\n",
            "Loss: 0.7291\n",
            "Époque 19/50\n",
            "Loss: 0.7000\n",
            "Époque 20/50\n",
            "Loss: 0.6718\n",
            "Époque 21/50\n",
            "Loss: 0.6443\n",
            "Époque 22/50\n",
            "Loss: 0.6175\n",
            "Époque 23/50\n",
            "Loss: 0.5917\n",
            "Époque 24/50\n",
            "Loss: 0.5669\n",
            "Époque 25/50\n",
            "Loss: 0.5432\n",
            "Époque 26/50\n",
            "Loss: 0.5203\n",
            "Époque 27/50\n",
            "Loss: 0.4982\n",
            "Époque 28/50\n",
            "Loss: 0.4769\n",
            "Époque 29/50\n",
            "Loss: 0.4563\n",
            "Époque 30/50\n",
            "Loss: 0.4365\n",
            "Époque 31/50\n",
            "Loss: 0.4175\n",
            "Époque 32/50\n",
            "Loss: 0.3992\n",
            "Époque 33/50\n",
            "Loss: 0.3816\n",
            "Époque 34/50\n",
            "Loss: 0.3649\n",
            "Époque 35/50\n",
            "Loss: 0.3488\n",
            "Époque 36/50\n",
            "Loss: 0.3333\n",
            "Époque 37/50\n",
            "Loss: 0.3185\n",
            "Époque 38/50\n",
            "Loss: 0.3042\n",
            "Époque 39/50\n",
            "Loss: 0.2906\n",
            "Époque 40/50\n",
            "Loss: 0.2777\n",
            "Époque 41/50\n",
            "Loss: 0.2654\n",
            "Époque 42/50\n",
            "Loss: 0.2536\n",
            "Époque 43/50\n",
            "Loss: 0.2424\n",
            "Époque 44/50\n",
            "Loss: 0.2317\n",
            "Époque 45/50\n",
            "Loss: 0.2215\n",
            "Époque 46/50\n",
            "Loss: 0.2118\n",
            "Époque 47/50\n",
            "Loss: 0.2025\n",
            "Époque 48/50\n",
            "Loss: 0.1937\n",
            "Époque 49/50\n",
            "Loss: 0.1854\n",
            "Époque 50/50\n",
            "Loss: 0.1774\n",
            "Époque 1/50\n",
            "Loss: 1.6214\n",
            "Époque 2/50\n",
            "Loss: 1.4265\n",
            "Époque 3/50\n",
            "Loss: 1.3200\n",
            "Époque 4/50\n",
            "Loss: 1.2502\n",
            "Époque 5/50\n",
            "Loss: 1.1956\n",
            "Époque 6/50\n",
            "Loss: 1.1477\n",
            "Époque 7/50\n",
            "Loss: 1.1037\n",
            "Époque 8/50\n",
            "Loss: 1.0624\n",
            "Époque 9/50\n",
            "Loss: 1.0231\n",
            "Époque 10/50\n",
            "Loss: 0.9857\n",
            "Époque 11/50\n",
            "Loss: 0.9501\n",
            "Époque 12/50\n",
            "Loss: 0.9161\n",
            "Époque 13/50\n",
            "Loss: 0.8834\n",
            "Époque 14/50\n",
            "Loss: 0.8516\n",
            "Époque 15/50\n",
            "Loss: 0.8203\n",
            "Époque 16/50\n",
            "Loss: 0.7895\n",
            "Époque 17/50\n",
            "Loss: 0.7590\n",
            "Époque 18/50\n",
            "Loss: 0.7291\n",
            "Époque 19/50\n",
            "Loss: 0.7000\n",
            "Époque 20/50\n",
            "Loss: 0.6718\n",
            "Époque 21/50\n",
            "Loss: 0.6443\n",
            "Époque 22/50\n",
            "Loss: 0.6175\n",
            "Époque 23/50\n",
            "Loss: 0.5917\n",
            "Époque 24/50\n",
            "Loss: 0.5669\n",
            "Époque 25/50\n",
            "Loss: 0.5432\n",
            "Époque 26/50\n",
            "Loss: 0.5203\n",
            "Époque 27/50\n",
            "Loss: 0.4982\n",
            "Époque 28/50\n",
            "Loss: 0.4769\n",
            "Époque 29/50\n",
            "Loss: 0.4563\n",
            "Époque 30/50\n",
            "Loss: 0.4365\n",
            "Époque 31/50\n",
            "Loss: 0.4175\n",
            "Époque 32/50\n",
            "Loss: 0.3992\n",
            "Époque 33/50\n",
            "Loss: 0.3816\n",
            "Époque 34/50\n",
            "Loss: 0.3649\n",
            "Époque 35/50\n",
            "Loss: 0.3488\n",
            "Époque 36/50\n",
            "Loss: 0.3333\n",
            "Époque 37/50\n",
            "Loss: 0.3185\n",
            "Époque 38/50\n",
            "Loss: 0.3042\n",
            "Époque 39/50\n",
            "Loss: 0.2906\n",
            "Époque 40/50\n",
            "Loss: 0.2777\n",
            "Époque 41/50\n",
            "Loss: 0.2654\n",
            "Époque 42/50\n",
            "Loss: 0.2536\n",
            "Époque 43/50\n",
            "Loss: 0.2424\n",
            "Époque 44/50\n",
            "Loss: 0.2317\n",
            "Époque 45/50\n",
            "Loss: 0.2215\n",
            "Époque 46/50\n",
            "Loss: 0.2118\n",
            "Époque 47/50\n",
            "Loss: 0.2025\n",
            "Époque 48/50\n",
            "Loss: 0.1937\n",
            "Époque 49/50\n",
            "Loss: 0.1854\n",
            "Époque 50/50\n",
            "Loss: 0.1774\n",
            "Époque 1/50\n",
            "Loss: 1.6214\n",
            "Époque 2/50\n",
            "Loss: 1.4265\n",
            "Époque 3/50\n",
            "Loss: 1.3200\n",
            "Époque 4/50\n",
            "Loss: 1.2502\n",
            "Époque 5/50\n",
            "Loss: 1.1956\n",
            "Époque 6/50\n",
            "Loss: 1.1477\n",
            "Époque 7/50\n",
            "Loss: 1.1037\n",
            "Époque 8/50\n",
            "Loss: 1.0624\n",
            "Époque 9/50\n",
            "Loss: 1.0231\n",
            "Époque 10/50\n",
            "Loss: 0.9857\n",
            "Époque 11/50\n",
            "Loss: 0.9501\n",
            "Époque 12/50\n",
            "Loss: 0.9161\n",
            "Époque 13/50\n",
            "Loss: 0.8834\n",
            "Époque 14/50\n",
            "Loss: 0.8516\n",
            "Époque 15/50\n",
            "Loss: 0.8203\n",
            "Époque 16/50\n",
            "Loss: 0.7895\n",
            "Époque 17/50\n",
            "Loss: 0.7590\n",
            "Époque 18/50\n",
            "Loss: 0.7291\n",
            "Époque 19/50\n",
            "Loss: 0.7000\n",
            "Époque 20/50\n",
            "Loss: 0.6718\n",
            "Époque 21/50\n",
            "Loss: 0.6443\n",
            "Époque 22/50\n",
            "Loss: 0.6175\n",
            "Époque 23/50\n",
            "Loss: 0.5917\n",
            "Époque 24/50\n",
            "Loss: 0.5669\n",
            "Époque 25/50\n",
            "Loss: 0.5432\n",
            "Époque 26/50\n",
            "Loss: 0.5203\n",
            "Époque 27/50\n",
            "Loss: 0.4982\n",
            "Époque 28/50\n",
            "Loss: 0.4769\n",
            "Époque 29/50\n",
            "Loss: 0.4563\n",
            "Époque 30/50\n",
            "Loss: 0.4365\n",
            "Époque 31/50\n",
            "Loss: 0.4175\n",
            "Époque 32/50\n",
            "Loss: 0.3992\n",
            "Époque 33/50\n",
            "Loss: 0.3816\n",
            "Époque 34/50\n",
            "Loss: 0.3649\n",
            "Époque 35/50\n",
            "Loss: 0.3488\n",
            "Époque 36/50\n",
            "Loss: 0.3333\n",
            "Époque 37/50\n",
            "Loss: 0.3185\n",
            "Époque 38/50\n",
            "Loss: 0.3042\n",
            "Époque 39/50\n",
            "Loss: 0.2906\n",
            "Époque 40/50\n",
            "Loss: 0.2777\n",
            "Époque 41/50\n",
            "Loss: 0.2654\n",
            "Époque 42/50\n",
            "Loss: 0.2536\n",
            "Époque 43/50\n",
            "Loss: 0.2424\n",
            "Époque 44/50\n",
            "Loss: 0.2317\n",
            "Époque 45/50\n",
            "Loss: 0.2215\n",
            "Époque 46/50\n",
            "Loss: 0.2118\n",
            "Époque 47/50\n",
            "Loss: 0.2025\n",
            "Époque 48/50\n",
            "Loss: 0.1937\n",
            "Époque 49/50\n",
            "Loss: 0.1854\n",
            "Époque 50/50\n",
            "Loss: 0.1774\n",
            "Époque 1/50\n",
            "Loss: 1.6214\n",
            "Époque 2/50\n",
            "Loss: 1.4265\n",
            "Époque 3/50\n",
            "Loss: 1.3200\n",
            "Époque 4/50\n",
            "Loss: 1.2502\n",
            "Époque 5/50\n",
            "Loss: 1.1956\n",
            "Époque 6/50\n",
            "Loss: 1.1477\n",
            "Époque 7/50\n",
            "Loss: 1.1037\n",
            "Époque 8/50\n",
            "Loss: 1.0624\n",
            "Époque 9/50\n",
            "Loss: 1.0231\n",
            "Époque 10/50\n",
            "Loss: 0.9857\n",
            "Époque 11/50\n",
            "Loss: 0.9501\n",
            "Époque 12/50\n",
            "Loss: 0.9161\n",
            "Époque 13/50\n",
            "Loss: 0.8834\n",
            "Époque 14/50\n",
            "Loss: 0.8516\n",
            "Époque 15/50\n",
            "Loss: 0.8203\n",
            "Époque 16/50\n",
            "Loss: 0.7895\n",
            "Époque 17/50\n",
            "Loss: 0.7590\n",
            "Époque 18/50\n",
            "Loss: 0.7291\n",
            "Époque 19/50\n",
            "Loss: 0.7000\n",
            "Époque 20/50\n",
            "Loss: 0.6718\n",
            "Époque 21/50\n",
            "Loss: 0.6443\n",
            "Époque 22/50\n",
            "Loss: 0.6175\n",
            "Époque 23/50\n",
            "Loss: 0.5917\n",
            "Époque 24/50\n",
            "Loss: 0.5669\n",
            "Époque 25/50\n",
            "Loss: 0.5432\n",
            "Époque 26/50\n",
            "Loss: 0.5203\n",
            "Époque 27/50\n",
            "Loss: 0.4982\n",
            "Époque 28/50\n",
            "Loss: 0.4769\n",
            "Époque 29/50\n",
            "Loss: 0.4563\n",
            "Époque 30/50\n",
            "Loss: 0.4365\n",
            "Époque 31/50\n",
            "Loss: 0.4175\n",
            "Époque 32/50\n",
            "Loss: 0.3992\n",
            "Époque 33/50\n",
            "Loss: 0.3816\n",
            "Époque 34/50\n",
            "Loss: 0.3649\n",
            "Époque 35/50\n",
            "Loss: 0.3488\n",
            "Époque 36/50\n",
            "Loss: 0.3333\n",
            "Époque 37/50\n",
            "Loss: 0.3185\n",
            "Époque 38/50\n",
            "Loss: 0.3042\n",
            "Époque 39/50\n",
            "Loss: 0.2906\n",
            "Époque 40/50\n",
            "Loss: 0.2777\n",
            "Époque 41/50\n",
            "Loss: 0.2654\n",
            "Époque 42/50\n",
            "Loss: 0.2536\n",
            "Époque 43/50\n",
            "Loss: 0.2424\n",
            "Époque 44/50\n",
            "Loss: 0.2317\n",
            "Époque 45/50\n",
            "Loss: 0.2215\n",
            "Époque 46/50\n",
            "Loss: 0.2118\n",
            "Époque 47/50\n",
            "Loss: 0.2025\n",
            "Époque 48/50\n",
            "Loss: 0.1937\n",
            "Époque 49/50\n",
            "Loss: 0.1854\n",
            "Époque 50/50\n",
            "Loss: 0.1774\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3331\u001b[39m\n\u001b[32m   3327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results, final_model_proposed, final_model_fedavg\n\u001b[32m   3330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3331\u001b[39m     \u001b[43mmain_two_phase\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3243\u001b[39m, in \u001b[36mmain_two_phase\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   3226\u001b[39m params = {\n\u001b[32m   3227\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mn_clients\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3\u001b[39m,            \u001b[38;5;66;03m# Nombre de clients\u001b[39;00m\n\u001b[32m   3228\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mn_epochs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m50\u001b[39m,             \u001b[38;5;66;03m# Nombre d'époques pour l'entraînement local\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3238\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33muse_synthetic_data\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Utiliser des données synthétiques\u001b[39;00m\n\u001b[32m   3239\u001b[39m }\n\u001b[32m   3242\u001b[39m  \u001b[38;5;66;03m# Exécuter l'expérience\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3243\u001b[39m results, final_model_proposed, final_model_fedavg = \u001b[43mfederated_main_two_phase_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3246\u001b[39m \u001b[38;5;66;03m# Afficher un tableau récapitulatif des performances\u001b[39;00m\n\u001b[32m   3247\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRÉCAPITULATIF DES PERFORMANCES :\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2580\u001b[39m, in \u001b[36mfederated_main_two_phase_local\u001b[39m\u001b[34m(n_clients, n_epochs, n_clusters, n_a, n_x, n_y, batch_size, sequence_length, num_transfer_epochs, learning_rate, n_communication_rounds, use_synthetic_data)\u001b[39m\n\u001b[32m   2577\u001b[39m     proposed_bandwidth.append(proposed_size)\n\u001b[32m   2579\u001b[39m \u001b[38;5;66;03m# Évaluation des deux modèles pour ce cycle\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2580\u001b[39m proposed_acc, proposed_loss = \u001b[43mevaluate_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2581\u001b[39m fedavg_acc, fedavg_loss = evaluate_lstm(X_test, Y_test, fedavg_params)\n\u001b[32m   2583\u001b[39m \u001b[38;5;66;03m# Stockage et affichage des résultats comme avant\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 433\u001b[39m, in \u001b[36mevaluate_lstm\u001b[39m\u001b[34m(X_test, Y_test, parameters)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m    432\u001b[39m a0 = np.zeros((n_a, X_test.shape[\u001b[32m1\u001b[39m]))\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m _, y_pred, _, _ = \u001b[43mlstm_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[38;5;66;03m# Calculer RMSE\u001b[39;00m\n\u001b[32m    436\u001b[39m rmse_val = np.sqrt(np.mean((y_pred - Y_test) ** \u001b[32m2\u001b[39m))\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 161\u001b[39m, in \u001b[36mlstm_forward\u001b[39m\u001b[34m(x, a0, parameters)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# Boucle sur tous les time-steps\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T_x):\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Mettre à jour a_next, c_next, calculer la prédiction et obtenir le cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     a_next, c_next, yt, cache = \u001b[43mlstm_cell_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_next\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_next\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# Sauvegarder les valeurs\u001b[39;00m\n\u001b[32m    163\u001b[39m     a[:, :, t] = a_next\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mlstm_cell_forward\u001b[39m\u001b[34m(xt, a_prev, c_prev, parameters)\u001b[39m\n\u001b[32m     61\u001b[39m concat[n_a:, :] = xt\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Calculer les valeurs pour ft, it, cct, c_next, ot, a_next\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m ft = \u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mbf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m it = sigmoid(np.matmul(Wi, concat) + bi)\n\u001b[32m     66\u001b[39m cct = np.tanh(np.matmul(Wc, concat) + bc)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36msigmoid\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msigmoid\u001b[39m(x):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m1\u001b[39m / (\u001b[32m1\u001b[39m + \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "#5413 paramètres transférés dans le cas usuel\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    xt -- Données d'entrée à time-step t, array de forme (n_x, m)\n",
        "    a_prev -- Etat caché précédent, array de forme (n_a, m)\n",
        "    c_prev -- Etat de la cellule précédente, array de forme (n_a, m)\n",
        "    parameters -- Dictionnaire Python contenant:\n",
        "                Wf -- Poids de la forget gate, array de forme (n_a, n_a + n_x)\n",
        "                bf -- Biais de la forget gate, array de forme (n_a, 1)\n",
        "                Wi -- Poids de l'update gate, array de forme (n_a, n_a + n_x)\n",
        "                bi -- Biais de l'update gate, array de forme (n_a, 1)\n",
        "                Wc -- Poids de la première \"tanh\", array de forme (n_a, n_a + n_x)\n",
        "                bc -- Biais de la première \"tanh\", array de forme (n_a, 1)\n",
        "                Wo -- Poids de l'output gate, array de forme (n_a, n_a + n_x)\n",
        "                bo -- Biais de l'output gate, array de forme (n_a, 1)\n",
        "                Wy -- Poids pour l'état caché, array de forme (n_y, n_a)\n",
        "                by -- Biais pour l'état caché, array de forme (n_y, 1)\n",
        "\n",
        "    Returns:\n",
        "    a_next -- Prochain état caché, array de forme (n_a, m)\n",
        "    c_next -- Prochain état de cellule, array de forme (n_a, m)\n",
        "    yt_pred -- Prédiction à time-step t, array de forme (n_y, m)\n",
        "    cache -- Tuple de valeurs pour la backpropagation\n",
        "    \"\"\"\n",
        "\n",
        "    # Récupérer les paramètres du dictionnaire\n",
        "    Wf = parameters[\"Wf\"]\n",
        "    bf = parameters[\"bf\"]\n",
        "    Wi = parameters[\"Wi\"]\n",
        "    bi = parameters[\"bi\"]\n",
        "    Wc = parameters[\"Wc\"]\n",
        "    bc = parameters[\"bc\"]\n",
        "    Wo = parameters[\"Wo\"]\n",
        "    bo = parameters[\"bo\"]\n",
        "    Wy = parameters[\"Wy\"]\n",
        "    by = parameters[\"by\"]\n",
        "\n",
        "    # Récupérer les dimensions\n",
        "    n_x, m = xt.shape\n",
        "    n_y, n_a = Wy.shape\n",
        "\n",
        "    # Concaténer a_prev et xt\n",
        "    concat = np.zeros((n_a + n_x, m))\n",
        "    concat[: n_a, :] = a_prev\n",
        "    concat[n_a:, :] = xt\n",
        "\n",
        "    # Calculer les valeurs pour ft, it, cct, c_next, ot, a_next\n",
        "    ft = sigmoid(np.matmul(Wf, concat) + bf)\n",
        "    it = sigmoid(np.matmul(Wi, concat) + bi)\n",
        "    cct = np.tanh(np.matmul(Wc, concat) + bc)\n",
        "    c_next = (ft * c_prev) + (it * cct)\n",
        "    ot = sigmoid(np.matmul(Wo, concat) + bo)\n",
        "    a_next = ot * np.tanh(c_next)\n",
        "\n",
        "    # Calculer la prédiction\n",
        "    yt_pred = softmax(np.matmul(Wy, a_next) + by)\n",
        "\n",
        "    # Stocker les valeurs pour la backpropagation\n",
        "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
        "\n",
        "    return a_next, c_next, yt_pred, cache\n",
        "\n",
        "def lstm_cell_backward(da_next, dc_next, cache):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    da_next -- Gradient du prochain état caché, array de forme (n_a, m)\n",
        "    dc_next -- Gradient du prochain état de cellule, array de forme (n_a, m)\n",
        "    cache -- Cache du forward pass\n",
        "\n",
        "    Returns:\n",
        "    gradients -- Dictionnaire contenant les gradients\n",
        "    \"\"\"\n",
        "\n",
        "    # Récupérer les informations du cache\n",
        "    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache\n",
        "\n",
        "    # Récupérer les dimensions\n",
        "    n_x, m = xt.shape\n",
        "    n_a, m = a_next.shape\n",
        "\n",
        "    # Calculer les dérivées des portes\n",
        "    dot = da_next * np.tanh(c_next) * ot * (1 - ot)\n",
        "    dcct = (dc_next * it + ot * (1 - np.square(np.tanh(c_next))) * it * da_next) * (1 - np.square(cct))\n",
        "    dit = (dc_next * cct + ot * (1 - np.square(np.tanh(c_next))) * cct * da_next) * it * (1 - it)\n",
        "    dft = (dc_next * c_prev + ot * (1 - np.square(np.tanh(c_next))) * c_prev * da_next) * ft * (1 - ft)\n",
        "\n",
        "    concat = np.concatenate((a_prev, xt), axis=0)\n",
        "\n",
        "    # Calculer les dérivées des paramètres\n",
        "    dWf = np.dot(dft, concat.T)\n",
        "    dWi = np.dot(dit, concat.T)\n",
        "    dWc = np.dot(dcct, concat.T)\n",
        "    dWo = np.dot(dot, concat.T)\n",
        "    dbf = np.sum(dft, axis=1, keepdims=True)\n",
        "    dbi = np.sum(dit, axis=1, keepdims=True)\n",
        "    dbc = np.sum(dcct, axis=1, keepdims=True)\n",
        "    dbo = np.sum(dot, axis=1, keepdims=True)\n",
        "\n",
        "    # Calculer les dérivées par rapport à l'état caché précédent, l'état de cellule précédent et l'entrée\n",
        "    da_prev = np.dot(parameters['Wf'][:, :n_a].T, dft) + np.dot(parameters['Wi'][:, :n_a].T, dit) + np.dot(\n",
        "        parameters['Wc'][:, :n_a].T, dcct) + np.dot(parameters['Wo'][:, :n_a].T, dot)\n",
        "    dc_prev = dc_next * ft + ot * (1 - np.square(np.tanh(c_next))) * ft * da_next\n",
        "    dxt = np.dot(parameters['Wf'][:, n_a:].T, dft) + np.dot(parameters['Wi'][:, n_a:].T, dit) + np.dot(\n",
        "        parameters['Wc'][:, n_a:].T, dcct) + np.dot(parameters['Wo'][:, n_a:].T, dot)\n",
        "\n",
        "    # Sauvegarder les gradients\n",
        "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev, \"dWf\": dWf, \"dbf\": dbf, \"dWi\": dWi, \"dbi\": dbi,\n",
        "                 \"dWc\": dWc, \"dbc\": dbc, \"dWo\": dWo, \"dbo\": dbo}\n",
        "\n",
        "    return gradients\n",
        "\n",
        "def lstm_forward(x, a0, parameters):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    x -- Données d'entrée pour chaque time-step, array de forme (n_x, m, T_x)\n",
        "    a0 -- État caché initial, array de forme (n_a, m)\n",
        "    parameters -- Dictionnaire Python des paramètres LSTM\n",
        "\n",
        "    Returns:\n",
        "    a -- États cachés pour chaque time-step, array de forme (n_a, m, T_x)\n",
        "    y -- Prédictions pour chaque time-step, array de forme (n_y, m, T_x)\n",
        "    c -- États de cellule pour chaque time-step, array de forme (n_a, m, T_x)\n",
        "    caches -- Tuple de valeurs pour la backpropagation\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialiser les caches\n",
        "    caches = []\n",
        "\n",
        "    # Récupérer les dimensions\n",
        "    n_x, m, T_x = x.shape\n",
        "    n_y, n_a = parameters[\"Wy\"].shape\n",
        "\n",
        "    # Initialiser a, c et y avec des zéros\n",
        "    a = np.zeros((n_a, m, T_x))\n",
        "    c = a.copy()\n",
        "    y = np.zeros((n_y, m, T_x))\n",
        "\n",
        "    # Initialiser a_next et c_next\n",
        "    a_next = a0\n",
        "    c_next = np.zeros(a_next.shape)\n",
        "\n",
        "    # Boucle sur tous les time-steps\n",
        "    for t in range(T_x):\n",
        "        # Mettre à jour a_next, c_next, calculer la prédiction et obtenir le cache\n",
        "        a_next, c_next, yt, cache = lstm_cell_forward(x[:, :, t], a_next, c_next, parameters)\n",
        "        # Sauvegarder les valeurs\n",
        "        a[:, :, t] = a_next\n",
        "        y[:, :, t] = yt\n",
        "        c[:, :, t] = c_next\n",
        "        # Ajouter le cache\n",
        "        caches.append(cache)\n",
        "\n",
        "    # Stocker les valeurs pour la backpropagation\n",
        "    caches = (caches, x)\n",
        "\n",
        "    return a, y, c, caches\n",
        "\n",
        "def lstm_backward(da, caches):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    da -- Gradient par rapport aux états cachés, array de forme (n_a, m, T_x)\n",
        "    caches -- Cache du forward pass\n",
        "\n",
        "    Returns:\n",
        "    gradients -- Dictionnaire Python contenant les gradients\n",
        "    \"\"\"\n",
        "\n",
        "    # Récupérer les valeurs du cache\n",
        "    (caches, x) = caches\n",
        "    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]\n",
        "\n",
        "    # Récupérer les dimensions\n",
        "    n_a, m, T_x = da.shape\n",
        "    n_x, m = x1.shape\n",
        "\n",
        "    # Initialiser les gradients\n",
        "    dx = np.zeros((n_x, m, T_x))\n",
        "    da0 = np.zeros((n_a, m))\n",
        "    da_prevt = np.zeros(da0.shape)\n",
        "    dc_prevt = np.zeros(da0.shape)\n",
        "    dWf = np.zeros((n_a, n_a + n_x))\n",
        "    dWi = np.zeros(dWf.shape)\n",
        "    dWc = np.zeros(dWf.shape)\n",
        "    dWo = np.zeros(dWf.shape)\n",
        "    dbf = np.zeros((n_a, 1))\n",
        "    dbi = np.zeros(dbf.shape)\n",
        "    dbc = np.zeros(dbf.shape)\n",
        "    dbo = np.zeros(dbf.shape)\n",
        "\n",
        "    # Boucle sur la séquence en sens inverse\n",
        "    for t in reversed(range(T_x)):\n",
        "        # Calculer tous les gradients à l'aide de lstm_cell_backward\n",
        "        gradients = lstm_cell_backward(da[:, :, t] + da_prevt, dc_prevt, caches[t])\n",
        "        # Stocker ou ajouter les gradients aux gradients de l'étape précédente\n",
        "        dx[:, :, t] = gradients[\"dxt\"]\n",
        "        dWf += gradients[\"dWf\"]\n",
        "        dWi += gradients[\"dWi\"]\n",
        "        dWc += gradients[\"dWc\"]\n",
        "        dWo += gradients[\"dWo\"]\n",
        "        dbf += gradients[\"dbf\"]\n",
        "        dbi += gradients[\"dbi\"]\n",
        "        dbc += gradients[\"dbc\"]\n",
        "        dbo += gradients[\"dbo\"]\n",
        "        da_prevt = gradients[\"da_prev\"]\n",
        "        dc_prevt = gradients[\"dc_prev\"]\n",
        "\n",
        "    # Définir le premier gradient d'activation\n",
        "    da0 = gradients[\"da_prev\"]\n",
        "\n",
        "    # Stocker les gradients dans un dictionnaire Python\n",
        "    gradients = {\"dx\": dx, \"da0\": da0, \"dWf\": dWf, \"dbf\": dbf, \"dWi\": dWi, \"dbi\": dbi,\n",
        "                 \"dWc\": dWc, \"dbc\": dbc, \"dWo\": dWo, \"dbo\": dbo}\n",
        "\n",
        "    return gradients\n",
        "\n",
        "def initialize_adam_for_lstm(parameters):\n",
        "    \"\"\"\n",
        "    Initialise v et s pour les paramètres du LSTM.\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- Dictionnaire Python contenant les paramètres du LSTM.\n",
        "\n",
        "    Returns:\n",
        "    v -- Dictionnaire Python qui contiendra la moyenne mobile exponentielle du gradient.\n",
        "    s -- Dictionnaire Python qui contiendra la moyenne mobile exponentielle du carré du gradient.\n",
        "    \"\"\"\n",
        "    v = {}\n",
        "    s = {}\n",
        "\n",
        "    # Initialiser v, s pour tous les paramètres du LSTM\n",
        "    for key in parameters.keys():\n",
        "        v[\"d\" + key] = np.zeros_like(parameters[key])\n",
        "        s[\"d\" + key] = np.zeros_like(parameters[key])\n",
        "\n",
        "    return v, s\n",
        "\n",
        "def update_parameters_with_adam_for_lstm(parameters, grads, v, s, t, learning_rate=0.01,\n",
        "                                         beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "    \"\"\"\n",
        "    Update parameters using Adam\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- dictionary containing your parameters\n",
        "    grads -- dictionary containing your gradients, output of lstm_backward\n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    t -- Timestep, integer\n",
        "    learning_rate -- the learning rate, scalar\n",
        "    beta1 -- Exponential decay hyperparameter for the first moment estimates\n",
        "    beta2 -- Exponential decay hyperparameter for the second moment estimates\n",
        "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters\n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    \"\"\"\n",
        "    v_corrected = {}  # Estimation du premier moment corrigée du biais\n",
        "    s_corrected = {}  # Estimation du second moment corrigée du biais\n",
        "\n",
        "    # Effectuer la mise à jour Adam sur tous les paramètres\n",
        "    for key in parameters.keys():\n",
        "        # Clé correspondante dans les dictionnaires grads, v, s\n",
        "        d_key = \"d\" + key\n",
        "\n",
        "        # S'assurer que nous avons le gradient correspondant\n",
        "        if d_key not in grads:\n",
        "            continue\n",
        "\n",
        "        # Moyenne mobile des gradients\n",
        "        v[d_key] = beta1 * v[d_key] + (1 - beta1) * grads[d_key]\n",
        "\n",
        "        # Calcul de l'estimation du premier moment corrigée du biais\n",
        "        v_corrected[d_key] = v[d_key] / (1 - beta1**t)\n",
        "\n",
        "        # Moyenne mobile des carrés des gradients\n",
        "        s[d_key] = beta2 * s[d_key] + (1 - beta2) * (grads[d_key]**2)\n",
        "\n",
        "        # Calcul de l'estimation du second moment corrigée du biais\n",
        "        s_corrected[d_key] = s[d_key] / (1 - beta2**t)\n",
        "\n",
        "        # Mise à jour des paramètres\n",
        "        parameters[key] = parameters[key] - learning_rate * v_corrected[d_key] / (np.sqrt(s_corrected[d_key]) + epsilon)\n",
        "\n",
        "    return parameters, v, s\n",
        "\n",
        "def initialize_lstm_parameters(n_a, n_x, n_y):\n",
        "    \"\"\"\n",
        "    Initialise les paramètres du LSTM.\n",
        "\n",
        "    Arguments:\n",
        "    n_a -- nombre d'unités dans la couche cachée\n",
        "    n_x -- taille d'entrée\n",
        "    n_y -- taille de sortie\n",
        "\n",
        "    Returns:\n",
        "    parameters -- dictionnaire Python contenant les paramètres initialisés\n",
        "    \"\"\"\n",
        "    np.random.seed(1)\n",
        "\n",
        "    # Initialisation avec He/Xavier\n",
        "    Wf = np.random.randn(n_a, n_a + n_x) * np.sqrt(1. / (n_a + n_x))\n",
        "    bf = np.zeros((n_a, 1))\n",
        "    Wi = np.random.randn(n_a, n_a + n_x) * np.sqrt(1. / (n_a + n_x))\n",
        "    bi = np.zeros((n_a, 1))\n",
        "    Wc = np.random.randn(n_a, n_a + n_x) * np.sqrt(1. / (n_a + n_x))\n",
        "    bc = np.zeros((n_a, 1))\n",
        "    Wo = np.random.randn(n_a, n_a + n_x) * np.sqrt(1. / (n_a + n_x))\n",
        "    bo = np.zeros((n_a, 1))\n",
        "    Wy = np.random.randn(n_y, n_a) * np.sqrt(1. / n_a)\n",
        "    by = np.zeros((n_y, 1))\n",
        "\n",
        "    parameters = {\"Wf\": Wf, \"bf\": bf, \"Wi\": Wi, \"bi\": bi, \"Wc\": Wc, \"bc\": bc, \"Wo\": Wo, \"bo\": bo, \"Wy\": Wy, \"by\": by}\n",
        "\n",
        "    return parameters\n",
        "\n",
        "def train_lstm(X_train, Y_train, n_a, n_x, n_y, num_epochs=10, seed=1, learning_rate=0.01, initial_params=None):\n",
        "    \"\"\"\n",
        "    Entraîne un LSTM sur les données fournies, avec possibilité d'initialiser avec des paramètres existants.\n",
        "\n",
        "    Arguments:\n",
        "    X_train -- données d'entrée, numpy array de forme (n_x, m, T_x)\n",
        "    Y_train -- étiquettes, numpy array de forme (n_y, m, T_x)\n",
        "    n_a -- nombre d'unités dans la couche cachée\n",
        "    n_x -- taille d'entrée\n",
        "    n_y -- taille de sortie\n",
        "    num_epochs -- nombre d'époques d'entraînement\n",
        "    seed -- graine pour la reproductibilité\n",
        "    learning_rate -- taux d'apprentissage\n",
        "    initial_params -- paramètres initiaux (optionnel)\n",
        "\n",
        "    Returns:\n",
        "    parameters -- paramètres finaux\n",
        "    parameters_history -- historique des paramètres à chaque époque\n",
        "    loss_history -- historique des pertes\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Initialisation des paramètres\n",
        "    if initial_params is None:\n",
        "        parameters = initialize_lstm_parameters(n_a, n_x, n_y)\n",
        "    else:\n",
        "        parameters = copy.deepcopy(initial_params)\n",
        "\n",
        "    parameters_history = []\n",
        "    loss_history = []\n",
        "\n",
        "    # Initialiser Adam\n",
        "    v, s = initialize_adam_for_lstm(parameters)\n",
        "    t = 0  # Compteur pour Adam\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Époque {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Forward pass\n",
        "        a0 = np.zeros((n_a, X_train.shape[1]))\n",
        "        a, y_pred, c, caches = lstm_forward(X_train, a0, parameters)\n",
        "\n",
        "        # Calcul de la perte (cross-entropy)\n",
        "        loss = -np.sum(Y_train * np.log(y_pred + 1e-8)) / (Y_train.shape[1] * Y_train.shape[2])\n",
        "        loss_history.append(loss)\n",
        "        print(f\"Loss: {loss:.4f}\")\n",
        "\n",
        "        # Initialisation du gradient de sortie\n",
        "        da = np.zeros_like(a)\n",
        "\n",
        "        # Créer un dictionnaire complet pour les gradients\n",
        "        gradients = {}\n",
        "\n",
        "        # Pour chaque pas de temps, calculer le gradient\n",
        "        dWy = np.zeros_like(parameters[\"Wy\"])\n",
        "        dby = np.zeros_like(parameters[\"by\"])\n",
        "\n",
        "        for t_idx in range(Y_train.shape[2]):\n",
        "            # Gradient de la cross-entropy\n",
        "            dy = y_pred[:, :, t_idx] - Y_train[:, :, t_idx]\n",
        "            # Accumuler les gradients pour Wy et by\n",
        "            dWy += np.dot(dy, a[:, :, t_idx].T)\n",
        "            dby += np.sum(dy, axis=1, keepdims=True)\n",
        "            # Gradient par rapport à a\n",
        "            da[:, :, t_idx] = np.dot(parameters[\"Wy\"].T, dy)\n",
        "\n",
        "        # Backward pass pour le reste des paramètres LSTM\n",
        "        lstm_gradients = lstm_backward(da, caches)\n",
        "\n",
        "        # Combiner tous les gradients\n",
        "        gradients = lstm_gradients.copy()\n",
        "        gradients[\"dWy\"] = dWy\n",
        "        gradients[\"dby\"] = dby\n",
        "\n",
        "        # Mise à jour des paramètres avec Adam\n",
        "        t += 1\n",
        "        parameters, v, s = update_parameters_with_adam_for_lstm(parameters, gradients, v, s, t, learning_rate)\n",
        "\n",
        "        # Sauvegarde des paramètres après cette époque\n",
        "        parameters_history.append(copy.deepcopy(parameters))\n",
        "\n",
        "    return parameters, parameters_history, loss_history\n",
        "\n",
        "def evaluate_lstm(X_test, Y_test, parameters):\n",
        "    \"\"\"\n",
        "    Évalue les performances d'un LSTM pour la régression.\n",
        "\n",
        "    Arguments:\n",
        "    X_test -- données de test, numpy array de forme (n_x, m, T_x)\n",
        "    Y_test -- étiquettes de test, numpy array de forme (n_y, m, T_x)\n",
        "    parameters -- dictionnaire Python contenant les paramètres du LSTM\n",
        "\n",
        "    Returns:\n",
        "    rmse_val -- erreur quadratique moyenne\n",
        "    loss -- perte du modèle\n",
        "    \"\"\"\n",
        "    n_a = parameters[\"Wf\"].shape[0]\n",
        "\n",
        "    # Forward pass\n",
        "    a0 = np.zeros((n_a, X_test.shape[1]))\n",
        "    _, y_pred, _, _ = lstm_forward(X_test, a0, parameters)\n",
        "\n",
        "    # Calculer RMSE\n",
        "    rmse_val = np.sqrt(np.mean((y_pred - Y_test) ** 2))\n",
        "\n",
        "    # Calcul de la perte (MSE)\n",
        "    loss = np.mean((y_pred - Y_test) ** 2)\n",
        "\n",
        "    return rmse_val, loss\n",
        "\n",
        "def rmse(predictions, targets):\n",
        "    \"\"\"\n",
        "    Calcule la racine de l'erreur quadratique moyenne (RMSE)\n",
        "\n",
        "    Arguments:\n",
        "    predictions -- prédictions du modèle, array numpy\n",
        "    targets -- valeurs cibles, array numpy\n",
        "\n",
        "    Returns:\n",
        "    rmse_value -- valeur RMSE (float)\n",
        "    \"\"\"\n",
        "    return np.sqrt(np.mean((predictions - targets) ** 2))\n",
        "\n",
        "def flatten_parameters(parameters):\n",
        "    \"\"\"\n",
        "    Aplatit les paramètres d'un LSTM en un seul vecteur.\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- dictionnaire Python contenant les paramètres\n",
        "\n",
        "    Returns:\n",
        "    flattened -- vecteur aplati des paramètres\n",
        "    param_shapes -- formes originales des paramètres\n",
        "    param_sizes -- tailles des paramètres aplatis\n",
        "    \"\"\"\n",
        "    flattened = []\n",
        "    param_shapes = {}\n",
        "    param_sizes = {}\n",
        "\n",
        "    for key in [\"Wf\", \"bf\", \"Wi\", \"bi\", \"Wc\", \"bc\", \"Wo\", \"bo\", \"Wy\", \"by\"]:\n",
        "        param = parameters[key]\n",
        "        param_shapes[key] = param.shape\n",
        "        flattened.append(param.flatten())\n",
        "        param_sizes[key] = param.size\n",
        "\n",
        "    return np.concatenate(flattened), param_shapes, param_sizes\n",
        "\n",
        "def unflatten_parameters(flattened, param_shapes, param_sizes):\n",
        "    \"\"\"\n",
        "    Restaure un vecteur aplati de paramètres à leur forme originale.\n",
        "\n",
        "    Arguments:\n",
        "    flattened -- vecteur aplati des paramètres\n",
        "    param_shapes -- formes originales des paramètres\n",
        "    param_sizes -- tailles des paramètres aplatis\n",
        "\n",
        "    Returns:\n",
        "    parameters -- dictionnaire Python contenant les paramètres\n",
        "    \"\"\"\n",
        "    parameters = {}\n",
        "    start = 0\n",
        "\n",
        "    for key in [\"Wf\", \"bf\", \"Wi\", \"bi\", \"Wc\", \"bc\", \"Wo\", \"bo\", \"Wy\", \"by\"]:\n",
        "        size = param_sizes[key]\n",
        "        parameters[key] = flattened[start:start+size].reshape(param_shapes[key])\n",
        "        start += size\n",
        "\n",
        "    return parameters\n",
        "\n",
        "\n",
        "\n",
        "def calculate_transmission_size(parameters=None, n_clusters=None, transition_matrix=None, client_centers=None):\n",
        "    \"\"\"\n",
        "    Calcule la taille de transmission en octets.\n",
        "\n",
        "    Args:\n",
        "        parameters: Dictionnaire ou liste des paramètres originaux (arrays numpy)\n",
        "        n_clusters: Nombre de clusters utilisés\n",
        "        transition_matrix: Matrice de transition entre clusters\n",
        "        client_centers: Centres des clusters par client (liste de tuples (centers, transition_matrix))\n",
        "\n",
        "    Returns:\n",
        "        Taille totale en octets\n",
        "    \"\"\"\n",
        "    total_size = 0\n",
        "\n",
        "    # Cas 1: Paramètres originaux (dictionnaire)\n",
        "    if parameters is not None and isinstance(parameters, dict):\n",
        "        for key in parameters:\n",
        "            if hasattr(parameters[key], 'size'):\n",
        "                # Chaque nombre flottant occupe 4 octets (float32)\n",
        "                total_size += parameters[key].size * 4\n",
        "        return total_size\n",
        "\n",
        "    # Cas 2: Paramètres originaux (liste)\n",
        "    elif parameters is not None and isinstance(parameters, list) and all(hasattr(p, 'size') for p in parameters if p is not None):\n",
        "        for param in parameters:\n",
        "            if param is not None and hasattr(param, 'size'):\n",
        "                # Chaque nombre flottant occupe 4 octets (float32)\n",
        "                total_size += param.size * 4\n",
        "        return total_size\n",
        "\n",
        "    # Cas 3: Clusterisation globale avec n_clusters et transition_matrix\n",
        "    elif n_clusters is not None and transition_matrix is not None:\n",
        "        # Taille de la matrice de transition\n",
        "        if hasattr(transition_matrix, 'size'):\n",
        "            total_size += transition_matrix.size * 4\n",
        "\n",
        "        # Si nous avons des centres de clusters explicites\n",
        "        if client_centers is not None and not isinstance(client_centers, list):\n",
        "            if hasattr(client_centers, 'size'):\n",
        "                total_size += client_centers.size * 4\n",
        "\n",
        "        return total_size\n",
        "\n",
        "    # Cas 4: Clusterisation locale avec client_centers\n",
        "    elif client_centers is not None and isinstance(client_centers, list):\n",
        "        for client_data in client_centers:\n",
        "            # Dans la clusterisation locale, client_data est un tuple (cluster_centers, transition_matrix)\n",
        "            if isinstance(client_data, tuple) and len(client_data) == 2:\n",
        "                centers, trans_matrix = client_data\n",
        "\n",
        "                # Taille des centres de clusters\n",
        "                if hasattr(centers, 'size'):\n",
        "                    total_size += centers.size * 4\n",
        "\n",
        "                # Taille de la matrice de transition\n",
        "                if hasattr(trans_matrix, 'size'):\n",
        "                    total_size += trans_matrix.size * 4\n",
        "\n",
        "        return total_size\n",
        "\n",
        "    # En cas d'erreur ou de paramètres non valides\n",
        "    raise ValueError(\"Entrée invalide pour le calcul de la taille de transmission\")\n",
        "\n",
        "\n",
        "def cluster_parameters_by_epoch_local(parameters_history_by_seed, n_clusters=3):\n",
        "    \"\"\"\n",
        "    Clusterise les paramètres à chaque époque pour chaque seed.\n",
        "    Cette version est utilisée pour la clusterisation locale par client.\n",
        "\n",
        "    Arguments:\n",
        "    parameters_history_by_seed -- liste de listes de dictionnaires Python contenant les paramètres\n",
        "    n_clusters -- nombre de clusters à former\n",
        "\n",
        "    Returns:\n",
        "    kmeans_models -- modèles KMeans pour chaque époque\n",
        "    cluster_labels -- étiquettes de cluster pour chaque graine à chaque époque\n",
        "    flat_params -- paramètres aplatis pour chaque graine à chaque époque\n",
        "    param_shapes -- formes des paramètres\n",
        "    param_sizes -- tailles des paramètres aplatis\n",
        "    \"\"\"\n",
        "    n_seeds = len(parameters_history_by_seed)\n",
        "    n_epochs = len(parameters_history_by_seed[0])\n",
        "\n",
        "    kmeans_models = []\n",
        "    cluster_labels = np.zeros((n_seeds, n_epochs), dtype=int)\n",
        "    flat_params = []\n",
        "\n",
        "    # Obtenir les formes et tailles des paramètres\n",
        "    _, param_shapes, param_sizes = flatten_parameters(parameters_history_by_seed[0][0])\n",
        "\n",
        "    # Aplatir les paramètres pour toutes les graines et époques\n",
        "    for seed in range(n_seeds):\n",
        "        seed_params = []\n",
        "        for epoch in range(n_epochs):\n",
        "            flattened, _, _ = flatten_parameters(parameters_history_by_seed[seed][epoch])\n",
        "            seed_params.append(flattened)\n",
        "        flat_params.append(seed_params)\n",
        "\n",
        "    flat_params = np.array(flat_params)\n",
        "\n",
        "    # Clusteriser par époque\n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_params = flat_params[:, epoch, :]\n",
        "        kmeans = KMeans(n_clusters=min(n_clusters, n_seeds), random_state=42)\n",
        "        cluster_labels[:, epoch] = kmeans.fit_predict(epoch_params)\n",
        "        kmeans_models.append(kmeans)\n",
        "\n",
        "    return kmeans_models, cluster_labels, flat_params, param_shapes, param_sizes\n",
        "\n",
        "def compute_transition_matrix_local(cluster_labels):\n",
        "    \"\"\"\n",
        "    Calcule la matrice de transition de Markov à partir des séquences de labels de clusters.\n",
        "    Cette version est utilisée pour la clusterisation locale par client.\n",
        "\n",
        "    Arguments:\n",
        "    cluster_labels -- étiquettes de cluster pour chaque graine à chaque époque\n",
        "\n",
        "    Returns:\n",
        "    transition_matrix -- matrice de transition de Markov\n",
        "    \"\"\"\n",
        "    n_seeds, n_epochs = cluster_labels.shape\n",
        "    n_clusters = np.max(cluster_labels) + 1\n",
        "\n",
        "    transition_counts = np.zeros((n_clusters, n_clusters))\n",
        "\n",
        "    # Compter les transitions\n",
        "    for seed in range(n_seeds):\n",
        "        for epoch in range(n_epochs - 1):\n",
        "            from_cluster = cluster_labels[seed, epoch]\n",
        "            to_cluster = cluster_labels[seed, epoch + 1]\n",
        "            transition_counts[from_cluster, to_cluster] += 1\n",
        "\n",
        "    # Normaliser pour obtenir les probabilités\n",
        "    transition_matrix = np.zeros_like(transition_counts)\n",
        "    for i in range(n_clusters):\n",
        "        row_sum = np.sum(transition_counts[i])\n",
        "        if row_sum > 0:\n",
        "            transition_matrix[i] = transition_counts[i] / row_sum\n",
        "        else:\n",
        "            # Si aucune transition n'est observée depuis ce cluster, distribution uniforme\n",
        "            transition_matrix[i] = 1.0 / n_clusters\n",
        "\n",
        "    return transition_matrix\n",
        "\n",
        "def simulate_parameter_trajectory_local(initial_cluster, transition_matrix, n_steps, kmeans_models):\n",
        "    \"\"\"\n",
        "    Simule une trajectoire de paramètres basée sur la matrice de transition.\n",
        "    Cette version est utilisée pour la clusterisation locale par client.\n",
        "\n",
        "    Arguments:\n",
        "    initial_cluster -- cluster initial\n",
        "    transition_matrix -- matrice de transition de Markov\n",
        "    n_steps -- nombre d'étapes à simuler\n",
        "    kmeans_models -- modèles KMeans pour chaque époque\n",
        "\n",
        "    Returns:\n",
        "    trajectory -- trajectoire simulée de paramètres\n",
        "    cluster_sequence -- séquence de clusters visitée\n",
        "    \"\"\"\n",
        "    n_clusters = transition_matrix.shape[0]\n",
        "    cluster_sequence = [initial_cluster]\n",
        "    current_cluster = initial_cluster\n",
        "\n",
        "    for _ in range(n_steps - 1):\n",
        "        # Échantillonner le prochain cluster\n",
        "        next_cluster = np.random.choice(n_clusters, p=transition_matrix[current_cluster])\n",
        "        cluster_sequence.append(next_cluster)\n",
        "        current_cluster = next_cluster\n",
        "\n",
        "    # Convertir la séquence de clusters en paramètres\n",
        "    trajectory = []\n",
        "    for step, cluster in enumerate(cluster_sequence):\n",
        "        # Utiliser le centre du cluster comme paramètres représentatifs\n",
        "        # Si nous avons dépassé le nombre d'époques dans kmeans_models, utiliser le dernier\n",
        "        model_idx = min(step, len(kmeans_models) - 1)\n",
        "        trajectory.append(kmeans_models[model_idx].cluster_centers_[cluster])\n",
        "\n",
        "    return trajectory, cluster_sequence\n",
        "\n",
        "def cluster_local_by_client(parameters_history_by_client_seed, n_clusters=3, n_steps=None):\n",
        "    \"\"\"\n",
        "    Effectue une clusterisation locale pour chaque client\n",
        "\n",
        "    Arguments:\n",
        "    parameters_history_by_client_seed -- dictionnaire contenant les historiques de paramètres pour chaque client et chaque seed\n",
        "    n_clusters -- nombre de clusters à former pour chaque client\n",
        "    n_steps -- nombre d'étapes à simuler (par défaut: même que la longueur d'origine)\n",
        "\n",
        "    Returns:\n",
        "    client_clusters -- liste des centres de clusters et matrices de transition pour chaque client\n",
        "    param_shapes -- formes des paramètres\n",
        "    param_sizes -- tailles des paramètres\n",
        "    \"\"\"\n",
        "    n_clients = len(parameters_history_by_client_seed)\n",
        "    client_clusters = []\n",
        "\n",
        "    # Pour stocker les formes et tailles des paramètres (identiques pour tous les clients)\n",
        "    param_shapes = None\n",
        "    param_sizes = None\n",
        "\n",
        "    for client_id in range(n_clients):\n",
        "        client_params_history = parameters_history_by_client_seed[client_id]\n",
        "        n_seeds = len(client_params_history)\n",
        "        n_epochs = len(client_params_history[0])\n",
        "\n",
        "        if n_steps is None:\n",
        "            n_steps = n_epochs\n",
        "\n",
        "        # Clusteriser les paramètres locaux du client\n",
        "        kmeans_models, cluster_labels, flat_params, param_shapes, param_sizes = cluster_parameters_by_epoch_local(\n",
        "            client_params_history, n_clusters=min(n_clusters, n_seeds))\n",
        "\n",
        "        # Calculer la matrice de transition locale\n",
        "        transition_matrix = compute_transition_matrix_local(cluster_labels)\n",
        "\n",
        "        # Extraire les centres de clusters (utiliser le dernier modèle KMeans)\n",
        "        last_kmeans = kmeans_models[-1]\n",
        "        cluster_centers = last_kmeans.cluster_centers_\n",
        "\n",
        "        # Stocker les centres et la matrice pour ce client\n",
        "        client_clusters.append((cluster_centers, transition_matrix))\n",
        "\n",
        "    return client_clusters, param_shapes, param_sizes\n",
        "\n",
        "\n",
        "def cluster_parameters_by_epoch(parameters_history_by_seed, n_clusters=3):\n",
        "    \"\"\"\n",
        "    Clusterise les paramètres à chaque époque pour tous les clients.\n",
        "\n",
        "    Arguments:\n",
        "    parameters_history_by_seed -- liste de listes de dictionnaires Python contenant les paramètres\n",
        "    n_clusters -- nombre de clusters à former\n",
        "\n",
        "    Returns:\n",
        "    kmeans_models -- modèles KMeans pour chaque époque\n",
        "    cluster_labels -- étiquettes de cluster pour chaque graine à chaque époque\n",
        "    flat_params -- paramètres aplatis pour chaque graine à chaque époque\n",
        "    param_shapes -- formes des paramètres\n",
        "    param_sizes -- tailles des paramètres aplatis\n",
        "    \"\"\"\n",
        "    n_seeds = len(parameters_history_by_seed)\n",
        "    n_epochs = len(parameters_history_by_seed[0])\n",
        "\n",
        "    kmeans_models = []\n",
        "    cluster_labels = np.zeros((n_seeds, n_epochs), dtype=int)\n",
        "    flat_params = []\n",
        "\n",
        "    # Obtenir les formes et tailles des paramètres\n",
        "    _, param_shapes, param_sizes = flatten_parameters(parameters_history_by_seed[0][0])\n",
        "\n",
        "    # Aplatir les paramètres pour toutes les graines et époques\n",
        "    for seed in range(n_seeds):\n",
        "        seed_params = []\n",
        "        for epoch in range(n_epochs):\n",
        "            flattened, _, _ = flatten_parameters(parameters_history_by_seed[seed][epoch])\n",
        "            seed_params.append(flattened)\n",
        "        flat_params.append(seed_params)\n",
        "\n",
        "    flat_params = np.array(flat_params)\n",
        "\n",
        "    # Clusteriser par époque\n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_params = flat_params[:, epoch, :]\n",
        "        kmeans = KMeans(n_clusters=min(n_clusters, n_seeds), random_state=42)\n",
        "        cluster_labels[:, epoch] = kmeans.fit_predict(epoch_params)\n",
        "        kmeans_models.append(kmeans)\n",
        "\n",
        "    return kmeans_models, cluster_labels, flat_params, param_shapes, param_sizes\n",
        "\n",
        "def compute_transition_matrix(cluster_labels):\n",
        "    \"\"\"\n",
        "    Calcule la matrice de transition de Markov à partir des séquences de labels de clusters.\n",
        "\n",
        "    Arguments:\n",
        "    cluster_labels -- étiquettes de cluster pour chaque graine à chaque époque\n",
        "\n",
        "    Returns:\n",
        "    transition_matrix -- matrice de transition de Markov\n",
        "    \"\"\"\n",
        "    n_seeds, n_epochs = cluster_labels.shape\n",
        "    n_clusters = np.max(cluster_labels) + 1\n",
        "\n",
        "    transition_counts = np.zeros((n_clusters, n_clusters))\n",
        "\n",
        "    # Compter les transitions\n",
        "    for seed in range(n_seeds):\n",
        "        for epoch in range(n_epochs - 1):\n",
        "            from_cluster = cluster_labels[seed, epoch]\n",
        "            to_cluster = cluster_labels[seed, epoch + 1]\n",
        "            transition_counts[from_cluster, to_cluster] += 1\n",
        "\n",
        "    # Normaliser pour obtenir les probabilités\n",
        "    transition_matrix = np.zeros_like(transition_counts)\n",
        "    for i in range(n_clusters):\n",
        "        row_sum = np.sum(transition_counts[i])\n",
        "        if row_sum > 0:\n",
        "            transition_matrix[i] = transition_counts[i] / row_sum\n",
        "        else:\n",
        "            # Si aucune transition n'est observée depuis ce cluster, distribution uniforme\n",
        "            transition_matrix[i] = 1.0 / n_clusters\n",
        "\n",
        "    return transition_matrix\n",
        "\n",
        "def simulate_parameter_trajectory(initial_cluster, transition_matrix, n_steps, kmeans_models):\n",
        "    \"\"\"\n",
        "    Simule une trajectoire de paramètres basée sur la matrice de transition.\n",
        "\n",
        "    Arguments:\n",
        "    initial_cluster -- cluster initial\n",
        "    transition_matrix -- matrice de transition de Markov\n",
        "    n_steps -- nombre d'étapes à simuler\n",
        "    kmeans_models -- modèles KMeans pour chaque époque\n",
        "\n",
        "    Returns:\n",
        "    trajectory -- trajectoire simulée de paramètres\n",
        "    cluster_sequence -- séquence de clusters visitée\n",
        "    \"\"\"\n",
        "    n_clusters = transition_matrix.shape[0]\n",
        "    cluster_sequence = [initial_cluster]\n",
        "    current_cluster = initial_cluster\n",
        "\n",
        "    for _ in range(n_steps - 1):\n",
        "        # Échantillonner le prochain cluster\n",
        "        next_cluster = np.random.choice(n_clusters, p=transition_matrix[current_cluster])\n",
        "        cluster_sequence.append(next_cluster)\n",
        "        current_cluster = next_cluster\n",
        "\n",
        "    # Convertir la séquence de clusters en paramètres\n",
        "    trajectory = []\n",
        "    for step, cluster in enumerate(cluster_sequence):\n",
        "        # Utiliser le centre du cluster comme paramètres représentatifs\n",
        "        # Si nous avons dépassé le nombre d'époques dans kmeans_models, utiliser le dernier\n",
        "        model_idx = min(step, len(kmeans_models) - 1)\n",
        "        trajectory.append(kmeans_models[model_idx].cluster_centers_[cluster])\n",
        "\n",
        "    return trajectory, cluster_sequence\n",
        "\n",
        "def simulate_full_parameter_transmission(parameters_history_by_client, X_test, Y_test, n_a, n_x, n_y, num_transfer_epochs, learning_rate):\n",
        "    \"\"\"\n",
        "    Simule la méthode traditionnelle de transmission complète des paramètres.\n",
        "\n",
        "    Arguments:\n",
        "    parameters_history_by_client -- historique des paramètres pour chaque client\n",
        "    X_test -- données de test\n",
        "    Y_test -- étiquettes de test\n",
        "    n_a -- nombre d'unités cachées\n",
        "    n_x -- dimension d'entrée\n",
        "    n_y -- dimension de sortie\n",
        "    num_transfer_epochs -- nombre d'époques pour le transfer learning\n",
        "    learning_rate -- taux d'apprentissage\n",
        "\n",
        "    Returns:\n",
        "    full_params_avg -- paramètres moyennés sans transfer\n",
        "    full_params_transfer -- paramètres après transfer\n",
        "    full_params_acc -- précision après transfer\n",
        "    full_params_loss -- perte après transfer\n",
        "    full_params_size -- taille de transmission (en octets)\n",
        "    \"\"\"\n",
        "    # Extraire les derniers paramètres pour chaque client\n",
        "    last_params = [client_history[-1] for client_history in parameters_history_by_client]\n",
        "\n",
        "    # Calculer la moyenne des paramètres (FedAvg)\n",
        "    full_params_avg = {}\n",
        "    for key in last_params[0].keys():\n",
        "        full_params_avg[key] = np.mean([params[key] for params in last_params], axis=0)\n",
        "\n",
        "    # Calculer la taille de transmission\n",
        "    full_params_size = calculate_transmission_size(full_params_avg)\n",
        "\n",
        "    # Réentraîner avec transfer learning\n",
        "    full_params_transfer, loss_history, acc_history = transfer_learning(\n",
        "        X_train=X_test, Y_train=Y_test,\n",
        "        X_test=X_test, Y_test=Y_test,\n",
        "        source_parameters=full_params_avg,\n",
        "        n_a=n_a, n_x=n_x, n_y=n_y,\n",
        "        num_epochs=num_transfer_epochs,\n",
        "        learning_rate=learning_rate\n",
        "    )\n",
        "\n",
        "    # Récupérer les performances finales\n",
        "    full_params_acc = acc_history[-1]\n",
        "    full_params_loss = loss_history[-1]\n",
        "\n",
        "    return full_params_avg, full_params_transfer, full_params_acc, full_params_loss, full_params_size\n",
        "\n",
        "\n",
        "def aggregate_client_clusters(client_clusters, param_shapes, param_sizes, n_clients, weight_by_client=None):\n",
        "    \"\"\"\n",
        "    Agrège les clusters de paramètres de plusieurs clients\n",
        "\n",
        "    Arguments:\n",
        "    client_clusters -- liste des centres de clusters et matrices de transition pour chaque client\n",
        "    param_shapes -- formes des paramètres\n",
        "    param_sizes -- tailles des paramètres\n",
        "    n_clients -- nombre de clients\n",
        "    weight_by_client -- poids pour l'agrégation de chaque client (optionnel)\n",
        "\n",
        "    Returns:\n",
        "    aggregated_params -- paramètres agrégés à partir des clusters des clients\n",
        "    \"\"\"\n",
        "    # Si les poids ne sont pas fournis, utiliser un poids uniforme\n",
        "    if weight_by_client is None:\n",
        "        weight_by_client = np.ones(n_clients) / n_clients\n",
        "\n",
        "    # Initialiser les paramètres agrégés\n",
        "    all_flattened_params = []\n",
        "\n",
        "    # Pour chaque client, simuler une trajectoire à partir de ses clusters\n",
        "    for client_id, (cluster_centers, transition_matrix) in enumerate(client_clusters):\n",
        "        # Choisir un cluster de départ (utiliser le cluster le plus fréquent ou le premier)\n",
        "        initial_cluster = 0\n",
        "\n",
        "        # Obtenir un ensemble de paramètres représentatifs de ce client\n",
        "        # Simplement moyenne des centres de clusters pondérée par la probabilité stationnaire\n",
        "        n_clusters = transition_matrix.shape[0]\n",
        "\n",
        "        # Calculer la distribution stationnaire (au lieu de simuler une trajectoire)\n",
        "        # Méthode simple: itérer la matrice de transition jusqu'à convergence\n",
        "        pi = np.ones(n_clusters) / n_clusters  # Distribution initiale uniforme\n",
        "        for _ in range(100):  # Nombre d'itérations arbitraire, devrait converger rapidement\n",
        "            pi_new = np.dot(pi, transition_matrix)\n",
        "            if np.allclose(pi, pi_new):\n",
        "                break\n",
        "            pi = pi_new\n",
        "\n",
        "        # Pondérer les centres par la distribution stationnaire\n",
        "        client_params = np.zeros_like(cluster_centers[0])\n",
        "        for i in range(n_clusters):\n",
        "            client_params += pi[i] * cluster_centers[i]\n",
        "\n",
        "        # Ajouter à la liste des paramètres aplatis\n",
        "        all_flattened_params.append(client_params)\n",
        "\n",
        "    # Combiner tous les paramètres avec les poids des clients\n",
        "    aggregated_flattened = np.zeros_like(all_flattened_params[0])\n",
        "    for client_id, flattened in enumerate(all_flattened_params):\n",
        "        aggregated_flattened += weight_by_client[client_id] * flattened\n",
        "\n",
        "    # Reconstruire les paramètres à leur forme d'origine\n",
        "    aggregated_params = unflatten_parameters(aggregated_flattened, param_shapes, param_sizes)\n",
        "\n",
        "    return aggregated_params\n",
        "\n",
        "# Correction de la fonction load_real_data pour générer une erreur si les données ne peuvent pas être chargées\n",
        "def load_real_data(n_clients, n_seeds_per_client, n_x, n_y, sequence_length):\n",
        "    \"\"\"\n",
        "    Charge les données réelles pour chaque client (pays).\n",
        "\n",
        "    Arguments:\n",
        "    n_clients -- nombre de clients (pays)\n",
        "    n_seeds_per_client -- nombre de seeds différentes pour chaque client\n",
        "    n_x -- dimension d'entrée (nombre de features)\n",
        "    n_y -- dimension de sortie (Consumption)\n",
        "    sequence_length -- longueur de la séquence temporelle\n",
        "\n",
        "    Returns:\n",
        "    clients_data -- liste de tuples (X_train, Y_train) pour chaque client et seed\n",
        "\n",
        "    Raises:\n",
        "    FileNotFoundError: Si les fichiers de données ne sont pas trouvés\n",
        "    ValueError: Si les données ne peuvent pas être traitées correctement\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "    clients_data = []\n",
        "    countries = [\"pays1\", \"pays2\", \"pays3\"]  # Remplacez par les noms réels de vos pays\n",
        "\n",
        "    for client_id in range(n_clients):\n",
        "        if client_id >= len(countries):\n",
        "            raise ValueError(f\"Pas assez de pays définis pour {n_clients} clients\")\n",
        "\n",
        "        client_seeds_data = []\n",
        "        country = countries[client_id]\n",
        "\n",
        "        # Charger les données météo pour ce pays (12 mois)\n",
        "        weather_data = []\n",
        "        missing_weather_files = []\n",
        "\n",
        "        for month in range(1, 13):\n",
        "            file_path = f\"/content/weather_{country}_month{month}.csv\"  # Ajustez selon votre structure\n",
        "            if not os.path.exists(file_path):\n",
        "                missing_weather_files.append(file_path)\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                df_month = pd.read_csv(file_path)\n",
        "                weather_data.append(df_month)\n",
        "            except Exception as e:\n",
        "                raise FileNotFoundError(f\"Erreur lors du chargement de {file_path}: {e}\")\n",
        "\n",
        "        if not weather_data:\n",
        "            raise FileNotFoundError(f\"Aucun fichier météo trouvé pour le pays {country}. Fichiers manquants: {missing_weather_files}\")\n",
        "\n",
        "        # Concaténer les données météo\n",
        "        weather_df = pd.concat(weather_data, ignore_index=True)\n",
        "\n",
        "        # Charger les données de consommation\n",
        "        consumption_path = f\"/content/consumption_{country}.csv\"  # Ajustez selon votre structure\n",
        "        if not os.path.exists(consumption_path):\n",
        "            raise FileNotFoundError(f\"Fichier de consommation non trouvé: {consumption_path}\")\n",
        "\n",
        "        try:\n",
        "            consumption_df = pd.read_csv(consumption_path)\n",
        "        except Exception as e:\n",
        "            raise FileNotFoundError(f\"Erreur lors du chargement de {consumption_path}: {e}\")\n",
        "\n",
        "        # Vérifier si les colonnes requises existent\n",
        "        if \"datetime\" not in weather_df.columns:\n",
        "            raise ValueError(f\"Colonne 'datetime' manquante dans les données météo du pays {country}\")\n",
        "\n",
        "        if \"MTU\" not in consumption_df.columns:\n",
        "            raise ValueError(f\"Colonne 'MTU' manquante dans les données de consommation du pays {country}\")\n",
        "\n",
        "        # Convertir les colonnes de date/heure\n",
        "        weather_df['datetime'] = pd.to_datetime(weather_df['datetime'])\n",
        "        consumption_df['MTU'] = pd.to_datetime(consumption_df['MTU'])\n",
        "\n",
        "        # Fusionner sur les timestamps\n",
        "        try:\n",
        "            merged_df = pd.merge_asof(\n",
        "                weather_df.sort_values('datetime'),\n",
        "                consumption_df.sort_values('MTU'),\n",
        "                left_on='datetime',\n",
        "                right_on='MTU',\n",
        "                direction='nearest'\n",
        "            )\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Erreur lors de la fusion des données pour le pays {country}: {e}\")\n",
        "\n",
        "        if merged_df.empty:\n",
        "            raise ValueError(f\"La fusion a produit un DataFrame vide pour le pays {country}\")\n",
        "\n",
        "        # Sélectionner les features pertinentes\n",
        "        features = ['temp', 'humidity', 'windspeed', 'feelslike', 'dew',\n",
        "                   'precip', 'cloudcover', 'visibility', 'uvindex']\n",
        "\n",
        "        # S'assurer que nous n'avons que n_x features et qu'elles existent\n",
        "        features = features[:n_x]\n",
        "        missing_features = [f for f in features if f not in merged_df.columns]\n",
        "        if missing_features:\n",
        "            raise ValueError(f\"Features manquantes dans les données pour le pays {country}: {missing_features}\")\n",
        "\n",
        "        if 'Consumption' not in merged_df.columns:\n",
        "            raise ValueError(f\"Colonne 'Consumption' manquante dans les données fusionnées pour le pays {country}\")\n",
        "\n",
        "        # Vérifier les valeurs manquantes\n",
        "        na_count = merged_df[features + ['Consumption']].isna().sum().sum()\n",
        "        if na_count > 0:\n",
        "            print(f\"AVERTISSEMENT: {na_count} valeurs manquantes trouvées dans les données du pays {country}\")\n",
        "            # Remplir les valeurs manquantes ou échouer si le pourcentage est trop élevé\n",
        "            na_percentage = na_count / (merged_df.shape[0] * (len(features) + 1)) * 100\n",
        "            if na_percentage > 10:  # Si plus de 10% de valeurs manquantes\n",
        "                raise ValueError(f\"Trop de valeurs manquantes ({na_percentage:.2f}%) dans les données du pays {country}\")\n",
        "\n",
        "            # Remplir les valeurs manquantes (par exemple avec la moyenne)\n",
        "            merged_df = merged_df.fillna(merged_df.mean())\n",
        "\n",
        "        X_data = merged_df[features].values\n",
        "        Y_data = merged_df[['Consumption']].values\n",
        "\n",
        "        # Vérifier les dimensions des données\n",
        "        if X_data.shape[0] < sequence_length:\n",
        "            raise ValueError(f\"Pas assez de données pour le pays {country}: {X_data.shape[0]} échantillons < {sequence_length} requis\")\n",
        "\n",
        "        # Normaliser les données\n",
        "        scaler_X = StandardScaler()\n",
        "        scaler_Y = StandardScaler()\n",
        "        X_normalized = scaler_X.fit_transform(X_data)\n",
        "        Y_normalized = scaler_Y.fit_transform(Y_data)\n",
        "\n",
        "        # Créer plusieurs seeds en découpant les données différemment\n",
        "        for seed in range(n_seeds_per_client):\n",
        "            # Créer des séquences de données\n",
        "            X_sequences, Y_sequences = create_sequences(\n",
        "                X_normalized, Y_normalized, sequence_length, offset=seed\n",
        "            )\n",
        "\n",
        "            # Vérifier que nous avons suffisamment de données\n",
        "            if len(X_sequences) == 0:\n",
        "                raise ValueError(f\"Pas assez de données pour créer des séquences pour le pays {country}\")\n",
        "\n",
        "            # Reformater pour LSTM (n_x, batch_size, sequence_length)\n",
        "            X_train = np.transpose(X_sequences, (2, 0, 1))  # (n_x, batch_size, sequence_length)\n",
        "            Y_train = np.transpose(Y_sequences, (1, 0, 2))  # (n_y, batch_size, sequence_length)\n",
        "\n",
        "            client_seeds_data.append((X_train, Y_train))\n",
        "\n",
        "        clients_data.append(client_seeds_data)\n",
        "\n",
        "    if len(clients_data) == 0:\n",
        "        raise ValueError(\"Aucune donnée client n'a pu être chargée\")\n",
        "\n",
        "    return clients_data\n",
        "\n",
        "def create_sequences(X, Y, sequence_length, offset=0):\n",
        "    \"\"\"\n",
        "    Crée des séquences temporelles à partir des données.\n",
        "\n",
        "    Arguments:\n",
        "    X -- données d'entrée, array de forme (n_samples, n_features)\n",
        "    Y -- données de sortie, array de forme (n_samples, n_outputs)\n",
        "    sequence_length -- longueur de la séquence\n",
        "    offset -- décalage pour créer différentes séquences (pour les seeds)\n",
        "\n",
        "    Returns:\n",
        "    X_sequences -- séquences d'entrée\n",
        "    Y_sequences -- séquences de sortie\n",
        "    \"\"\"\n",
        "    X_sequences = []\n",
        "    Y_sequences = []\n",
        "\n",
        "    start_idx = offset % (sequence_length // 2) if offset > 0 else 0\n",
        "\n",
        "    for i in range(start_idx, len(X) - sequence_length + 1, sequence_length // 2):\n",
        "        X_sequences.append(X[i:i+sequence_length])\n",
        "        Y_sequences.append(Y[i:i+sequence_length])\n",
        "\n",
        "    return np.array(X_sequences), np.array(Y_sequences)\n",
        "\n",
        "def create_test_dataset(clients_data, batch_size, n_x, n_y, sequence_length):\n",
        "    \"\"\"\n",
        "    Crée un jeu de données de test à partir d'une fraction des données de chaque client.\n",
        "\n",
        "    Arguments:\n",
        "    clients_data -- données de tous les clients\n",
        "    batch_size -- taille du batch pour le test\n",
        "    n_x -- dimension d'entrée\n",
        "    n_y -- dimension de sortie\n",
        "    sequence_length -- longueur de la séquence temporelle\n",
        "\n",
        "    Returns:\n",
        "    X_test -- données de test, array de forme (n_x, batch_size, sequence_length)\n",
        "    Y_test -- étiquettes de test, array de forme (n_y, batch_size, sequence_length)\n",
        "    \"\"\"\n",
        "    # Extraire un échantillon de chaque client pour le test\n",
        "    test_samples_X = []\n",
        "    test_samples_Y = []\n",
        "\n",
        "    for client_data in clients_data:\n",
        "        # Prendre la première seed de chaque client\n",
        "        X, Y = client_data[0]\n",
        "\n",
        "        # Prendre 20% des échantillons pour le test\n",
        "        n_samples = X.shape[1]\n",
        "        n_test = min(batch_size // len(clients_data), n_samples // 5)\n",
        "\n",
        "        test_samples_X.append(X[:, :n_test, :])\n",
        "        test_samples_Y.append(Y[:, :n_test, :])\n",
        "\n",
        "    # Concaténer les échantillons de test\n",
        "    X_test = np.concatenate(test_samples_X, axis=1)\n",
        "    Y_test = np.concatenate(test_samples_Y, axis=1)\n",
        "\n",
        "    # S'assurer que la taille du batch est correcte\n",
        "    if X_test.shape[1] > batch_size:\n",
        "        X_test = X_test[:, :batch_size, :]\n",
        "        Y_test = Y_test[:, :batch_size, :]\n",
        "\n",
        "    return X_test, Y_test\n",
        "\n",
        "def federated_main_local_clustering(\n",
        "    n_clients=3,  # 3 pays comme clients\n",
        "    n_seeds_per_client=5,\n",
        "    n_epochs=30,\n",
        "    n_clusters=3,\n",
        "    n_a=64,\n",
        "    n_x=9,  # Nombre de features d'entrée (temp, humidity, windspeed, etc.)\n",
        "    n_y=1,  # Prédiction de Consumption\n",
        "    batch_size=32,\n",
        "    sequence_length=24,  # Par exemple, 24 heures\n",
        "    num_transfer_epochs=5,\n",
        "    learning_rate=0.01,\n",
        "    use_synthetic_data=False  # Utilisez vos données réelles\n",
        "):\n",
        "    print(\"=== Lancement du Federated Learning avec compression markovienne locale ===\")\n",
        "\n",
        "    # Pour mesurer le temps d'exécution total\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Charger les données réelles\n",
        "    if not use_synthetic_data:\n",
        "        print(\"Chargement des données réelles...\")\n",
        "        clients_data = load_real_data(n_clients, n_seeds_per_client, n_x, n_y, sequence_length)\n",
        "\n",
        "        # Données de test (par exemple, utiliser un sous-ensemble des données)\n",
        "        X_test, Y_test = create_test_dataset(clients_data, batch_size, n_x, n_y, sequence_length)\n",
        "\n",
        "    else:\n",
        "        # Ici, vous pourriez charger des données réelles\n",
        "        # Pour l'instant, utiliser les données synthétiques\n",
        "        print(\"Pas de données réelles disponibles, utilisation de données synthétiques...\")\n",
        "        clients_data = []\n",
        "\n",
        "        for client_id in range(n_clients):\n",
        "            client_seeds_data = []\n",
        "\n",
        "            for seed in range(n_seeds_per_client):\n",
        "                X_train = np.random.randn(n_x, batch_size, sequence_length)\n",
        "                Y_train = np.zeros((n_y, batch_size, sequence_length))\n",
        "                for t in range(sequence_length):\n",
        "                    for i in range(batch_size):\n",
        "                        class_idx = np.random.randint(0, n_y)\n",
        "                        Y_train[class_idx, i, t] = 1\n",
        "                client_seeds_data.append((X_train, Y_train))\n",
        "\n",
        "            clients_data.append(client_seeds_data)\n",
        "\n",
        "        # Données de test\n",
        "        X_test = np.random.randn(n_x, batch_size//2, sequence_length)\n",
        "        Y_test = np.zeros((n_y, batch_size//2, sequence_length))\n",
        "        for t in range(sequence_length):\n",
        "            for i in range(batch_size//2):\n",
        "                class_idx = np.random.randint(0, n_y)\n",
        "                Y_test[class_idx, i, t] = 1\n",
        "\n",
        "    # 1. Entraînement local sur chaque client avec plusieurs seeds\n",
        "    print(\"\\n=== Entraînement local des clients avec plusieurs seeds ===\")\n",
        "    local_training_start = time.time()\n",
        "    parameters_history_by_client_seed = []\n",
        "\n",
        "    for client_id, client_seeds_data in enumerate(clients_data):\n",
        "        print(f\"\\nClient {client_id+1}/{n_clients}\")\n",
        "        client_seeds_history = []\n",
        "\n",
        "        for seed_id, (X_c, Y_c) in enumerate(client_seeds_data):\n",
        "            print(f\"  Seed {seed_id+1}/{n_seeds_per_client}\")\n",
        "            base_seed = client_id * 100 + seed_id  # Pour assurer l'unicité des seeds\n",
        "            params, history, _ = train_lstm(X_c, Y_c, n_a, n_x, n_y,\n",
        "                                          num_epochs=n_epochs, seed=base_seed)\n",
        "            client_seeds_history.append(history)\n",
        "\n",
        "        parameters_history_by_client_seed.append(client_seeds_history)\n",
        "\n",
        "    local_training_time = time.time() - local_training_start\n",
        "    print(f\"Temps d'entraînement local total: {local_training_time:.2f} secondes\")\n",
        "\n",
        "    # 2. Clusterisation locale par client\n",
        "    print(\"\\n=== Clusterisation locale par client ===\")\n",
        "    clustering_start = time.time()\n",
        "\n",
        "    client_clusters, param_shapes, param_sizes = cluster_local_by_client(\n",
        "        parameters_history_by_client_seed, n_clusters=n_clusters\n",
        "    )\n",
        "\n",
        "    clustering_time = time.time() - clustering_start\n",
        "    print(f\"Temps de clusterisation locale: {clustering_time:.2f} secondes\")\n",
        "\n",
        "    # 3. Agrégation des clusters des clients\n",
        "    print(\"\\n=== Agrégation des clusters des clients ===\")\n",
        "    aggregation_start = time.time()\n",
        "\n",
        "    # Agréger les clusters pour obtenir le modèle global\n",
        "    local_compressed_params = aggregate_client_clusters(\n",
        "        client_clusters, param_shapes, param_sizes, n_clients\n",
        "    )\n",
        "\n",
        "    aggregation_time = time.time() - aggregation_start\n",
        "    print(f\"Temps d'agrégation: {aggregation_time:.2f} secondes\")\n",
        "\n",
        "    # 4. Évaluation des performances du modèle\n",
        "    print(\"\\n=== Évaluation des performances ===\")\n",
        "\n",
        "    # 4.1 Modèle global obtenu par agrégation des clusters locaux\n",
        "    local_compressed_acc, local_compressed_loss = evaluate_lstm(X_test, Y_test, local_compressed_params)\n",
        "    print(f\"Modèle compressé local - Accuracy: {local_compressed_acc:.4f}, Loss: {local_compressed_loss:.4f}\")\n",
        "\n",
        "    # 4.2 Obtenir le modèle original (méthode FedAvg standard) pour comparaison\n",
        "    # Extraire les derniers paramètres de chaque client pour chaque seed\n",
        "    last_params_by_client = []\n",
        "    for client_id in range(n_clients):\n",
        "        client_seed_last_params = []\n",
        "        for seed_id in range(n_seeds_per_client):\n",
        "            client_seed_last_params.append(parameters_history_by_client_seed[client_id][seed_id][-1])\n",
        "\n",
        "        # Moyenne des paramètres sur toutes les seeds pour ce client\n",
        "        avg_params = {}\n",
        "        for key in client_seed_last_params[0].keys():\n",
        "            avg_params[key] = np.mean([params[key] for params in client_seed_last_params], axis=0)\n",
        "\n",
        "        last_params_by_client.append(avg_params)\n",
        "\n",
        "    # Moyenne des paramètres sur tous les clients (FedAvg standard)\n",
        "    original_params = {}\n",
        "    for key in last_params_by_client[0].keys():\n",
        "        original_params[key] = np.mean([params[key] for params in last_params_by_client], axis=0)\n",
        "\n",
        "    original_acc, original_loss = evaluate_lstm(X_test, Y_test, original_params)\n",
        "    print(f\"Original (FedAvg) - Accuracy: {original_acc:.4f}, Loss: {original_loss:.4f}\")\n",
        "\n",
        "    # 5. Calculer la taille de transmission des paramètres\n",
        "    original_size = calculate_transmission_size(original_params)\n",
        "\n",
        "    # Pour la méthode locale, nous transmettons les centres de clusters et matrices de transition\n",
        "    # de chaque client au serveur\n",
        "    local_compressed_size = calculate_transmission_size(parameters=None, client_centers=client_clusters)\n",
        "\n",
        "    print(f\"Taille originale (FedAvg): {original_size/1024:.2f} KB\")\n",
        "    print(f\"Taille compressée (locale): {local_compressed_size/1024:.2f} KB\")\n",
        "    print(f\"Réduction: {(1 - local_compressed_size/original_size)*100:.2f}%\")\n",
        "\n",
        "    # 6. Réentraînement à partir du modèle compressé (transfer learning)\n",
        "    print(\"\\n=== Réentraînement à partir du modèle compressé local ===\")\n",
        "    transfer_start = time.time()\n",
        "\n",
        "    local_transfer_params, local_transfer_loss_history, local_transfer_acc_history = transfer_learning(\n",
        "        X_train=X_test, Y_train=Y_test,\n",
        "        X_test=X_test, Y_test=Y_test,\n",
        "        source_parameters=local_compressed_params,\n",
        "        n_a=n_a, n_x=n_x, n_y=n_y,\n",
        "        num_epochs=num_transfer_epochs,\n",
        "        learning_rate=learning_rate\n",
        "    )\n",
        "\n",
        "    transfer_time = time.time() - transfer_start\n",
        "    print(f\"Temps de transfer learning: {transfer_time:.2f} secondes\")\n",
        "\n",
        "    local_transfer_acc = local_transfer_acc_history[-1]\n",
        "    local_transfer_loss = local_transfer_loss_history[-1]\n",
        "\n",
        "    # 7. Simulation de la méthode traditionnelle (transmission complète des paramètres)\n",
        "    print(\"\\n=== Simulation de la méthode traditionnelle (FedAvg) ===\")\n",
        "    full_params_avg = original_params  # Déjà calculé précédemment\n",
        "\n",
        "    # Évaluer les performances de FedAvg après transfer learning\n",
        "    full_transfer_params, full_transfer_loss_history, full_transfer_acc_history = transfer_learning(\n",
        "        X_train=X_test, Y_train=Y_test,\n",
        "        X_test=X_test, Y_test=Y_test,\n",
        "        source_parameters=full_params_avg,\n",
        "        n_a=n_a, n_x=n_x, n_y=n_y,\n",
        "        num_epochs=num_transfer_epochs,\n",
        "        learning_rate=learning_rate\n",
        "    )\n",
        "\n",
        "    full_transfer_acc = full_transfer_acc_history[-1]\n",
        "    full_transfer_loss = full_transfer_loss_history[-1]\n",
        "\n",
        "    print(f\"FedAvg après transfer - Accuracy: {full_transfer_acc:.4f}, Loss: {full_transfer_loss:.4f}\")\n",
        "\n",
        "    # 8. Analyse de la complexité computationnelle\n",
        "    complexity_dict = analyze_computational_complexity_local(\n",
        "        n_clients, n_seeds_per_client, n_a, n_x, n_y, n_epochs, n_clusters, sequence_length, batch_size\n",
        "    )\n",
        "\n",
        "    # 9. Visualisations\n",
        "    # Visualiser les résultats de performance\n",
        "    visualize_results_local(\n",
        "        original_acc, local_compressed_acc, local_transfer_acc, full_transfer_acc,\n",
        "        original_loss, local_compressed_loss, local_transfer_loss, full_transfer_loss\n",
        "    )\n",
        "\n",
        "    # Visualiser les matrices de transition de chaque client\n",
        "    visualize_transition_matrices_local(client_clusters)\n",
        "\n",
        "    # Visualiser l'économie de bande passante\n",
        "    visualize_bandwidth_savings(original_size, local_compressed_size)\n",
        "\n",
        "    # Visualiser la complexité computationnelle\n",
        "    visualize_computational_complexity_local(complexity_dict)\n",
        "\n",
        "    # Calculer le temps total d'exécution\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nTemps total d'exécution: {total_time:.2f} secondes\")\n",
        "\n",
        "    # Résumé des résultats\n",
        "    print(\"\\n=== Résumé ===\")\n",
        "    print(f\"Taux de compression : {original_size/local_compressed_size:.2f}x\")\n",
        "    print(f\"Économie de bande passante : {(1 - local_compressed_size/original_size)*100:.2f}%\")\n",
        "    print(f\"Perte relative de performance (compression) : {(original_acc - local_compressed_acc)/original_acc*100:.2f}%\")\n",
        "    print(f\"Récupération après transfert : {(local_transfer_acc - local_compressed_acc)/local_compressed_acc*100:.2f}%\")\n",
        "    print(f\"Efficacité computationnelle : {complexity_dict['Efficiency Ratio']:.2f}x\")\n",
        "    print(f\"Comparaison avec FedAvg après transfert: {(local_transfer_acc/full_transfer_acc)*100:.2f}% des performances\")\n",
        "\n",
        "    # Collecter tous les résultats dans un dictionnaire\n",
        "    results = {\n",
        "        \"original_acc\": original_acc,\n",
        "        \"local_compressed_acc\": local_compressed_acc,\n",
        "        \"local_transfer_acc\": local_transfer_acc,\n",
        "        \"full_transfer_acc\": full_transfer_acc,\n",
        "        \"original_loss\": original_loss,\n",
        "        \"local_compressed_loss\": local_compressed_loss,\n",
        "        \"local_transfer_loss\": local_transfer_loss,\n",
        "        \"full_transfer_loss\": full_transfer_loss,\n",
        "        \"original_size\": original_size,\n",
        "        \"local_compressed_size\": local_compressed_size,\n",
        "        \"bandwidth_saving\": (1 - local_compressed_size/original_size)*100,\n",
        "        \"compression_ratio\": original_size/local_compressed_size,\n",
        "        \"performance_loss\": (original_acc - local_compressed_acc)/original_acc*100,\n",
        "        \"performance_recovery\": (local_transfer_acc - local_compressed_acc)/local_compressed_acc*100,\n",
        "        \"performance_vs_full\": (local_transfer_acc/full_transfer_acc)*100,\n",
        "        \"computational_efficiency\": complexity_dict['Efficiency Ratio'],\n",
        "        \"total_time\": total_time,\n",
        "        \"local_training_time\": local_training_time,\n",
        "        \"clustering_time\": clustering_time,\n",
        "        \"aggregation_time\": aggregation_time,\n",
        "        \"transfer_time\": transfer_time,\n",
        "        \"complexity\": complexity_dict,\n",
        "        \"n_clients\": n_clients,\n",
        "        \"n_seeds_per_client\": n_seeds_per_client,\n",
        "        \"n_epochs\": n_epochs,\n",
        "        \"n_clusters\": n_clusters\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Fonction principale pour comparer les approches de clusterisation globale et locale.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"COMPARAISON DES MÉTHODES DE COMPRESSION MARKOVIENNE DES PARAMÈTRES LSTM\")\n",
        "    print(\"=\" * 80)\n",
        "    setup_google_colab()\n",
        "    # Paramètres communs\n",
        "    params = {\n",
        "        \"n_clients\": 3,           # Nombre de clients\n",
        "        \"n_epochs\": 50,            # Nombre d'époques pour l'entraînement\n",
        "        \"n_clusters\": 3,          # Nombre de clusters pour la compression\n",
        "        \"n_a\": 32,                # Dimension cachée\n",
        "        \"n_x\": 8,                 # Dimension d'entrée\n",
        "        \"n_y\": 5,                 # Nombre de classes\n",
        "        \"batch_size\": 32,         # Taille du batch\n",
        "        \"sequence_length\": 10,    # Longueur de séquence\n",
        "        \"num_transfer_epochs\": 3, # Époques pour le transfer learning\n",
        "        \"learning_rate\": 0.01,    # Taux d'apprentissage\n",
        "        \"use_synthetic_data\": True # Utiliser des données synthétiques\n",
        "    }\n",
        "\n",
        "    # 1. Exécuter l'approche de clusterisation globale\n",
        "    print(\"\\n\\n\" + \"=\" * 80)\n",
        "    print(\"APPROCHE GLOBALE : Clusterisation sur l'ensemble des clients\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    results_global = federated_main(**params)\n",
        "\n",
        "    # 2. Exécuter l'approche de clusterisation locale\n",
        "    print(\"\\n\\n\" + \"=\" * 80)\n",
        "    print(\"APPROCHE LOCALE : Clusterisation individuelle par client\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Ajouter le paramètre spécifique à l'approche locale\n",
        "    params_local = params.copy()\n",
        "    params_local[\"n_seeds_per_client\"] = 5  # Nombre de seeds par client\n",
        "\n",
        "    results_local = federated_main_local_clustering(**params_local)\n",
        "\n",
        "    # 3. Comparer les résultats\n",
        "    compare_approaches(results_global, results_local)\n",
        "\n",
        "    return results_global, results_local\n",
        "def analyze_computational_complexity_local(n_clients, n_seeds_per_client, n_a, n_x, n_y, n_epochs,\n",
        "                                          n_clusters, sequence_length, batch_size):\n",
        "    \"\"\"\n",
        "    Analyse la complexité computationnelle du processus d'apprentissage fédéré avec clusterisation locale.\n",
        "    \"\"\"\n",
        "    complexity_dict = {}\n",
        "\n",
        "    # Dimension des paramètres LSTM\n",
        "    d = n_a**2 + n_a*n_x + n_a*n_y + n_a + n_y*n_a + n_y  # Dimension totale des paramètres\n",
        "\n",
        "    # 1. Complexité de l'entraînement local sur un client pour chaque seed (par époque)\n",
        "    forward_complexity = batch_size * sequence_length * (n_a**2 + n_a*n_x + n_a*n_y)\n",
        "    backward_complexity = batch_size * sequence_length * (n_a**2 + n_a*n_x + n_a*n_y)\n",
        "    client_seed_training_complexity = n_epochs * (forward_complexity + backward_complexity)\n",
        "    client_training_complexity = n_seeds_per_client * client_seed_training_complexity\n",
        "\n",
        "    complexity_dict[\"Local Training (per client)\"] = client_training_complexity\n",
        "    complexity_dict[\"Local Training (per seed)\"] = client_seed_training_complexity\n",
        "    complexity_dict[\"Local Training (all clients)\"] = n_clients * client_training_complexity\n",
        "\n",
        "    # 2. Complexité de la clusterisation locale\n",
        "    kmeans_iterations = 100  # Hypothèse pour le nombre d'itérations K-means\n",
        "    n_samples = n_seeds_per_client * n_epochs  # Nombre d'échantillons à clusteriser\n",
        "\n",
        "    # Complexité de K-means pour un client\n",
        "    clustering_complexity_per_client = kmeans_iterations * n_samples * n_clusters * d\n",
        "    clustering_complexity_all_clients = n_clients * clustering_complexity_per_client\n",
        "\n",
        "    complexity_dict[\"Local Clustering (per client)\"] = clustering_complexity_per_client\n",
        "    complexity_dict[\"Local Clustering (all clients)\"] = clustering_complexity_all_clients\n",
        "\n",
        "    # 3. Complexité du calcul des matrices de transition locales\n",
        "    transition_matrix_complexity_per_client = n_seeds_per_client * n_epochs * n_clusters**2\n",
        "    transition_matrix_complexity_all_clients = n_clients * transition_matrix_complexity_per_client\n",
        "\n",
        "    complexity_dict[\"Transition Matrix Computation (per client)\"] = transition_matrix_complexity_per_client\n",
        "    complexity_dict[\"Transition Matrix Computation (all clients)\"] = transition_matrix_complexity_all_clients\n",
        "\n",
        "    # 4. Complexité de la communication client-serveur\n",
        "    cluster_centers_size = n_clusters * d  # Taille des centres de clusters\n",
        "    transition_matrix_size = n_clusters * n_clusters  # Taille de la matrice de transition\n",
        "\n",
        "    communication_complexity_per_client = cluster_centers_size + transition_matrix_size\n",
        "    communication_complexity_all_clients = n_clients * communication_complexity_per_client\n",
        "\n",
        "    traditional_communication_per_client = d\n",
        "    traditional_communication_all_clients = n_clients * traditional_communication_per_client\n",
        "\n",
        "    complexity_dict[\"Communication (proposed, per client)\"] = communication_complexity_per_client\n",
        "    complexity_dict[\"Communication (proposed, all clients)\"] = communication_complexity_all_clients\n",
        "    complexity_dict[\"Communication (traditional, per client)\"] = traditional_communication_per_client\n",
        "    complexity_dict[\"Communication (traditional, all clients)\"] = traditional_communication_all_clients\n",
        "    complexity_dict[\"Communication Reduction Ratio\"] = traditional_communication_all_clients / communication_complexity_all_clients\n",
        "\n",
        "    # 5. Complexité de l'agrégation au serveur\n",
        "    distribution_stationary_complexity = n_clients * n_clusters * n_clusters * 100\n",
        "    aggregation_complexity_proposed = distribution_stationary_complexity + n_clients * d\n",
        "\n",
        "    aggregation_complexity_traditional = n_clients * d\n",
        "\n",
        "    complexity_dict[\"Server Aggregation (proposed)\"] = aggregation_complexity_proposed\n",
        "    complexity_dict[\"Server Aggregation (traditional)\"] = aggregation_complexity_traditional\n",
        "\n",
        "    # 6. Complexité du transfer learning\n",
        "    transfer_epochs = 5  # Hypothèse\n",
        "    transfer_complexity = transfer_epochs * (forward_complexity + backward_complexity)\n",
        "\n",
        "    complexity_dict[\"Transfer Learning\"] = transfer_complexity\n",
        "\n",
        "    # 7. Complexité totale des approches\n",
        "    proposed_approach_complexity = (\n",
        "        n_clients * client_training_complexity +  # Entraînement local\n",
        "        clustering_complexity_all_clients +       # Clusterisation locale\n",
        "        transition_matrix_complexity_all_clients + # Calcul des matrices de transition\n",
        "        communication_complexity_all_clients +    # Communication client-serveur\n",
        "        aggregation_complexity_proposed +         # Agrégation au serveur\n",
        "        transfer_complexity                       # Transfer learning\n",
        "    )\n",
        "\n",
        "    traditional_approach_complexity = (\n",
        "        n_clients * client_training_complexity +  # Entraînement local (même coût)\n",
        "        traditional_communication_all_clients +   # Communication client-serveur\n",
        "        aggregation_complexity_traditional +      # Agrégation au serveur\n",
        "        transfer_complexity                       # Transfer learning (même coût)\n",
        "    )\n",
        "\n",
        "    complexity_dict[\"Total (Proposed Approach)\"] = proposed_approach_complexity\n",
        "    complexity_dict[\"Total (Traditional Approach)\"] = traditional_approach_complexity\n",
        "\n",
        "    efficiency_ratio = traditional_approach_complexity / proposed_approach_complexity\n",
        "    complexity_dict[\"Efficiency Ratio\"] = efficiency_ratio\n",
        "\n",
        "    # 8. Analyse par composante (pourcentage du temps total)\n",
        "    total_proposed = proposed_approach_complexity\n",
        "\n",
        "    complexity_dict[\"Local Training (% of total)\"] = n_clients * client_training_complexity / total_proposed * 100\n",
        "    complexity_dict[\"Local Clustering (% of total)\"] = clustering_complexity_all_clients / total_proposed * 100\n",
        "    complexity_dict[\"Transition Matrix (% of total)\"] = transition_matrix_complexity_all_clients / total_proposed * 100\n",
        "    complexity_dict[\"Communication (% of total)\"] = communication_complexity_all_clients / total_proposed * 100\n",
        "    complexity_dict[\"Server Aggregation (% of total)\"] = aggregation_complexity_proposed / total_proposed * 100\n",
        "    complexity_dict[\"Transfer Learning (% of total)\"] = transfer_complexity / total_proposed * 100\n",
        "\n",
        "    return complexity_dict\n",
        "\n",
        "def transfer_learning(X_train, Y_train, X_test, Y_test, source_parameters, n_a, n_x, n_y, num_epochs=5, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Effectue un transfert d'apprentissage à partir des paramètres source.\n",
        "    \"\"\"\n",
        "    # Utiliser les paramètres source comme initialisation\n",
        "    parameters = copy.deepcopy(source_parameters)\n",
        "\n",
        "    # Initialiser Adam\n",
        "    v, s = initialize_adam_for_lstm(parameters)\n",
        "    t = 0  # Compteur pour Adam\n",
        "\n",
        "    loss_history = []\n",
        "    accuracy_history = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Époque de transfert {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Forward pass\n",
        "        a0 = np.zeros((n_a, X_train.shape[1]))\n",
        "        a, y_pred, c, caches = lstm_forward(X_train, a0, parameters)\n",
        "\n",
        "        # Calcul de la perte (cross-entropy)\n",
        "        loss = -np.sum(Y_train * np.log(y_pred + 1e-8)) / (Y_train.shape[1] * Y_train.shape[2])\n",
        "\n",
        "        # Initialisation du gradient de sortie\n",
        "        da = np.zeros_like(a)\n",
        "\n",
        "        # Calculer les gradients pour Wy et by\n",
        "        dWy = np.zeros_like(parameters[\"Wy\"])\n",
        "        dby = np.zeros_like(parameters[\"by\"])\n",
        "\n",
        "        # Pour chaque pas de temps, calculer le gradient\n",
        "        for t_idx in range(Y_train.shape[2]):\n",
        "            # Gradient de la cross-entropy\n",
        "            dy = y_pred[:, :, t_idx] - Y_train[:, :, t_idx]\n",
        "            # Accumuler les gradients pour Wy et by\n",
        "            dWy += np.dot(dy, a[:, :, t_idx].T)\n",
        "            dby += np.sum(dy, axis=1, keepdims=True)\n",
        "            # Gradient par rapport à a\n",
        "            da[:, :, t_idx] = np.dot(parameters[\"Wy\"].T, dy)\n",
        "\n",
        "        # Backward pass pour les autres paramètres\n",
        "        lstm_gradients = lstm_backward(da, caches)\n",
        "\n",
        "        # Combiner tous les gradients\n",
        "        gradients = lstm_gradients.copy()\n",
        "        gradients[\"dWy\"] = dWy\n",
        "        gradients[\"dby\"] = dby\n",
        "\n",
        "        # Mise à jour des paramètres avec Adam\n",
        "        t += 1\n",
        "        parameters, v, s = update_parameters_with_adam_for_lstm(parameters, gradients, v, s, t, learning_rate)\n",
        "\n",
        "        # Évaluer sur l'ensemble de test\n",
        "        accuracy, test_loss = evaluate_lstm(X_test, Y_test, parameters)\n",
        "        loss_history.append(test_loss)\n",
        "        accuracy_history.append(accuracy)\n",
        "\n",
        "        print(f\"Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    return parameters, loss_history, accuracy_history\n",
        "\n",
        "def visualize_results_local(original_acc, local_compressed_acc, local_transfer_acc, full_transfer_acc,\n",
        "                           original_loss, local_compressed_loss, local_transfer_loss, full_transfer_loss):\n",
        "    \"\"\"\n",
        "    Visualise les résultats de l'expérience avec clusterisation locale.\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Graphique des précisions\n",
        "    labels = ['Original', 'Compressé Local', 'Transfert Local', 'FedAvg+Transfert']\n",
        "    accuracies = [original_acc, local_compressed_acc, local_transfer_acc, full_transfer_acc]\n",
        "    colors = ['blue', 'orange', 'green', 'red']\n",
        "    ax1.bar(labels, accuracies, color=colors)\n",
        "    ax1.set_ylabel('Précision')\n",
        "    ax1.set_title('Comparaison des précisions')\n",
        "\n",
        "    # Ajouter les valeurs numériques sur les barres\n",
        "    for i, v in enumerate(accuracies):\n",
        "        ax1.text(i, v + 0.01, f\"{v:.3f}\", ha='center')\n",
        "\n",
        "    # Graphique des pertes\n",
        "    losses = [original_loss, local_compressed_loss, local_transfer_loss, full_transfer_loss]\n",
        "    ax2.bar(labels, losses, color=colors)\n",
        "    ax2.set_ylabel('Perte')\n",
        "    ax2.set_title('Comparaison des pertes')\n",
        "\n",
        "    # Ajouter les valeurs numériques sur les barres\n",
        "    for i, v in enumerate(losses):\n",
        "        ax2.text(i, v + 0.1, f\"{v:.3f}\", ha='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('resultats_compression_locale_lstm.png')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_transition_matrices_local(client_clusters):\n",
        "    \"\"\"\n",
        "    Visualise les matrices de transition de chaque client.\n",
        "    \"\"\"\n",
        "    n_clients = len(client_clusters)\n",
        "\n",
        "    # Déterminer la disposition optimale des sous-figures\n",
        "    n_cols = min(3, n_clients)\n",
        "    n_rows = (n_clients + n_cols - 1) // n_cols\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
        "\n",
        "    # Gérer le cas d'un seul client (axes n'est pas un tableau)\n",
        "    if n_clients == 1:\n",
        "        axes = np.array([axes])\n",
        "\n",
        "    # Aplatir le tableau d'axes pour itération facile\n",
        "    if n_rows * n_cols > 1:\n",
        "        axes = axes.flatten()\n",
        "\n",
        "    for i, (_, transition_matrix) in enumerate(client_clusters):\n",
        "        if i < len(axes):\n",
        "            ax = axes[i]\n",
        "            im = ax.imshow(transition_matrix, cmap='viridis', interpolation='none')\n",
        "            ax.set_title(f'Client {i+1}')\n",
        "            ax.set_xlabel('Cluster de destination')\n",
        "            ax.set_ylabel('Cluster de départ')\n",
        "\n",
        "            # Ajouter les valeurs sur la figure\n",
        "            for row in range(transition_matrix.shape[0]):\n",
        "                for col in range(transition_matrix.shape[1]):\n",
        "                    ax.text(col, row, f'{transition_matrix[row, col]:.2f}',\n",
        "                             ha='center', va='center',\n",
        "                             color='white' if transition_matrix[row, col] > 0.5 else 'black')\n",
        "\n",
        "    # Masquer les axes supplémentaires s'il y en a\n",
        "    for i in range(n_clients, len(axes)):\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    # Ajouter une barre de couleur commune\n",
        "    fig.colorbar(im, ax=axes.tolist(), label='Probabilité de transition')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('matrices_transition_locales.png')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_computational_complexity_local(complexity_dict):\n",
        "    \"\"\"\n",
        "    Visualise la complexité computationnelle pour l'approche de clusterisation locale.\n",
        "    \"\"\"\n",
        "    # Extraction des composantes principales pour la visualisation\n",
        "    components = [\"Local Training (per client)\", \"Local Clustering (per client)\",\n",
        "                  \"Transition Matrix Computation (per client)\",\n",
        "                  \"Server Aggregation (proposed)\", \"Transfer Learning\"]\n",
        "\n",
        "    # Vérifier que toutes les clés existent\n",
        "    for comp in components:\n",
        "        if comp not in complexity_dict:\n",
        "            print(f\"Attention: Clé '{comp}' non trouvée dans complexity_dict\")\n",
        "            # Remplacer par une valeur par défaut pour éviter l'erreur\n",
        "            complexity_dict[comp] = 0\n",
        "\n",
        "    values = [complexity_dict[comp] for comp in components]\n",
        "\n",
        "    # Normalisation pour une meilleure visualisation\n",
        "    total = sum(values)\n",
        "    if total > 0:\n",
        "        normalized_values = np.array(values) / total * 100\n",
        "    else:\n",
        "        normalized_values = np.zeros_like(values)\n",
        "\n",
        "    # Création du graphique\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    bars = plt.bar(components, normalized_values, color=['blue', 'green', 'orange', 'red', 'purple'])\n",
        "\n",
        "    # Ajout des valeurs en pourcentage\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "                 f'{height:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "    plt.title('Décomposition de la complexité computationnelle (Clusterisation Locale)')\n",
        "    plt.ylabel('Pourcentage du temps de calcul total')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('decomposition_complexite_locale.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Comparaison des approches\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    ratio = complexity_dict.get(\"Efficiency Ratio\", 1.0)  # Valeur par défaut de 1.0\n",
        "    plt.bar(['Approche traditionnelle (FedAvg)', 'Approche proposée (Locale)'],\n",
        "            [100, 100/ratio if ratio > 0 else 0],\n",
        "            color=['gray', 'green'])\n",
        "    plt.title(f'Comparaison de l\\'efficacité (Traditionnel / Proposé = {ratio:.2f}x)')\n",
        "    plt.ylabel('Complexité relative (%)')\n",
        "    plt.ylim(0, max(100, 100/ratio if ratio > 0 else 0) * 1.1)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('comparaison_complexite_locale.png')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_bandwidth_savings(original_size, compressed_size):\n",
        "    \"\"\"\n",
        "    Visualise l'économie de bande passante.\n",
        "    \"\"\"\n",
        "    # Vérifier que original_size n'est pas zéro pour éviter division par zéro\n",
        "    if original_size <= 0:\n",
        "        print(\"AVERTISSEMENT: La taille originale est nulle ou négative, impossible de calculer le pourcentage d'économie.\")\n",
        "        saved_percentage = 0\n",
        "    else:\n",
        "        saved_percentage = (1 - compressed_size / original_size) * 100\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Barres de taille\n",
        "    bars = plt.bar(['Paramètres originaux', 'Paramètres compressés'],\n",
        "                   [original_size/1024 if original_size > 0 else 0, compressed_size/1024],\n",
        "                   color=['blue', 'green'])\n",
        "\n",
        "    # Ajouter valeurs numériques\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                 f'{height:.2f} KB', ha='center', va='bottom')\n",
        "\n",
        "    if original_size > 0:\n",
        "        plt.title(f'Économie de bande passante: {saved_percentage:.2f}%')\n",
        "    else:\n",
        "        plt.title('Économie de bande passante: Non calculable (taille originale nulle)')\n",
        "    plt.ylabel('Taille (KB)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('economies_bande_passante.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Diagramme circulaire pour visualiser les proportions - uniquement si les valeurs sont valides\n",
        "    if original_size > 0 and compressed_size < original_size:  # Cas normal: compression réduit la taille\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.pie([compressed_size, original_size - compressed_size],\n",
        "                labels=['Utilisé', 'Économisé'],\n",
        "                colors=['green', 'lightgray'],\n",
        "                autopct='%1.1f%%',\n",
        "                startangle=90,\n",
        "                explode=(0, 0.1))\n",
        "        plt.axis('equal')\n",
        "        plt.title(f'Économie de bande passante: {saved_percentage:.2f}%')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('pourcentage_economie.png')\n",
        "        plt.show()\n",
        "    elif original_size > 0 and compressed_size > original_size:  # Cas où la compression augmente la taille\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.pie([original_size, compressed_size - original_size],\n",
        "                labels=['Taille originale', 'Surcoût de compression'],\n",
        "                colors=['blue', 'red'],\n",
        "                autopct='%1.1f%%',\n",
        "                startangle=90,\n",
        "                explode=(0, 0.1))\n",
        "        plt.axis('equal')\n",
        "        plt.title(f'Augmentation de la taille: {-saved_percentage:.2f}%')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('pourcentage_surcout.png')\n",
        "        plt.show()\n",
        "    # Pas de diagramme si original_size est nul ou négatif\n",
        "\n",
        "def update_with_transition_matrices(client_transition_matrices, global_kmeans_model, initial_global_params, param_shapes, param_sizes):\n",
        "    \"\"\"\n",
        "    Met à jour le modèle global en utilisant uniquement les matrices de transition des clients.\n",
        "\n",
        "    Arguments:\n",
        "    client_transition_matrices -- liste des matrices de transition de chaque client\n",
        "    global_kmeans_model -- modèle KMeans global utilisé pour la clusterisation\n",
        "    initial_global_params -- paramètres initiaux obtenus par FedAvg\n",
        "    param_shapes -- formes des paramètres\n",
        "    param_sizes -- tailles des paramètres aplatis\n",
        "\n",
        "    Returns:\n",
        "    updated_global_params -- paramètres globaux mis à jour\n",
        "    \"\"\"\n",
        "    n_clients = len(client_transition_matrices)\n",
        "    n_clusters = global_kmeans_model.cluster_centers_.shape[0]\n",
        "\n",
        "    # Agréger les matrices de transition des clients\n",
        "    global_transition_matrix = np.zeros((n_clusters, n_clusters))\n",
        "    for client_id in range(n_clients):\n",
        "        global_transition_matrix += client_transition_matrices[client_id]\n",
        "\n",
        "    # Normaliser la matrice de transition globale\n",
        "    for i in range(n_clusters):\n",
        "        row_sum = np.sum(global_transition_matrix[i])\n",
        "        if row_sum > 0:\n",
        "            global_transition_matrix[i] = global_transition_matrix[i] / row_sum\n",
        "        else:\n",
        "            global_transition_matrix[i] = np.ones(n_clusters) / n_clusters\n",
        "\n",
        "    # Calculer la distribution stationnaire de la matrice de transition\n",
        "    pi = np.ones(n_clusters) / n_clusters  # Distribution initiale uniforme\n",
        "    for _ in range(100):  # Nombre d'itérations arbitraire pour convergence\n",
        "        pi_new = np.dot(pi, global_transition_matrix)\n",
        "        if np.allclose(pi, pi_new):\n",
        "            break\n",
        "        pi = pi_new\n",
        "\n",
        "    # Créer le nouveau modèle global en pondérant les centres par la distribution stationnaire\n",
        "    weighted_centers = np.zeros_like(global_kmeans_model.cluster_centers_[0])\n",
        "    for i in range(n_clusters):\n",
        "        weighted_centers += pi[i] * global_kmeans_model.cluster_centers_[i]\n",
        "\n",
        "    # Convertir les paramètres aplatis en structure de dictionnaire\n",
        "    updated_global_params = unflatten_parameters(weighted_centers, param_shapes, param_sizes)\n",
        "\n",
        "    return updated_global_params\n",
        "def visualize_communication_rounds(communication_results):\n",
        "    \"\"\"\n",
        "    Visualise les résultats pour chaque cycle de communication.\n",
        "\n",
        "    Arguments:\n",
        "    communication_results -- liste de dictionnaires contenant les résultats pour chaque cycle\n",
        "    \"\"\"\n",
        "    n_rounds = len(communication_results)\n",
        "    cycles = [result[\"cycle\"] + 1 for result in communication_results]  # +1 pour commencer à 1\n",
        "\n",
        "    # Extraction des données\n",
        "    accuracies = [result[\"accuracy\"] for result in communication_results]\n",
        "    losses = [result[\"loss\"] for result in communication_results]\n",
        "    traditional_bw = [result[\"traditional_bandwidth\"]/1024 for result in communication_results]  # KB\n",
        "    proposed_bw = [result[\"proposed_bandwidth\"]/1024 for result in communication_results]  # KB\n",
        "\n",
        "    # Créer une figure avec 2 sous-graphiques\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "    # Graphique des performances\n",
        "    ax1 = axes[0]\n",
        "    color = 'tab:blue'\n",
        "    ax1.set_xlabel('Cycle de communication')\n",
        "    ax1.set_ylabel('Précision', color=color)\n",
        "    ax1.plot(cycles, accuracies, 'o-', color=color, label='Précision')\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    # Ajouter la perte sur le même graphique avec un axe y secondaire\n",
        "    ax1_bis = ax1.twinx()\n",
        "    color = 'tab:red'\n",
        "    ax1_bis.set_ylabel('Perte', color=color)\n",
        "    ax1_bis.plot(cycles, losses, 's-', color=color, label='Perte')\n",
        "    ax1_bis.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    # Ajouter un titre et une légende\n",
        "    ax1.set_title('Évolution des performances par cycle de communication')\n",
        "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "    lines2, labels2 = ax1_bis.get_legend_handles_labels()\n",
        "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "    # Graphique de la bande passante\n",
        "    ax2 = axes[1]\n",
        "    ax2.set_xlabel('Cycle de communication')\n",
        "    ax2.set_ylabel('Bande passante (KB)')\n",
        "    ax2.bar(np.array(cycles) - 0.2, traditional_bw, width=0.4, color='gray', label='Traditionnelle (FedAvg)')\n",
        "    ax2.bar(np.array(cycles) + 0.2, proposed_bw, width=0.4, color='green', label='Proposée (Matrices de transition)')\n",
        "\n",
        "    # Ajouter les chiffres sur les barres\n",
        "    for i, (trad, prop) in enumerate(zip(traditional_bw, proposed_bw)):\n",
        "        ax2.text(cycles[i] - 0.2, trad + 0.5, f'{trad:.1f}', ha='center')\n",
        "        ax2.text(cycles[i] + 0.2, prop + 0.5, f'{prop:.1f}', ha='center')\n",
        "\n",
        "    # Ajouter pourcentage d'économie pour les cycles > 0\n",
        "    for i in range(1, n_rounds):\n",
        "        saving = (1 - proposed_bw[i]/traditional_bw[i]) * 100\n",
        "        ax2.text(cycles[i], max(traditional_bw[i], proposed_bw[i]) * 1.1,\n",
        "                 f'Économie: {saving:.1f}%', ha='center', fontweight='bold')\n",
        "\n",
        "    ax2.set_title('Comparaison de la bande passante par cycle de communication')\n",
        "    ax2.legend()\n",
        "\n",
        "    # Ajuster la mise en page\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('resultats_federated_two_phase.png')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_communication_rounds_comparison(communication_results):\n",
        "    \"\"\"\n",
        "    Visualise la comparaison des deux méthodes pour chaque cycle de communication.\n",
        "\n",
        "    Arguments:\n",
        "    communication_results -- liste de dictionnaires contenant les résultats pour chaque cycle\n",
        "    \"\"\"\n",
        "    n_rounds = len(communication_results)\n",
        "    cycles = [result[\"cycle\"] + 1 for result in communication_results]  # +1 pour commencer à 1\n",
        "\n",
        "    # Extraction des données\n",
        "    proposed_acc = [result[\"proposed_accuracy\"] for result in communication_results]\n",
        "    proposed_loss = [result[\"proposed_loss\"] for result in communication_results]\n",
        "    fedavg_acc = [result[\"fedavg_accuracy\"] for result in communication_results]\n",
        "    fedavg_loss = [result[\"fedavg_loss\"] for result in communication_results]\n",
        "    traditional_bw = [result[\"traditional_bandwidth\"]/1024 for result in communication_results]  # KB\n",
        "    proposed_bw = [result[\"proposed_bandwidth\"]/1024 for result in communication_results]  # KB\n",
        "\n",
        "    # Créer une figure avec 3 sous-graphiques\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(12, 15))\n",
        "\n",
        "    # Graphique de précision\n",
        "    ax1 = axes[0]\n",
        "    ax1.set_xlabel('Cycle de communication')\n",
        "    ax1.set_ylabel('Précision')\n",
        "    ax1.plot(cycles, proposed_acc, 'o-', color='blue', label='Méthode proposée')\n",
        "    ax1.plot(cycles, fedavg_acc, 's-', color='red', label='FedAvg traditionnel')\n",
        "\n",
        "    # Ajouter les valeurs sur les points\n",
        "    for i, (acc_p, acc_f) in enumerate(zip(proposed_acc, fedavg_acc)):\n",
        "        ax1.text(cycles[i], acc_p + 0.01, f'{acc_p:.3f}', ha='center')\n",
        "        ax1.text(cycles[i], acc_f - 0.02, f'{acc_f:.3f}', ha='center')\n",
        "\n",
        "    ax1.set_title('Comparaison de la précision')\n",
        "    ax1.legend()\n",
        "    ax1.grid(alpha=0.3)\n",
        "\n",
        "    # Graphique de perte\n",
        "    ax2 = axes[1]\n",
        "    ax2.set_xlabel('Cycle de communication')\n",
        "    ax2.set_ylabel('Perte')\n",
        "    ax2.plot(cycles, proposed_loss, 'o-', color='blue', label='Méthode proposée')\n",
        "    ax2.plot(cycles, fedavg_loss, 's-', color='red', label='FedAvg traditionnel')\n",
        "\n",
        "    # Ajouter les valeurs sur les points\n",
        "    for i, (loss_p, loss_f) in enumerate(zip(proposed_loss, fedavg_loss)):\n",
        "        ax2.text(cycles[i], loss_p + 0.02, f'{loss_p:.3f}', ha='center')\n",
        "        ax2.text(cycles[i], loss_f - 0.04, f'{loss_f:.3f}', ha='center')\n",
        "\n",
        "    ax2.set_title('Comparaison de la perte')\n",
        "    ax2.legend()\n",
        "    ax2.grid(alpha=0.3)\n",
        "\n",
        "    # Graphique de la bande passante\n",
        "    ax3 = axes[2]\n",
        "    ax3.set_xlabel('Cycle de communication')\n",
        "    ax3.set_ylabel('Bande passante (KB)')\n",
        "\n",
        "    # Largeur des barres\n",
        "    width = 0.35\n",
        "    x = np.array(cycles)\n",
        "\n",
        "    # Barres\n",
        "    ax3.bar(x - width/2, traditional_bw, width, color='red', label='FedAvg traditionnel')\n",
        "    ax3.bar(x + width/2, proposed_bw, width, color='blue', label='Méthode proposée')\n",
        "\n",
        "    # Ajouter les chiffres sur les barres\n",
        "    for i, (trad, prop) in enumerate(zip(traditional_bw, proposed_bw)):\n",
        "        ax3.text(cycles[i] - width/2, trad + max(traditional_bw)/40, f'{trad:.1f}', ha='center')\n",
        "        ax3.text(cycles[i] + width/2, prop + max(traditional_bw)/40, f'{prop:.1f}', ha='center')\n",
        "\n",
        "    # Ajouter pourcentage d'économie pour les cycles > 0\n",
        "    for i in range(1, n_rounds):\n",
        "        saving = (1 - proposed_bw[i]/traditional_bw[i]) * 100\n",
        "        ax3.text(cycles[i], max(traditional_bw[i], proposed_bw[i]) * 1.1,\n",
        "                 f'Économie: {saving:.1f}%', ha='center', fontweight='bold')\n",
        "\n",
        "    ax3.set_title('Comparaison de la bande passante')\n",
        "    ax3.legend()\n",
        "\n",
        "    # Ajuster la mise en page\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('resultats_comparaison_federated.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Créer un second graphique pour une comparaison directe des performances\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Pour la précision\n",
        "    plt.subplot(1, 2, 1)\n",
        "    x = np.arange(n_rounds)\n",
        "    width = 0.35\n",
        "    plt.bar(x - width/2, proposed_acc, width, label='Méthode proposée', color='blue')\n",
        "    plt.bar(x + width/2, fedavg_acc, width, label='FedAvg traditionnel', color='red')\n",
        "\n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for i, (acc_p, acc_f) in enumerate(zip(proposed_acc, fedavg_acc)):\n",
        "        plt.text(i - width/2, acc_p + 0.01, f'{acc_p:.3f}', ha='center')\n",
        "        plt.text(i + width/2, acc_f + 0.01, f'{acc_f:.3f}', ha='center')\n",
        "\n",
        "    plt.xlabel('Cycle de communication')\n",
        "    plt.ylabel('Précision')\n",
        "    plt.title('Comparaison de la précision')\n",
        "    plt.xticks(x, [f'Cycle {i+1}' for i in range(n_rounds)])\n",
        "    plt.legend()\n",
        "\n",
        "    # Pour la perte\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.bar(x - width/2, proposed_loss, width, label='Méthode proposée', color='blue')\n",
        "    plt.bar(x + width/2, fedavg_loss, width, label='FedAvg traditionnel', color='red')\n",
        "\n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for i, (loss_p, loss_f) in enumerate(zip(proposed_loss, fedavg_loss)):\n",
        "        plt.text(i - width/2, loss_p + 0.02, f'{loss_p:.3f}', ha='center')\n",
        "        plt.text(i + width/2, loss_f + 0.02, f'{loss_f:.3f}', ha='center')\n",
        "\n",
        "    plt.xlabel('Cycle de communication')\n",
        "    plt.ylabel('Perte')\n",
        "    plt.title('Comparaison de la perte')\n",
        "    plt.xticks(x, [f'Cycle {i+1}' for i in range(n_rounds)])\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('resultats_performances_comparaison.png')\n",
        "    plt.show()\n",
        "\n",
        "def federated_main_two_phase(\n",
        "    n_clients=3, n_epochs=50, n_clusters=3,\n",
        "    n_a=64, n_x=10, n_y=5,\n",
        "    batch_size=32, sequence_length=10,\n",
        "    num_transfer_epochs=5, learning_rate=0.01,\n",
        "    n_communication_rounds=6,  # Nombre total de cycles de communication\n",
        "    use_synthetic_data=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Pipeline complet de Federated Learning avec approche en deux phases :\n",
        "    Phase 1: Initialisation avec FedAvg\n",
        "    Phase 2: Mises à jour avec matrices de transition\n",
        "    Compare aussi avec la méthode traditionnelle FedAvg à chaque cycle\n",
        "    \"\"\"\n",
        "    print(\"=== Lancement du Federated Learning en deux phases ===\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Génération/chargement des données\n",
        "    if use_synthetic_data:\n",
        "        print(\"Génération des données synthétiques...\")\n",
        "        clients_data = []\n",
        "        for seed in range(n_clients):\n",
        "            X_train = np.random.randn(n_x, batch_size, sequence_length)\n",
        "            Y_train = np.zeros((n_y, batch_size, sequence_length))\n",
        "            for t in range(sequence_length):\n",
        "                for i in range(batch_size):\n",
        "                    class_idx = np.random.randint(0, n_y)\n",
        "                    Y_train[class_idx, i, t] = 1\n",
        "            clients_data.append((X_train, Y_train))\n",
        "\n",
        "        # Données de test\n",
        "        X_test = np.random.randn(n_x, batch_size//2, sequence_length)\n",
        "        Y_test = np.zeros((n_y, batch_size//2, sequence_length))\n",
        "        for t in range(sequence_length):\n",
        "            for i in range(batch_size//2):\n",
        "                class_idx = np.random.randint(0, n_y)\n",
        "                Y_test[class_idx, i, t] = 1\n",
        "    else:\n",
        "        # Code pour les données réelles\n",
        "        print(\"Chargement des données réelles...\")\n",
        "        clients_data = load_real_data(n_clients, 1, n_x, n_y, sequence_length)\n",
        "        # Extraire seulement la première seed pour chaque client\n",
        "        clients_data = [client_seeds[0] for client_seeds in clients_data]\n",
        "        # Créer des données de test\n",
        "        X_test, Y_test = create_test_dataset(clients_data, batch_size//2, n_x, n_y, sequence_length)\n",
        "\n",
        "    # Résultats pour chaque cycle de communication\n",
        "    communication_results = []\n",
        "    global_params = None\n",
        "    fedavg_params = None  # Pour garder une trace des paramètres FedAvg à chaque cycle\n",
        "    global_kmeans_model = None\n",
        "    param_shapes = None\n",
        "    param_sizes = None\n",
        "\n",
        "    # Pour le calcul de la bande passante\n",
        "    traditional_bandwidth = []\n",
        "    proposed_bandwidth = []\n",
        "\n",
        "    # Itérer sur les cycles de communication\n",
        "    for comm_round in range(n_communication_rounds):\n",
        "        print(f\"\\n=== Cycle de communication {comm_round+1}/{n_communication_rounds} ===\")\n",
        "\n",
        "        # Phase 1: Premier cycle - FedAvg complet\n",
        "        if comm_round == 0:\n",
        "            print(\"Phase 1: Initialisation avec FedAvg\")\n",
        "\n",
        "            # Entraînement local sur chaque client\n",
        "            parameters_by_client = []\n",
        "            parameters_history_by_client = []\n",
        "\n",
        "            for client_id, (X_c, Y_c) in enumerate(clients_data):\n",
        "                print(f\"Entraînement du client {client_id+1}/{n_clients}\")\n",
        "                params, history, _ = train_lstm(X_c, Y_c, n_a, n_x, n_y,\n",
        "                                              num_epochs=n_epochs, seed=client_id)\n",
        "                parameters_by_client.append(params)\n",
        "                parameters_history_by_client.append(history)\n",
        "\n",
        "            # FedAvg: Moyenne des paramètres des clients\n",
        "            global_params = {}\n",
        "            for key in parameters_by_client[0].keys():\n",
        "                global_params[key] = np.mean([params[key] for params in parameters_by_client], axis=0)\n",
        "\n",
        "            # Garder également ces paramètres comme référence FedAvg\n",
        "            fedavg_params = copy.deepcopy(global_params)\n",
        "\n",
        "            # Clusterisation globale des paramètres\n",
        "            kmeans_models, cluster_labels, flat_params, param_shapes, param_sizes = cluster_parameters_by_epoch(\n",
        "                parameters_history_by_client, n_clusters=n_clusters\n",
        "            )\n",
        "\n",
        "            # Stocker le dernier modèle KMeans comme modèle global\n",
        "            global_kmeans_model = kmeans_models[-1]\n",
        "\n",
        "            # Calcul de la bande passante pour la transmission complète des paramètres\n",
        "            initial_bandwidth = calculate_transmission_size(global_params) * n_clients  # Total pour tous les clients\n",
        "            traditional_bandwidth.append(initial_bandwidth)\n",
        "            proposed_bandwidth.append(initial_bandwidth)  # Première phase identique\n",
        "\n",
        "        # Phase 2: Cycles suivants - Mises à jour par matrices de transition\n",
        "        else:\n",
        "            print(\"Phase 2: Mise à jour avec matrices de transition\")\n",
        "\n",
        "            # MÉTHODE PROPOSÉE: Matrices de transition\n",
        "            # Chaque client calcule sa matrice de transition\n",
        "            client_transition_matrices = []\n",
        "\n",
        "            # MÉTHODE TRADITIONNELLE: FedAvg standard\n",
        "            # Entraînement local basé sur le modèle global FedAvg précédent\n",
        "            fedavg_parameters_by_client = []\n",
        "\n",
        "            for client_id, (X_c, Y_c) in enumerate(clients_data):\n",
        "                print(f\"Client {client_id+1}/{n_clients}\")\n",
        "\n",
        "                # POUR LES DEUX MÉTHODES: Entraînement local avec le dernier modèle global\n",
        "                # Pour la méthode proposée, utiliser global_params\n",
        "                # Pour FedAvg, utiliser fedavg_params\n",
        "\n",
        "                # 1. Entraînement pour la méthode proposée\n",
        "                params_proposed, history_proposed, _ = train_lstm(X_c, Y_c, n_a, n_x, n_y,\n",
        "                                                  num_epochs=n_epochs,\n",
        "                                                  seed=client_id+comm_round*100,\n",
        "                                                  initial_params=global_params)\n",
        "\n",
        "                # 2. Entraînement pour FedAvg traditionnel\n",
        "                params_fedavg, _, _ = train_lstm(X_c, Y_c, n_a, n_x, n_y,\n",
        "                                               num_epochs=n_epochs,\n",
        "                                               seed=client_id+comm_round*100,\n",
        "                                               initial_params=fedavg_params)\n",
        "                fedavg_parameters_by_client.append(params_fedavg)\n",
        "\n",
        "                # Calcul de la matrice de transition pour la méthode proposée\n",
        "                flat_history = []\n",
        "                for epoch_params in history_proposed:\n",
        "                    flattened, _, _ = flatten_parameters(epoch_params)\n",
        "                    flat_history.append(flattened)\n",
        "                flat_history = np.array(flat_history)\n",
        "\n",
        "                # Assigner des clusters à chaque état de paramètres\n",
        "                cluster_assignments = global_kmeans_model.predict(flat_history)\n",
        "\n",
        "                # Calculer la matrice de transition\n",
        "                n_transitions = len(cluster_assignments) - 1\n",
        "                transition_matrix = np.zeros((n_clusters, n_clusters))\n",
        "\n",
        "                for t in range(n_transitions):\n",
        "                    from_cluster = cluster_assignments[t]\n",
        "                    to_cluster = cluster_assignments[t+1]\n",
        "                    transition_matrix[from_cluster, to_cluster] += 1\n",
        "\n",
        "                # Normaliser\n",
        "                for i in range(n_clusters):\n",
        "                    row_sum = np.sum(transition_matrix[i])\n",
        "                    if row_sum > 0:\n",
        "                        transition_matrix[i] = transition_matrix[i] / row_sum\n",
        "                    else:\n",
        "                        transition_matrix[i] = np.ones(n_clusters) / n_clusters\n",
        "\n",
        "                client_transition_matrices.append(transition_matrix)\n",
        "\n",
        "            # Mettre à jour le modèle global avec les matrices de transition\n",
        "            global_params = update_with_transition_matrices(\n",
        "                client_transition_matrices, global_kmeans_model,\n",
        "                global_params, param_shapes, param_sizes\n",
        "            )\n",
        "\n",
        "            # Mettre à jour le modèle FedAvg traditionnel\n",
        "            fedavg_params = {}\n",
        "            for key in fedavg_parameters_by_client[0].keys():\n",
        "                fedavg_params[key] = np.mean([params[key] for params in fedavg_parameters_by_client], axis=0)\n",
        "\n",
        "            # Calcul de la bande passante\n",
        "            # Pour FedAvg, tous les clients envoient tous leurs paramètres\n",
        "            traditional_size = calculate_transmission_size(fedavg_params) * n_clients\n",
        "            traditional_bandwidth.append(traditional_size)\n",
        "\n",
        "            # Pour la méthode proposée, seulement les matrices de transition\n",
        "            transition_size = n_clusters * n_clusters * 4 * n_clients  # Taille en octets (4 octets par float)\n",
        "            proposed_bandwidth.append(transition_size)\n",
        "\n",
        "        # Évaluation des deux modèles pour ce cycle\n",
        "        proposed_acc, proposed_loss = evaluate_lstm(X_test, Y_test, global_params)\n",
        "        fedavg_acc, fedavg_loss = evaluate_lstm(X_test, Y_test, fedavg_params)\n",
        "\n",
        "        # Stocker les résultats\n",
        "        cycle_result = {\n",
        "            \"cycle\": comm_round,\n",
        "            \"proposed_accuracy\": proposed_acc,\n",
        "            \"proposed_loss\": proposed_loss,\n",
        "            \"fedavg_accuracy\": fedavg_acc,\n",
        "            \"fedavg_loss\": fedavg_loss,\n",
        "            \"traditional_bandwidth\": traditional_bandwidth[-1],\n",
        "            \"proposed_bandwidth\": proposed_bandwidth[-1]\n",
        "        }\n",
        "        communication_results.append(cycle_result)\n",
        "\n",
        "        print(f\"Résultats du cycle {comm_round+1}:\")\n",
        "        print(f\"  Méthode proposée - Accuracy: {proposed_acc:.4f}, Loss: {proposed_loss:.4f}\")\n",
        "        print(f\"  FedAvg traditionnel - Accuracy: {fedavg_acc:.4f}, Loss: {fedavg_loss:.4f}\")\n",
        "        print(f\"  Bande passante traditionnelle: {traditional_bandwidth[-1]/1024:.2f} KB\")\n",
        "        print(f\"  Bande passante proposée: {proposed_bandwidth[-1]/1024:.2f} KB\")\n",
        "        if comm_round > 0:\n",
        "            saving = (1 - proposed_bandwidth[-1]/traditional_bandwidth[-1]) * 100\n",
        "            print(f\"  Économie de bande passante: {saving:.2f}%\")\n",
        "\n",
        "    # Calcul du temps total d'exécution\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    # Visualisation des résultats\n",
        "    visualize_communication_rounds_comparison(communication_results)\n",
        "\n",
        "    # Résumé des résultats\n",
        "    print(\"\\n=== Résumé de l'expérience en deux phases ===\")\n",
        "    print(f\"Nombre de cycles de communication: {n_communication_rounds}\")\n",
        "    print(f\"Temps total d'exécution: {total_time:.2f} secondes\")\n",
        "\n",
        "    # Calcul des économies de bande passante cumulées\n",
        "    total_traditional = sum(traditional_bandwidth)\n",
        "    total_proposed = sum(proposed_bandwidth)\n",
        "    total_saving = (1 - total_proposed/total_traditional) * 100\n",
        "\n",
        "    print(f\"Bande passante traditionnelle totale: {total_traditional/1024:.2f} KB\")\n",
        "    print(f\"Bande passante proposée totale: {total_proposed/1024:.2f} KB\")\n",
        "    print(f\"Économie de bande passante totale: {total_saving:.2f}%\")\n",
        "\n",
        "    return communication_results, global_params, fedavg_params\n",
        "\n",
        "\n",
        "def cluster_trajectories_with_existing_centers(client_seeds_history, cluster_centers):\n",
        "    \"\"\"\n",
        "    Attribue des clusters aux trajectoires en utilisant des centres préexistants.\n",
        "\n",
        "    Arguments:\n",
        "    client_seeds_history -- liste d'historiques de paramètres pour différentes seeds\n",
        "    cluster_centers -- centres des clusters préétablis\n",
        "\n",
        "    Returns:\n",
        "    flat_labels -- étiquettes de cluster pour chaque seed à chaque époque\n",
        "    \"\"\"\n",
        "    n_seeds = len(client_seeds_history)\n",
        "    n_epochs = len(client_seeds_history[0])\n",
        "    n_clusters = len(cluster_centers)\n",
        "\n",
        "    # Aplatir les paramètres pour toutes les graines et époques\n",
        "    flat_labels = np.zeros((n_seeds, n_epochs), dtype=int)\n",
        "\n",
        "    for seed in range(n_seeds):\n",
        "        for epoch in range(n_epochs):\n",
        "            # Aplatir les paramètres de cette époque\n",
        "            flattened, _, _ = flatten_parameters(client_seeds_history[seed][epoch])\n",
        "\n",
        "            # Calculer les distances aux centres des clusters\n",
        "            distances = np.array([np.linalg.norm(flattened - center) for center in cluster_centers])\n",
        "\n",
        "            # Attribuer au cluster le plus proche\n",
        "            flat_labels[seed, epoch] = np.argmin(distances)\n",
        "\n",
        "    return flat_labels\n",
        "\n",
        "def compute_transition_matrix_from_labels(flat_labels, n_clusters):\n",
        "    \"\"\"\n",
        "    Calcule la matrice de transition à partir des séquences de labels de clusters.\n",
        "\n",
        "    Arguments:\n",
        "    flat_labels -- étiquettes de cluster pour chaque seed à chaque époque\n",
        "    n_clusters -- nombre de clusters\n",
        "\n",
        "    Returns:\n",
        "    transition_matrix -- matrice de transition de Markov\n",
        "    \"\"\"\n",
        "    n_seeds, n_epochs = flat_labels.shape\n",
        "\n",
        "    transition_counts = np.zeros((n_clusters, n_clusters))\n",
        "\n",
        "    # Compter les transitions\n",
        "    for seed in range(n_seeds):\n",
        "        for epoch in range(n_epochs - 1):\n",
        "            from_cluster = flat_labels[seed, epoch]\n",
        "            to_cluster = flat_labels[seed, epoch + 1]\n",
        "            transition_counts[from_cluster, to_cluster] += 1\n",
        "\n",
        "    # Normaliser pour obtenir les probabilités\n",
        "    transition_matrix = np.zeros_like(transition_counts)\n",
        "    for i in range(n_clusters):\n",
        "        row_sum = np.sum(transition_counts[i])\n",
        "        if row_sum > 0:\n",
        "            transition_matrix[i] = transition_counts[i] / row_sum\n",
        "        else:\n",
        "            # Si aucune transition n'est observée depuis ce cluster, distribution uniforme\n",
        "            transition_matrix[i] = 1.0 / n_clusters\n",
        "\n",
        "    return transition_matrix\n",
        "\n",
        "def update_with_transition_matrices_only(client_transition_matrices, client_cluster_centers, param_shapes, param_sizes, n_clients):\n",
        "    \"\"\"\n",
        "    Met à jour le modèle global en utilisant uniquement les matrices de transition.\n",
        "\n",
        "    Arguments:\n",
        "    client_transition_matrices -- liste des matrices de transition de chaque client\n",
        "    client_cluster_centers -- liste des centres de clusters de chaque client (définis en phase 1)\n",
        "    param_shapes -- formes des paramètres\n",
        "    param_sizes -- tailles des paramètres aplatis\n",
        "    n_clients -- nombre de clients\n",
        "\n",
        "    Returns:\n",
        "    updated_global_params -- paramètres globaux mis à jour\n",
        "    \"\"\"\n",
        "    # Initialiser les paramètres globaux\n",
        "    aggregated_params_flat = np.zeros_like(client_cluster_centers[0][0])\n",
        "\n",
        "    # Pour chaque client\n",
        "    for client_id in range(n_clients):\n",
        "        transition_matrix = client_transition_matrices[client_id]\n",
        "        cluster_centers = client_cluster_centers[client_id]\n",
        "        n_clusters = transition_matrix.shape[0]\n",
        "\n",
        "        # Calculer la distribution stationnaire de la matrice de transition\n",
        "        pi = np.ones(n_clusters) / n_clusters  # Distribution initiale uniforme\n",
        "        for _ in range(100):  # Nombre d'itérations arbitraire pour convergence\n",
        "            pi_new = np.dot(pi, transition_matrix)\n",
        "            if np.allclose(pi, pi_new):\n",
        "                break\n",
        "            pi = pi_new\n",
        "\n",
        "        # Pondérer les centres par la distribution stationnaire\n",
        "        client_params_flat = np.zeros_like(cluster_centers[0])\n",
        "        for i in range(n_clusters):\n",
        "            client_params_flat += pi[i] * cluster_centers[i]\n",
        "\n",
        "        # Ajouter à l'agrégation globale\n",
        "        aggregated_params_flat += client_params_flat / n_clients\n",
        "\n",
        "    # Reconstruire les paramètres à leur forme d'origine\n",
        "    updated_global_params = unflatten_parameters(aggregated_params_flat, param_shapes, param_sizes)\n",
        "\n",
        "    return updated_global_params\n",
        "\n",
        "def federated_main_two_phase_local(\n",
        "    n_clients=3, n_epochs=50, n_clusters=3,\n",
        "    n_a=64, n_x=10, n_y=5,\n",
        "    batch_size=32, sequence_length=10,\n",
        "    num_transfer_epochs=5, learning_rate=0.01,\n",
        "    n_communication_rounds=6,  # Nombre total de cycles de communication\n",
        "    use_synthetic_data=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Pipeline complet de Federated Learning avec approche en deux phases et clusterisation locale:\n",
        "    Phase 1: Initialisation avec FedAvg + centres de clusters\n",
        "    Phase 2: Mises à jour avec matrices de transition uniquement\n",
        "    Compare aussi avec la méthode traditionnelle FedAvg à chaque cycle\n",
        "    \"\"\"\n",
        "    print(\"=== Lancement du Federated Learning en deux phases avec clusterisation locale ===\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Génération/chargement des données comme avant\n",
        "    if use_synthetic_data:\n",
        "        print(\"Génération des données synthétiques...\")\n",
        "        clients_data = []\n",
        "        for seed in range(n_clients):\n",
        "            X_train = np.random.randn(n_x, batch_size, sequence_length)\n",
        "            Y_train = np.zeros((n_y, batch_size, sequence_length))\n",
        "            for t in range(sequence_length):\n",
        "                for i in range(batch_size):\n",
        "                    class_idx = np.random.randint(0, n_y)\n",
        "                    Y_train[class_idx, i, t] = 1\n",
        "            clients_data.append((X_train, Y_train))\n",
        "\n",
        "        # Données de test\n",
        "        X_test = np.random.randn(n_x, batch_size//2, sequence_length)\n",
        "        Y_test = np.zeros((n_y, batch_size//2, sequence_length))\n",
        "        for t in range(sequence_length):\n",
        "            for i in range(batch_size//2):\n",
        "                class_idx = np.random.randint(0, n_y)\n",
        "                Y_test[class_idx, i, t] = 1\n",
        "    else:\n",
        "        # Code pour les données réelles\n",
        "        print(\"Chargement des données réelles...\")\n",
        "        clients_data = load_real_data(n_clients, 1, n_x, n_y, sequence_length)\n",
        "        # Extraire seulement la première seed pour chaque client\n",
        "        clients_data = [client_seeds[0] for client_seeds in clients_data]\n",
        "        # Créer des données de test\n",
        "        X_test, Y_test = create_test_dataset(clients_data, batch_size//2, n_x, n_y, sequence_length)\n",
        "\n",
        "    # Résultats pour chaque cycle de communication\n",
        "    communication_results = []\n",
        "    global_params = None\n",
        "    fedavg_params = None  # Pour garder une trace des paramètres FedAvg à chaque cycle\n",
        "    client_cluster_centers = []  # Stockage des centres de clusters pour chaque client\n",
        "    param_shapes = None\n",
        "    param_sizes = None\n",
        "\n",
        "    # Pour le calcul de la bande passante\n",
        "    traditional_bandwidth = []\n",
        "    proposed_bandwidth = []\n",
        "\n",
        "    # Itérer sur les cycles de communication\n",
        "    for comm_round in range(n_communication_rounds):\n",
        "        print(f\"\\n=== Cycle de communication {comm_round+1}/{n_communication_rounds} ===\")\n",
        "\n",
        "        # Phase 1: Premier cycle - FedAvg complet et centres de clusters\n",
        "        if comm_round == 0:\n",
        "            print(\"Phase 1: Initialisation avec FedAvg et création des centres de clusters\")\n",
        "\n",
        "            # Entraînement local sur chaque client\n",
        "            parameters_by_client = []\n",
        "            parameters_history_by_client = []\n",
        "\n",
        "            for client_id, (X_c, Y_c) in enumerate(clients_data):\n",
        "                print(f\"Entraînement du client {client_id+1}/{n_clients}\")\n",
        "\n",
        "                # Plusieurs seeds pour chaque client (pour la clusterisation locale)\n",
        "                client_seeds_history = []\n",
        "                for seed_id in range(5):  # Utiliser 5 seeds différentes par client\n",
        "                    base_seed = client_id * 100 + seed_id\n",
        "                    params, history, _ = train_lstm(X_c, Y_c, n_a, n_x, n_y,\n",
        "                                                  num_epochs=n_epochs, seed=base_seed)\n",
        "                    client_seeds_history.append(history)\n",
        "\n",
        "                # Stocker la dernière itération des paramètres pour FedAvg initial\n",
        "                parameters_by_client.append(client_seeds_history[0][-1])\n",
        "                parameters_history_by_client.append(client_seeds_history)\n",
        "\n",
        "            # FedAvg: Moyenne des paramètres des clients\n",
        "            global_params = {}\n",
        "            for key in parameters_by_client[0].keys():\n",
        "                global_params[key] = np.mean([params[key] for params in parameters_by_client], axis=0)\n",
        "\n",
        "            # Garder également ces paramètres comme référence FedAvg\n",
        "            fedavg_params = copy.deepcopy(global_params)\n",
        "\n",
        "            # Réaliser la clusterisation locale au niveau de chaque client\n",
        "            client_results = []\n",
        "\n",
        "            for client_id in range(n_clients):\n",
        "                # Clusteriser les paramètres de ce client\n",
        "                cluster_centers, transition_matrix = cluster_local_by_client_single(\n",
        "                    parameters_history_by_client[client_id], n_clusters=n_clusters\n",
        "                )\n",
        "\n",
        "                # Conserver les formes et tailles des paramètres (identiques pour tous les clients)\n",
        "                if client_id == 0:\n",
        "                    _, param_shapes, param_sizes = flatten_parameters(parameters_history_by_client[0][0][0])\n",
        "\n",
        "                # Stocker les centres et matrices pour chaque client\n",
        "                client_results.append((cluster_centers, transition_matrix))\n",
        "\n",
        "                # Stocker uniquement les centres pour utilisation future\n",
        "                client_cluster_centers.append(cluster_centers)\n",
        "\n",
        "            # Calcul de la bande passante\n",
        "            # Pour la phase 1, les deux approches envoient les paramètres complets\n",
        "            initial_bandwidth = calculate_transmission_size(global_params) * n_clients\n",
        "\n",
        "            # Pour la méthode proposée, on envoie aussi les clusters\n",
        "            clusters_bandwidth = calculate_transmission_size(parameters=None, client_centers=client_results)\n",
        "            proposed_initial_bandwidth = initial_bandwidth + clusters_bandwidth\n",
        "\n",
        "            traditional_bandwidth.append(initial_bandwidth)\n",
        "            proposed_bandwidth.append(proposed_initial_bandwidth)\n",
        "\n",
        "        # Phase 2: Cycles suivants - Mises à jour par matrices de transition uniquement\n",
        "        else:\n",
        "            print(\"Phase 2: Mise à jour avec matrices de transition uniquement\")\n",
        "\n",
        "            # MÉTHODE PROPOSÉE: Matrices de transition uniquement\n",
        "            client_transition_matrices = []\n",
        "\n",
        "            # MÉTHODE TRADITIONNELLE: FedAvg standard\n",
        "            fedavg_parameters_by_client = []\n",
        "\n",
        "            for client_id, (X_c, Y_c) in enumerate(clients_data):\n",
        "                print(f\"Client {client_id+1}/{n_clients}\")\n",
        "\n",
        "                # 1. Entraînement pour FedAvg traditionnel\n",
        "                params_fedavg, _, _ = train_lstm(X_c, Y_c, n_a, n_x, n_y,\n",
        "                                               num_epochs=n_epochs,\n",
        "                                               seed=client_id+comm_round*100,\n",
        "                                               initial_params=fedavg_params)\n",
        "                fedavg_parameters_by_client.append(params_fedavg)\n",
        "\n",
        "                # 2. Pour la méthode proposée: générer plusieurs séquences d'entraînement\n",
        "                client_seeds_history = []\n",
        "                for seed_id in range(5):  # Utiliser 5 seeds différentes\n",
        "                    base_seed = client_id * 100 + seed_id + comm_round * 1000\n",
        "                    params, history, _ = train_lstm(X_c, Y_c, n_a, n_x, n_y,\n",
        "                                                  num_epochs=n_epochs,\n",
        "                                                  seed=base_seed,\n",
        "                                                  initial_params=global_params)\n",
        "                    client_seeds_history.append(history)\n",
        "\n",
        "                # Transformer les trajectoires d'entraînement en séquences de clusters\n",
        "                # en utilisant les centres de la Phase 1\n",
        "                flat_labels = cluster_trajectories_with_existing_centers(\n",
        "                    client_seeds_history, client_cluster_centers[client_id]\n",
        "                )\n",
        "\n",
        "                # Calculer uniquement la matrice de transition à partir de ces labels\n",
        "                transition_matrix = compute_transition_matrix_from_labels(flat_labels, n_clusters)\n",
        "\n",
        "                # Stocker la matrice de transition pour ce client\n",
        "                client_transition_matrices.append(transition_matrix)\n",
        "\n",
        "            # Mettre à jour le modèle global en utilisant uniquement les matrices de transition\n",
        "            global_params = update_with_transition_matrices_only(\n",
        "                client_transition_matrices, client_cluster_centers, param_shapes, param_sizes, n_clients\n",
        "            )\n",
        "\n",
        "            # Mettre à jour le modèle FedAvg traditionnel\n",
        "            fedavg_params = {}\n",
        "            for key in fedavg_parameters_by_client[0].keys():\n",
        "                fedavg_params[key] = np.mean([params[key] for params in fedavg_parameters_by_client], axis=0)\n",
        "\n",
        "            # Calcul de la bande passante\n",
        "            # Pour FedAvg, tous les clients envoient tous leurs paramètres\n",
        "            traditional_size = calculate_transmission_size(fedavg_params) * n_clients\n",
        "            traditional_bandwidth.append(traditional_size)\n",
        "\n",
        "            # Pour la méthode proposée, seulement les matrices de transition\n",
        "            # Taille d'une matrice de transition: n_clusters x n_clusters x 4 octets\n",
        "            proposed_size = n_clusters * n_clusters * 4 * n_clients\n",
        "            proposed_bandwidth.append(proposed_size)\n",
        "\n",
        "        # Évaluation des deux modèles pour ce cycle\n",
        "        proposed_acc, proposed_loss = evaluate_lstm(X_test, Y_test, global_params)\n",
        "        fedavg_acc, fedavg_loss = evaluate_lstm(X_test, Y_test, fedavg_params)\n",
        "\n",
        "        # Stockage et affichage des résultats comme avant\n",
        "        cycle_result = {\n",
        "            \"cycle\": comm_round,\n",
        "            \"proposed_accuracy\": proposed_acc,\n",
        "            \"proposed_loss\": proposed_loss,\n",
        "            \"fedavg_accuracy\": fedavg_acc,\n",
        "            \"fedavg_loss\": fedavg_loss,\n",
        "            \"traditional_bandwidth\": traditional_bandwidth[-1],\n",
        "            \"proposed_bandwidth\": proposed_bandwidth[-1]\n",
        "        }\n",
        "        communication_results.append(cycle_result)\n",
        "\n",
        "        print(f\"Résultats du cycle {comm_round+1}:\")\n",
        "        print(f\"  Méthode proposée - Accuracy: {proposed_acc:.4f}, Loss: {proposed_loss:.4f}\")\n",
        "        print(f\"  FedAvg traditionnel - Accuracy: {fedavg_acc:.4f}, Loss: {fedavg_loss:.4f}\")\n",
        "        print(f\"  Bande passante traditionnelle: {traditional_bandwidth[-1]/1024:.2f} KB\")\n",
        "        print(f\"  Bande passante proposée: {proposed_bandwidth[-1]/1024:.2f} KB\")\n",
        "        if comm_round > 0:\n",
        "            saving = (1 - proposed_bandwidth[-1]/traditional_bandwidth[-1]) * 100\n",
        "            print(f\"  Économie de bande passante: {saving:.2f}%\")\n",
        "\n",
        "    # Calcul du temps total d'exécution et affichage des résultats comme avant\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    # Visualisation des résultats\n",
        "    visualize_communication_rounds_comparison(communication_results)\n",
        "\n",
        "    # Résumé des résultats\n",
        "    print(\"\\n=== Résumé de l'expérience en deux phases avec clusterisation locale ===\")\n",
        "    print(f\"Nombre de cycles de communication: {n_communication_rounds}\")\n",
        "    print(f\"Temps total d'exécution: {total_time:.2f} secondes\")\n",
        "\n",
        "    # Calcul des économies de bande passante cumulées\n",
        "    total_traditional = sum(traditional_bandwidth)\n",
        "    total_proposed = sum(proposed_bandwidth)\n",
        "    total_saving = (1 - total_proposed/total_traditional) * 100\n",
        "\n",
        "    print(f\"Bande passante traditionnelle totale: {total_traditional/1024:.2f} KB\")\n",
        "    print(f\"Bande passante proposée totale: {total_proposed/1024:.2f} KB\")\n",
        "    print(f\"Économie de bande passante totale: {total_saving:.2f}%\")\n",
        "\n",
        "    return communication_results, global_params, fedavg_params\n",
        "\n",
        "def cluster_local_by_client_single(client_seeds_history, n_clusters):\n",
        "    \"\"\"\n",
        "    Version simplifiée de cluster_local_by_client pour un seul client\n",
        "\n",
        "    Arguments:\n",
        "    client_seeds_history -- liste de listes de dictionnaires Python contenant les paramètres\n",
        "    n_clusters -- nombre de clusters à former\n",
        "\n",
        "    Returns:\n",
        "    cluster_centers -- centres des clusters\n",
        "    transition_matrix -- matrice de transition\n",
        "    \"\"\"\n",
        "    n_seeds = len(client_seeds_history)\n",
        "    n_epochs = len(client_seeds_history[0])\n",
        "\n",
        "    # Obtenir les formes et tailles des paramètres\n",
        "    _, param_shapes, param_sizes = flatten_parameters(client_seeds_history[0][0])\n",
        "\n",
        "    # Aplatir les paramètres pour toutes les graines et époques\n",
        "    flat_params = []\n",
        "    for seed in range(n_seeds):\n",
        "        seed_params = []\n",
        "        for epoch in range(n_epochs):\n",
        "            flattened, _, _ = flatten_parameters(client_seeds_history[seed][epoch])\n",
        "            seed_params.append(flattened)\n",
        "        flat_params.append(seed_params)\n",
        "\n",
        "    flat_params = np.array(flat_params)\n",
        "\n",
        "    # Clusteriser par époque\n",
        "    kmeans_models = []\n",
        "    cluster_labels = np.zeros((n_seeds, n_epochs), dtype=int)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_params = flat_params[:, epoch, :]\n",
        "        kmeans = KMeans(n_clusters=min(n_clusters, n_seeds), random_state=42)\n",
        "        cluster_labels[:, epoch] = kmeans.fit_predict(epoch_params)\n",
        "        kmeans_models.append(kmeans)\n",
        "\n",
        "    # Calculer la matrice de transition\n",
        "    transition_matrix = np.zeros((n_clusters, n_clusters))\n",
        "\n",
        "    # Compter les transitions\n",
        "    for seed in range(n_seeds):\n",
        "        for epoch in range(n_epochs - 1):\n",
        "            from_cluster = cluster_labels[seed, epoch]\n",
        "            to_cluster = cluster_labels[seed, epoch + 1]\n",
        "            transition_matrix[from_cluster, to_cluster] += 1\n",
        "\n",
        "    # Normaliser pour obtenir les probabilités\n",
        "    for i in range(n_clusters):\n",
        "        row_sum = np.sum(transition_matrix[i])\n",
        "        if row_sum > 0:\n",
        "            transition_matrix[i] = transition_matrix[i] / row_sum\n",
        "        else:\n",
        "            # Si aucune transition n'est observée depuis ce cluster, distribution uniforme\n",
        "            transition_matrix[i] = 1.0 / n_clusters\n",
        "\n",
        "    # Utiliser les centres de clusters du dernier modèle KMeans\n",
        "    last_kmeans = kmeans_models[-1]\n",
        "    cluster_centers = last_kmeans.cluster_centers_\n",
        "\n",
        "    return cluster_centers, transition_matrix\n",
        "\n",
        "def analyze_computational_complexity(n_clients, n_a, n_x, n_y, n_epochs, n_clusters, sequence_length, batch_size):\n",
        "    \"\"\"\n",
        "    Analyse la complexité computationnelle du processus d'apprentissage fédéré avec compression markovienne.\n",
        "    \"\"\"\n",
        "    complexity_dict = {}\n",
        "\n",
        "    # 1. Complexité de l'entraînement local sur un client (par époque)\n",
        "    # Forward pass LSTM: O(batch_size * sequence_length * (n_a^2 + n_a*n_x + n_a*n_y))\n",
        "    # Backward pass LSTM: O(batch_size * sequence_length * (n_a^2 + n_a*n_x + n_a*n_y))\n",
        "    forward_complexity = batch_size * sequence_length * (n_a**2 + n_a*n_x + n_a*n_y)\n",
        "    backward_complexity = batch_size * sequence_length * (n_a**2 + n_a*n_x + n_a*n_y)\n",
        "    client_training_complexity = n_epochs * (forward_complexity + backward_complexity)\n",
        "    complexity_dict[\"Local Training (per client)\"] = client_training_complexity\n",
        "\n",
        "    # 2. Complexité de la clusterisation\n",
        "    # K-means: O(n_clients * n_epochs * n_clusters * d * i)\n",
        "    # où d est la dimension des paramètres et i est le nombre d'itérations K-means\n",
        "    d = n_a**2 + n_a*n_x + n_a*n_y + n_a + n_y*n_a + n_y\n",
        "    kmeans_iterations = 100  # Hypothèse pour le nombre d'itérations K-means\n",
        "    clustering_complexity = n_clients * n_epochs * n_clusters * d * kmeans_iterations\n",
        "    complexity_dict[\"Clustering\"] = clustering_complexity\n",
        "\n",
        "    # 3. Complexité du calcul de la matrice de transition\n",
        "    # O(n_clients * n_epochs * n_clusters^2)\n",
        "    transition_matrix_complexity = n_clients * n_epochs * n_clusters**2\n",
        "    complexity_dict[\"Transition Matrix Computation\"] = transition_matrix_complexity\n",
        "\n",
        "    # 4. Complexité de la simulation de trajectoire\n",
        "    # O(n_steps * n_clusters)\n",
        "    trajectory_complexity = n_epochs * n_clusters\n",
        "    complexity_dict[\"Trajectory Simulation\"] = trajectory_complexity\n",
        "\n",
        "    # 5. Complexité de la méthode traditionnelle (moyenne des paramètres)\n",
        "    # O(n_clients * d)\n",
        "    traditional_complexity = n_clients * d\n",
        "    complexity_dict[\"Traditional Aggregation\"] = traditional_complexity\n",
        "\n",
        "    # 6. Complexité du transfer learning\n",
        "    # Similaire à l'entraînement local mais avec moins d'époques\n",
        "    transfer_epochs = 5  # Hypothèse\n",
        "    transfer_complexity = transfer_epochs * (forward_complexity + backward_complexity)\n",
        "    complexity_dict[\"Transfer Learning\"] = transfer_complexity\n",
        "\n",
        "    # Complexité totale de l'approche proposée vs approche traditionnelle\n",
        "    proposed_approach = (n_clients * client_training_complexity +\n",
        "                         clustering_complexity +\n",
        "                         transition_matrix_complexity +\n",
        "                         trajectory_complexity +\n",
        "                         transfer_complexity)\n",
        "\n",
        "    traditional_approach = n_clients * client_training_complexity + traditional_complexity\n",
        "\n",
        "    complexity_dict[\"Total (Proposed Approach)\"] = proposed_approach\n",
        "    complexity_dict[\"Total (Traditional Approach)\"] = traditional_approach\n",
        "    complexity_dict[\"Efficiency Ratio\"] = traditional_approach / proposed_approach\n",
        "\n",
        "    return complexity_dict\n",
        "\n",
        "def visualize_computational_complexity(complexity_dict):\n",
        "    \"\"\"\n",
        "    Visualise la complexité computationnelle.\n",
        "    \"\"\"\n",
        "    # Extraction des composantes principales pour la visualisation\n",
        "    components = [\"Local Training (per client)\", \"Clustering\", \"Transition Matrix Computation\",\n",
        "                 \"Trajectory Simulation\", \"Transfer Learning\"]\n",
        "    values = [complexity_dict[comp] for comp in components]\n",
        "\n",
        "    # Normalisation pour une meilleure visualisation\n",
        "    normalized_values = np.array(values) / np.sum(values) * 100\n",
        "\n",
        "    # Création du graphique\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    bars = plt.bar(components, normalized_values, color=['blue', 'green', 'orange', 'red', 'purple'])\n",
        "\n",
        "    # Ajout des valeurs en pourcentage\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "                 f'{height:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "    plt.title('Décomposition de la complexité computationnelle')\n",
        "    plt.ylabel('Pourcentage du temps de calcul total')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('decomposition_complexite.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Comparaison des approches\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    ratio = complexity_dict[\"Efficiency Ratio\"]\n",
        "    plt.bar(['Approche traditionnelle', 'Approche proposée'],\n",
        "            [100, 100/ratio],\n",
        "            color=['gray', 'green'])\n",
        "    plt.title(f'Comparaison de l\\'efficacité (Traditionnel / Proposé = {ratio:.2f}x)')\n",
        "    plt.ylabel('Complexité relative (%)')\n",
        "    plt.ylim(0, max(100, 100/ratio) * 1.1)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('comparaison_complexite.png')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_results(original_acc, compressed_acc, transfer_acc, full_params_acc, original_loss, compressed_loss, transfer_loss, full_params_loss):\n",
        "    \"\"\"\n",
        "    Visualise les résultats de l'expérience avec comparaison des méthodes.\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Graphique des précisions\n",
        "    labels = ['Original', 'Compressé', 'Transfert', 'Full Params']\n",
        "    accuracies = [original_acc, compressed_acc, transfer_acc, full_params_acc]\n",
        "    colors = ['blue', 'orange', 'green', 'red']\n",
        "    ax1.bar(labels, accuracies, color=colors)\n",
        "    ax1.set_ylabel('Précision')\n",
        "    ax1.set_title('Comparaison des précisions')\n",
        "\n",
        "    # Graphique des pertes\n",
        "    losses = [original_loss, compressed_loss, transfer_loss, full_params_loss]\n",
        "    ax2.bar(labels, losses, color=colors)\n",
        "    ax2.set_ylabel('Perte')\n",
        "    ax2.set_title('Comparaison des pertes')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('resultats_compression_lstm.png')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_transition_matrix(transition_matrix):\n",
        "    \"\"\"\n",
        "    Visualise la matrice de transition.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(transition_matrix, cmap='viridis', interpolation='none')\n",
        "    plt.colorbar(label='Probabilité de transition')\n",
        "    plt.title('Matrice de transition de Markov')\n",
        "    plt.xlabel('Cluster de destination')\n",
        "    plt.ylabel('Cluster de départ')\n",
        "\n",
        "    # Ajouter les valeurs sur la figure\n",
        "    for i in range(transition_matrix.shape[0]):\n",
        "        for j in range(transition_matrix.shape[1]):\n",
        "            plt.text(j, i, f'{transition_matrix[i, j]:.2f}',\n",
        "                     ha='center', va='center',\n",
        "                     color='white' if transition_matrix[i, j] > 0.5 else 'black')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('matrice_transition_markov.png')\n",
        "    plt.show()\n",
        "\n",
        "def compress_parameters(parameters_history_by_seed, n_clusters=3, n_steps=None):\n",
        "    \"\"\"\n",
        "    Compresse les trajectoires de paramètres en utilisant des clusters et des processus de Markov.\n",
        "    \"\"\"\n",
        "    n_seeds = len(parameters_history_by_seed)\n",
        "    n_epochs = len(parameters_history_by_seed[0])\n",
        "\n",
        "    if n_steps is None:\n",
        "        n_steps = n_epochs\n",
        "\n",
        "    # Clusteriser les paramètres\n",
        "    kmeans_models, cluster_labels, flat_params, param_shapes, param_sizes = cluster_parameters_by_epoch(\n",
        "        parameters_history_by_seed, n_clusters)\n",
        "\n",
        "    # Calculer la matrice de transition\n",
        "    transition_matrix = compute_transition_matrix(cluster_labels)\n",
        "\n",
        "    # Simuler de nouvelles trajectoires\n",
        "    compressed_params_flat = []\n",
        "\n",
        "    for seed in range(n_seeds):\n",
        "        # Utiliser le premier cluster observé pour cette graine comme point de départ\n",
        "        initial_cluster = cluster_labels[seed, 0]\n",
        "        trajectory, _ = simulate_parameter_trajectory(initial_cluster, transition_matrix, n_steps, kmeans_models)\n",
        "        compressed_params_flat.append(trajectory[-1])  # Utiliser le dernier état simulé\n",
        "\n",
        "    # Calculer les paramètres moyens compressés (moyennant sur les graines)\n",
        "    avg_compressed_params_flat = np.mean(compressed_params_flat, axis=0)\n",
        "    compressed_params = unflatten_parameters(avg_compressed_params_flat, param_shapes, param_sizes)\n",
        "\n",
        "    # Paramètres originaux moyens pour comparaison\n",
        "    avg_original_params_flat = np.mean(flat_params[:, -1, :], axis=0)  # Moyenne de la dernière époque\n",
        "    original_params = unflatten_parameters(avg_original_params_flat, param_shapes, param_sizes)\n",
        "\n",
        "    return compressed_params, original_params, transition_matrix, kmeans_models, cluster_labels, flat_params, param_shapes, param_sizes\n",
        "\n",
        "def compare_approaches(results_global, results_local):\n",
        "    \"\"\"\n",
        "    Compare et visualise les résultats des deux approches.\n",
        "\n",
        "    Arguments:\n",
        "    results_global -- résultats de l'approche globale\n",
        "    results_local -- résultats de l'approche locale\n",
        "    \"\"\"\n",
        "    # Fonction utilitaire pour récupérer en toute sécurité les valeurs des dictionnaires\n",
        "    def safe_get(results, key, default=0):\n",
        "        value = results.get(key, default)\n",
        "        if not isinstance(value, (int, float)) or not np.isfinite(value):\n",
        "            return default\n",
        "        return value\n",
        "\n",
        "    print(\"\\n\\n\" + \"=\" * 80)\n",
        "    print(\"COMPARAISON DES APPROCHES\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # 1. Comparaison des performances (précision)\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # Précision\n",
        "    acc_labels = ['Original', 'Compressé', 'Après transfert']\n",
        "    acc_global = [results_global['original_acc'], results_global['compressed_acc'], results_global['transfer_acc']]\n",
        "    acc_local = [results_local['original_acc'], results_local['local_compressed_acc'], results_local['local_transfer_acc']]\n",
        "\n",
        "    x = np.arange(len(acc_labels))\n",
        "    width = 0.35\n",
        "\n",
        "    axes[0, 0].bar(x - width/2, acc_global, width, label='Approche globale')\n",
        "    axes[0, 0].bar(x + width/2, acc_local, width, label='Approche locale')\n",
        "    axes[0, 0].set_ylabel('Précision')\n",
        "    axes[0, 0].set_title('Comparaison des précisions')\n",
        "    axes[0, 0].set_xticks(x)\n",
        "    axes[0, 0].set_xticklabels(acc_labels)\n",
        "    axes[0, 0].legend()\n",
        "\n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for i, v in enumerate(acc_global):\n",
        "        axes[0, 0].text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center')\n",
        "    for i, v in enumerate(acc_local):\n",
        "        axes[0, 0].text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center')\n",
        "\n",
        "    # Perte\n",
        "    loss_labels = ['Original', 'Compressé', 'Après transfert']\n",
        "    loss_global = [results_global['original_loss'], results_global['compressed_loss'], results_global['transfer_loss']]\n",
        "    loss_local = [results_local['original_loss'], results_local['local_compressed_loss'], results_local['local_transfer_loss']]\n",
        "\n",
        "    axes[0, 1].bar(x - width/2, loss_global, width, label='Approche globale')\n",
        "    axes[0, 1].bar(x + width/2, loss_local, width, label='Approche locale')\n",
        "    axes[0, 1].set_ylabel('Perte')\n",
        "    axes[0, 1].set_title('Comparaison des pertes')\n",
        "    axes[0, 1].set_xticks(x)\n",
        "    axes[0, 1].set_xticklabels(loss_labels)\n",
        "    axes[0, 1].legend()\n",
        "\n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for i, v in enumerate(loss_global):\n",
        "        axes[0, 1].text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center')\n",
        "    for i, v in enumerate(loss_local):\n",
        "        axes[0, 1].text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center')\n",
        "\n",
        "    # 2. Comparaison des tailles de transmission\n",
        "    size_labels = ['Original', 'Compressé']\n",
        "    size_global = [results_global['original_size']/1024, results_global['compressed_size']/1024] # en KB\n",
        "    size_local = [results_local['original_size']/1024, results_local['local_compressed_size']/1024] # en KB\n",
        "\n",
        "    axes[1, 0].bar(x[:2] - width/2, size_global, width, label='Approche globale')\n",
        "    axes[1, 0].bar(x[:2] + width/2, size_local, width, label='Approche locale')\n",
        "    axes[1, 0].set_ylabel('Taille (KB)')\n",
        "    axes[1, 0].set_title('Comparaison des tailles de transmission')\n",
        "    axes[1, 0].set_xticks(x[:2])\n",
        "    axes[1, 0].set_xticklabels(size_labels)\n",
        "    axes[1, 0].legend()\n",
        "\n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for i, v in enumerate(size_global):\n",
        "        axes[1, 0].text(i - width/2, v + 0.5, f\"{v:.1f}\", ha='center')\n",
        "    for i, v in enumerate(size_local):\n",
        "        axes[1, 0].text(i + width/2, v + 0.5, f\"{v:.1f}\", ha='center')\n",
        "\n",
        "    # 3. Comparaison des temps d'exécution\n",
        "    time_labels = ['Entraînement', 'Compression', 'Transfer']\n",
        "    time_global = [results_global['local_training_time'], results_global['compression_time'], results_global['transfer_time']]\n",
        "    time_local = [results_local['local_training_time'], results_local['clustering_time'] + results_local['aggregation_time'], results_local['transfer_time']]\n",
        "\n",
        "    axes[1, 1].bar(x[:3] - width/2, time_global, width, label='Approche globale')\n",
        "    axes[1, 1].bar(x[:3] + width/2, time_local, width, label='Approche locale')\n",
        "    axes[1, 1].set_ylabel('Temps (secondes)')\n",
        "    axes[1, 1].set_title('Comparaison des temps d\\'exécution')\n",
        "    axes[1, 1].set_xticks(x[:3])\n",
        "    axes[1, 1].set_xticklabels(time_labels)\n",
        "    axes[1, 1].legend()\n",
        "\n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for i, v in enumerate(time_global):\n",
        "        axes[1, 1].text(i - width/2, v + 0.5, f\"{v:.1f}\", ha='center')\n",
        "    for i, v in enumerate(time_local):\n",
        "        axes[1, 1].text(i + width/2, v + 0.5, f\"{v:.1f}\", ha='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('comparaison_approches.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Tableau récapitulatif\n",
        "    print(\"\\nRÉSUMÉ COMPARATIF :\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Métrique':<30} | {'Approche globale':<20} | {'Approche locale':<20}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Vérification des valeurs avant affichage pour éviter les erreurs de formatage\n",
        "    # dues à des valeurs NaN ou inf qui résulteraient de divisions par zéro\n",
        "\n",
        "    # Taux de compression\n",
        "    global_compression = safe_get(results_global, 'compression_ratio', 0)\n",
        "    local_compression = safe_get(results_local, 'compression_ratio', 0)\n",
        "    if not isinstance(global_compression, (int, float)) or not np.isfinite(global_compression):\n",
        "        global_compression = 0\n",
        "    if not isinstance(local_compression, (int, float)) or not np.isfinite(local_compression):\n",
        "        local_compression = 0\n",
        "    print(f\"{'Taux de compression':<30} | {global_compression:<20.2f} | {local_compression:<20.2f}\")\n",
        "\n",
        "    # Économie de bande passante\n",
        "    global_saving = safe_get(results_global, 'bandwidth_saving', 0)\n",
        "    local_saving = safe_get(results_local, 'bandwidth_saving', 0)\n",
        "    if not isinstance(global_saving, (int, float)) or not np.isfinite(global_saving):\n",
        "        global_saving = 0\n",
        "    if not isinstance(local_saving, (int, float)) or not np.isfinite(local_saving):\n",
        "        local_saving = 0\n",
        "    print(f\"{'Économie de bande passante (%)':<30} | {global_saving:<20.2f} | {local_saving:<20.2f}\")\n",
        "\n",
        "    # Perte relative de performance\n",
        "    global_loss = safe_get(results_global, 'performance_loss', 0)\n",
        "    local_loss = safe_get(results_local, 'performance_loss', 0)\n",
        "    if not isinstance(global_loss, (int, float)) or not np.isfinite(global_loss):\n",
        "        global_loss = 0\n",
        "    if not isinstance(local_loss, (int, float)) or not np.isfinite(local_loss):\n",
        "        local_loss = 0\n",
        "    print(f\"{'Perte relative de perf. (%)':<30} | {global_loss:<20.2f} | {local_loss:<20.2f}\")\n",
        "\n",
        "    # Récupération après transfert\n",
        "    global_recovery = safe_get(results_global, 'performance_recovery', 0)\n",
        "    local_recovery = safe_get(results_local, 'performance_recovery', 0)\n",
        "    if not isinstance(global_recovery, (int, float)) or not np.isfinite(global_recovery):\n",
        "        global_recovery = 0\n",
        "    if not isinstance(local_recovery, (int, float)) or not np.isfinite(local_recovery):\n",
        "        local_recovery = 0\n",
        "    print(f\"{'Récupération après transfert (%)':<30} | {global_recovery:<20.2f} | {local_recovery:<20.2f}\")\n",
        "\n",
        "    # Efficacité computationnelle\n",
        "    global_efficiency = safe_get(results_global, 'computational_efficiency', 0)\n",
        "    local_efficiency = safe_get(results_local, 'computational_efficiency', 0)\n",
        "    if not isinstance(global_efficiency, (int, float)) or not np.isfinite(global_efficiency):\n",
        "        global_efficiency = 0\n",
        "    if not isinstance(local_efficiency, (int, float)) or not np.isfinite(local_efficiency):\n",
        "        local_efficiency = 0\n",
        "    print(f\"{'Efficacité computationnelle':<30} | {global_efficiency:<20.2f} | {local_efficiency:<20.2f}\")\n",
        "\n",
        "    # Temps d'exécution\n",
        "    global_time = safe_get(results_global, 'total_time', 0)\n",
        "    local_time = safe_get(results_local, 'total_time', 0)\n",
        "    if not isinstance(global_time, (int, float)) or not np.isfinite(global_time):\n",
        "        global_time = 0\n",
        "    if not isinstance(local_time, (int, float)) or not np.isfinite(local_time):\n",
        "        local_time = 0\n",
        "    print(f\"{'Temps total d exécution (s)':<30} | {global_time:<20.2f} | {local_time:<20.2f}\")\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Analyse des résultats\n",
        "    print(\"\\nANALYSE DES RÉSULTATS :\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Récupérer les valeurs avec une gestion sécurisée\n",
        "    def safe_compare(local_val, global_val, better_if_higher=True):\n",
        "        # Vérifie si les deux valeurs sont des nombres valides\n",
        "        if not isinstance(local_val, (int, float)) or not np.isfinite(local_val) or \\\n",
        "           not isinstance(global_val, (int, float)) or not np.isfinite(global_val):\n",
        "            return None\n",
        "\n",
        "        # Si les deux valeurs sont 0, on ne peut pas vraiment comparer\n",
        "        if local_val == 0 and global_val == 0:\n",
        "            return None\n",
        "\n",
        "        # Comparaison selon le critère spécifié\n",
        "        if better_if_higher:\n",
        "            return local_val > global_val\n",
        "        else:\n",
        "            return local_val < global_val\n",
        "\n",
        "    # Compression (plus élevé = mieux)\n",
        "    comp_result = safe_compare(\n",
        "        safe_get(results_local, 'compression_ratio'),\n",
        "        safe_get(results_global, 'compression_ratio'),\n",
        "        True\n",
        "    )\n",
        "    if comp_result is not None:\n",
        "        if comp_result:\n",
        "            diff = safe_get(results_local, 'compression_ratio') - safe_get(results_global, 'compression_ratio')\n",
        "            print(f\"✓ L'approche locale offre un meilleur taux de compression (+{diff:.2f}x)\")\n",
        "        else:\n",
        "            diff = safe_get(results_global, 'compression_ratio') - safe_get(results_local, 'compression_ratio')\n",
        "            print(f\"✗ L'approche locale offre un moins bon taux de compression (-{diff:.2f}x)\")\n",
        "    else:\n",
        "        print(\"⚠ Impossible de comparer les taux de compression (valeurs non valides ou nulles)\")\n",
        "\n",
        "    # Performance\n",
        "    perf_result = safe_compare(\n",
        "        safe_get(results_local, 'performance_loss'),\n",
        "        safe_get(results_global, 'performance_loss'),\n",
        "        False\n",
        "    )\n",
        "    if perf_result is not None:\n",
        "        if perf_result:\n",
        "            diff = safe_get(results_global, 'performance_loss') - safe_get(results_local, 'performance_loss')\n",
        "            print(f\"✓ L'approche locale préserve mieux les performances (-{diff:.2f}% de perte)\")\n",
        "        else:\n",
        "            diff = safe_get(results_local, 'performance_loss') - safe_get(results_global, 'performance_loss')\n",
        "            print(f\"✗ L'approche locale préserve moins bien les performances (+{diff:.2f}% de perte)\")\n",
        "    else:\n",
        "        print(\"⚠ Impossible de comparer les pertes de performance (valeurs égales ou non valides)\")\n",
        "\n",
        "    # Récupération\n",
        "    recovery_result = safe_compare(\n",
        "        safe_get(results_local, 'performance_recovery'),\n",
        "        safe_get(results_global, 'performance_recovery'),\n",
        "        True\n",
        "    )\n",
        "    if recovery_result is not None:\n",
        "        if recovery_result:\n",
        "            diff = safe_get(results_local, 'performance_recovery') - safe_get(results_global, 'performance_recovery')\n",
        "            print(f\"✓ L'approche locale montre une meilleure récupération après transfert (+{diff:.2f}%)\")\n",
        "        else:\n",
        "            diff = safe_get(results_global, 'performance_recovery') - safe_get(results_local, 'performance_recovery')\n",
        "            print(f\"✗ L'approche locale montre une moins bonne récupération après transfert (-{diff:.2f}%)\")\n",
        "    else:\n",
        "        print(\"⚠ Impossible de comparer la récupération après transfert (valeurs égales ou non valides)\")\n",
        "\n",
        "    # Efficacité\n",
        "    efficiency_result = safe_compare(\n",
        "        safe_get(results_local, 'computational_efficiency'),\n",
        "        safe_get(results_global, 'computational_efficiency'),\n",
        "        True\n",
        "    )\n",
        "    if efficiency_result is not None:\n",
        "        if efficiency_result:\n",
        "            diff = safe_get(results_local, 'computational_efficiency') - safe_get(results_global, 'computational_efficiency')\n",
        "            print(f\"✓ L'approche locale est plus efficace computationnellement (+{diff:.2f}x)\")\n",
        "        else:\n",
        "            diff = safe_get(results_global, 'computational_efficiency') - safe_get(results_local, 'computational_efficiency')\n",
        "            print(f\"✗ L'approche locale est moins efficace computationnellement (-{diff:.2f}x)\")\n",
        "    else:\n",
        "        print(\"⚠ Impossible de comparer l'efficacité computationnelle (valeurs non valides ou nulles)\")\n",
        "\n",
        "    # Temps d'exécution\n",
        "    time_result = safe_compare(\n",
        "        safe_get(results_local, 'total_time'),\n",
        "        safe_get(results_global, 'total_time'),\n",
        "        False\n",
        "    )\n",
        "    if time_result is not None:\n",
        "        if time_result:\n",
        "            diff = safe_get(results_global, 'total_time') - safe_get(results_local, 'total_time')\n",
        "            print(f\"✓ L'approche locale est plus rapide (-{diff:.2f} secondes)\")\n",
        "        else:\n",
        "            diff = safe_get(results_local, 'total_time') - safe_get(results_global, 'total_time')\n",
        "            print(f\"✗ L'approche locale est plus lente (+{diff:.2f} secondes)\")\n",
        "    else:\n",
        "        print(\"⚠ Impossible de comparer les temps d'exécution (valeurs non valides ou nulles)\")\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Conclusion\n",
        "    advantages_local = 0\n",
        "    advantages_analyzed = 0\n",
        "\n",
        "    # Compression (plus élevé = mieux)\n",
        "    comp_result = safe_compare(\n",
        "        safe_get(results_local, 'compression_ratio'),\n",
        "        safe_get(results_global, 'compression_ratio'),\n",
        "        True\n",
        "    )\n",
        "    if comp_result is not None:\n",
        "        advantages_analyzed += 1\n",
        "        if comp_result:\n",
        "            advantages_local += 1\n",
        "\n",
        "    # Performance loss (plus bas = mieux)\n",
        "    perf_result = safe_compare(\n",
        "        safe_get(results_local, 'performance_loss'),\n",
        "        safe_get(results_global, 'performance_loss'),\n",
        "        False\n",
        "    )\n",
        "    if perf_result is not None:\n",
        "        advantages_analyzed += 1\n",
        "        if perf_result:\n",
        "            advantages_local += 1\n",
        "\n",
        "    # Recovery (plus élevé = mieux)\n",
        "    recovery_result = safe_compare(\n",
        "        safe_get(results_local, 'performance_recovery'),\n",
        "        safe_get(results_global, 'performance_recovery'),\n",
        "        True\n",
        "    )\n",
        "    if recovery_result is not None:\n",
        "        advantages_analyzed += 1\n",
        "        if recovery_result:\n",
        "            advantages_local += 1\n",
        "\n",
        "    # Efficiency (plus élevé = mieux)\n",
        "    efficiency_result = safe_compare(\n",
        "        safe_get(results_local, 'computational_efficiency'),\n",
        "        safe_get(results_global, 'computational_efficiency'),\n",
        "        True\n",
        "    )\n",
        "    if efficiency_result is not None:\n",
        "        advantages_analyzed += 1\n",
        "        if efficiency_result:\n",
        "            advantages_local += 1\n",
        "\n",
        "    # Time (plus bas = mieux)\n",
        "    time_result = safe_compare(\n",
        "        safe_get(results_local, 'total_time'),\n",
        "        safe_get(results_global, 'total_time'),\n",
        "        False\n",
        "    )\n",
        "    if time_result is not None:\n",
        "        advantages_analyzed += 1\n",
        "        if time_result:\n",
        "            advantages_local += 1\n",
        "\n",
        "    print(\"\\nCONCLUSION :\")\n",
        "    if advantages_analyzed == 0:\n",
        "        print(\"⚠ Impossible de conclure (données insuffisantes pour la comparaison)\")\n",
        "    elif advantages_local > advantages_analyzed / 2:\n",
        "        print(f\"✅ L'approche de clusterisation locale est globalement meilleure ({advantages_local}/{advantages_analyzed} avantages)\")\n",
        "    else:\n",
        "        print(f\"❌ L'approche de clusterisation globale reste préférable ({advantages_analyzed-advantages_local}/{advantages_analyzed} avantages)\")\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "    # Exécuter la comparaison des approches\n",
        "   # results_global, results_local = main()\n",
        "  #  print(\"\\n=== Comparaison des approches terminée avec succès ===\")\n",
        "\n",
        "def main_two_phase():\n",
        "    \"\"\"\n",
        "    Fonction principale pour l'approche en deux phases.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"EXPÉRIENCE FEDERATED LEARNING AVEC APPROCHE EN DEUX PHASES\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Paramètres\n",
        "    params = {\n",
        "        \"n_clients\": 3,            # Nombre de clients\n",
        "        \"n_epochs\": 50,             # Nombre d'époques pour l'entraînement local\n",
        "        \"n_clusters\": 3,           # Nombre de clusters pour la compression\n",
        "        \"n_a\": 32,                 # Dimension cachée\n",
        "        \"n_x\": 8,                  # Dimension d'entrée\n",
        "        \"n_y\": 5,                  # Nombre de classes\n",
        "        \"batch_size\": 32,          # Taille du batch\n",
        "        \"sequence_length\": 10,     # Longueur de séquence\n",
        "        \"num_transfer_epochs\": 3,  # Époques pour le transfer learning\n",
        "        \"learning_rate\": 0.01,     # Taux d'apprentissage\n",
        "        \"n_communication_rounds\": 3, # Nombre de cycles de communication\n",
        "        \"use_synthetic_data\": True  # Utiliser des données synthétiques\n",
        "    }\n",
        "\n",
        "\n",
        "     # Exécuter l'expérience\n",
        "    results, final_model_proposed, final_model_fedavg = federated_main_two_phase_local(**params)\n",
        "\n",
        "\n",
        "    # Afficher un tableau récapitulatif des performances\n",
        "    print(\"\\nRÉCAPITULATIF DES PERFORMANCES :\")\n",
        "    print(\"-\" * 100)\n",
        "    print(\"| Cycle | Proposée Acc | FedAvg Acc | Diff Acc (%) | Proposée Loss | FedAvg Loss | Diff Loss (%) |\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    for result in results:\n",
        "        cycle = result[\"cycle\"] + 1\n",
        "        prop_acc = result[\"proposed_accuracy\"]\n",
        "        fedavg_acc = result[\"fedavg_accuracy\"]\n",
        "        prop_loss = result[\"proposed_loss\"]\n",
        "        fedavg_loss = result[\"fedavg_loss\"]\n",
        "\n",
        "        # Calcul des différences en pourcentage\n",
        "        if fedavg_acc > 0:\n",
        "            acc_diff = ((prop_acc - fedavg_acc) / fedavg_acc) * 100\n",
        "        else:\n",
        "            acc_diff = float('inf')\n",
        "\n",
        "        if fedavg_loss > 0:\n",
        "            loss_diff = ((prop_loss - fedavg_loss) / fedavg_loss) * 100\n",
        "        else:\n",
        "            loss_diff = float('inf')\n",
        "\n",
        "        # Signe pour indiquer si c'est mieux (+) ou moins bien (-)\n",
        "        acc_sign = \"+\" if acc_diff > 0 else \"\"\n",
        "        loss_sign = \"+\" if loss_diff > 0 else \"\"\n",
        "\n",
        "        print(f\"| {cycle:5d} | {prop_acc:11.4f} | {fedavg_acc:9.4f} | {acc_sign}{acc_diff:10.2f} | {prop_loss:12.4f} | {fedavg_loss:10.4f} | {loss_sign}{loss_diff:11.2f} |\")\n",
        "\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    # Afficher un récapitulatif des économies de bande passante\n",
        "    print(\"\\nRÉCAPITULATIF DES ÉCONOMIES DE BANDE PASSANTE :\")\n",
        "    print(\"-\" * 80)\n",
        "    print(\"| Cycle | Traditionnelle (KB) | Proposée (KB) | Économie (%) |\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    total_trad = 0\n",
        "    total_prop = 0\n",
        "\n",
        "    for result in results:\n",
        "        cycle = result[\"cycle\"] + 1\n",
        "        trad_kb = result[\"traditional_bandwidth\"] / 1024\n",
        "        prop_kb = result[\"proposed_bandwidth\"] / 1024\n",
        "\n",
        "        if cycle > 1:  # Phase 2\n",
        "            saving = (1 - prop_kb/trad_kb) * 100\n",
        "            print(f\"| {cycle:5d} | {trad_kb:18.2f} | {prop_kb:13.2f} | {saving:11.2f} |\")\n",
        "        else:  # Phase 1\n",
        "            print(f\"| {cycle:5d} | {trad_kb:18.2f} | {prop_kb:13.2f} | {'N/A':11s} |\")\n",
        "\n",
        "        total_trad += result[\"traditional_bandwidth\"]\n",
        "        total_prop += result[\"proposed_bandwidth\"]\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Économie totale\n",
        "    total_trad_kb = total_trad / 1024\n",
        "    total_prop_kb = total_prop / 1024\n",
        "    total_saving = (1 - total_prop/total_trad) * 100\n",
        "\n",
        "    print(f\"| Total | {total_trad_kb:18.2f} | {total_prop_kb:13.2f} | {total_saving:11.2f} |\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Conclusion\n",
        "    print(\"\\nCONCLUSION:\")\n",
        "    avg_acc_diff = sum([(r[\"proposed_accuracy\"] - r[\"fedavg_accuracy\"]) / r[\"fedavg_accuracy\"] * 100 if r[\"fedavg_accuracy\"] > 0 else 0 for r in results]) / len(results)\n",
        "    avg_loss_diff = sum([(r[\"proposed_loss\"] - r[\"fedavg_loss\"]) / r[\"fedavg_loss\"] * 100 if r[\"fedavg_loss\"] > 0 else 0 for r in results]) / len(results)\n",
        "\n",
        "    print(f\"Différence moyenne de précision: {avg_acc_diff:.2f}%\")\n",
        "    print(f\"Différence moyenne de perte: {avg_loss_diff:.2f}%\")\n",
        "    print(f\"Économie moyenne de bande passante (cycles 2-3): {total_saving:.2f}%\")\n",
        "\n",
        "    if avg_acc_diff > -1 and total_saving > 50:  # Seuils arbitraires pour la conclusion\n",
        "        print(\"VERDICT: La méthode proposée permet d'économiser significativement de la bande passante tout en maintenant des performances comparables à FedAvg.\")\n",
        "    elif avg_acc_diff < -5:\n",
        "        print(\"VERDICT: La méthode proposée économise de la bande passante mais au détriment d'une baisse notable des performances.\")\n",
        "    else:\n",
        "        print(\"VERDICT: Compromis modéré entre économie de bande passante et performances.\")\n",
        "\n",
        "    return results, final_model_proposed, final_model_fedavg\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_two_phase()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
