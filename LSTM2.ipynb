{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fonctions LSTM ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    xt -- Données d'entrée à time-step t, array de forme (n_x, m)\n",
        "    a_prev -- Etat caché précédent, array de forme (n_a, m)\n",
        "    c_prev -- Etat de la cellule précédente, array de forme (n_a, m)\n",
        "    parameters -- Dictionnaire Python contenant:\n",
        "                Wf -- Poids de la forget gate, array de forme (n_a, n_a + n_x)\n",
        "                bf -- Biais de la forget gate, array de forme (n_a, 1)\n",
        "                Wi -- Poids de l'update gate, array de forme (n_a, n_a + n_x)\n",
        "                bi -- Biais de l'update gate, array de forme (n_a, 1)\n",
        "                Wc -- Poids de la première \"tanh\", array de forme (n_a, n_a + n_x)\n",
        "                bc -- Biais de la première \"tanh\", array de forme (n_a, 1)\n",
        "                Wo -- Poids de l'output gate, array de forme (n_a, n_a + n_x)\n",
        "                bo -- Biais de l'output gate, array de forme (n_a, 1)\n",
        "                Wy -- Poids pour l'état caché, array de forme (n_y, n_a)\n",
        "                by -- Biais pour l'état caché, array de forme (n_y, 1)\n",
        "\n",
        "    Returns:\n",
        "    a_next -- Prochain état caché, array de forme (n_a, m)\n",
        "    c_next -- Prochain état de cellule, array de forme (n_a, m)\n",
        "    yt_pred -- Prédiction à time-step t, array de forme (n_y, m)\n",
        "    cache -- Tuple de valeurs pour la backpropagation\n",
        "    \"\"\"\n",
        "\n",
        "    # Récupérer les paramètres du dictionnaire\n",
        "    Wf = parameters[\"Wf\"]\n",
        "    bf = parameters[\"bf\"]\n",
        "    Wi = parameters[\"Wi\"]\n",
        "    bi = parameters[\"bi\"]\n",
        "    Wc = parameters[\"Wc\"]\n",
        "    bc = parameters[\"bc\"]\n",
        "    Wo = parameters[\"Wo\"]\n",
        "    bo = parameters[\"bo\"]\n",
        "    Wy = parameters[\"Wy\"]\n",
        "    by = parameters[\"by\"]\n",
        "\n",
        "    # Récupérer les dimensions\n",
        "    n_x, m = xt.shape\n",
        "    n_y, n_a = Wy.shape\n",
        "\n",
        "    # Concaténer a_prev et xt\n",
        "    concat = np.zeros((n_a + n_x, m))\n",
        "    concat[: n_a, :] = a_prev\n",
        "    concat[n_a:, :] = xt\n",
        "\n",
        "    # Calculer les valeurs pour ft, it, cct, c_next, ot, a_next\n",
        "    ft = sigmoid(np.matmul(Wf, concat) + bf)\n",
        "    it = sigmoid(np.matmul(Wi, concat) + bi)\n",
        "    cct = np.tanh(np.matmul(Wc, concat) + bc)\n",
        "    c_next = (ft * c_prev) + (it * cct)\n",
        "    ot = sigmoid(np.matmul(Wo, concat) + bo)\n",
        "    a_next = ot * np.tanh(c_next)\n",
        "\n",
        "    # Calculer la prédiction\n",
        "    yt_pred = softmax(np.matmul(Wy, a_next) + by)\n",
        "\n",
        "    # Stocker les valeurs pour la backpropagation\n",
        "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
        "\n",
        "    return a_next, c_next, yt_pred, cache\n",
        "\n",
        "def lstm_cell_backward(da_next, dc_next, cache):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    da_next -- Gradient du prochain état caché, array de forme (n_a, m)\n",
        "    dc_next -- Gradient du prochain état de cellule, array de forme (n_a, m)\n",
        "    cache -- Cache du forward pass\n",
        "\n",
        "    Returns:\n",
        "    gradients -- Dictionnaire contenant les gradients\n",
        "    \"\"\"\n",
        "\n",
        "    # Récupérer les informations du cache\n",
        "    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache\n",
        "\n",
        "    # Récupérer les dimensions\n",
        "    n_x, m = xt.shape\n",
        "    n_a, m = a_next.shape\n",
        "\n",
        "    # Calculer les dérivées des portes\n",
        "    dot = da_next * np.tanh(c_next) * ot * (1 - ot)\n",
        "    dcct = (dc_next * it + ot * (1 - np.square(np.tanh(c_next))) * it * da_next) * (1 - np.square(cct))\n",
        "    dit = (dc_next * cct + ot * (1 - np.square(np.tanh(c_next))) * cct * da_next) * it * (1 - it)\n",
        "    dft = (dc_next * c_prev + ot * (1 - np.square(np.tanh(c_next))) * c_prev * da_next) * ft * (1 - ft)\n",
        "\n",
        "    concat = np.concatenate((a_prev, xt), axis=0)\n",
        "\n",
        "    # Calculer les dérivées des paramètres\n",
        "    dWf = np.dot(dft, concat.T)\n",
        "    dWi = np.dot(dit, concat.T)\n",
        "    dWc = np.dot(dcct, concat.T)\n",
        "    dWo = np.dot(dot, concat.T)\n",
        "    dbf = np.sum(dft, axis=1, keepdims=True)\n",
        "    dbi = np.sum(dit, axis=1, keepdims=True)\n",
        "    dbc = np.sum(dcct, axis=1, keepdims=True)\n",
        "    dbo = np.sum(dot, axis=1, keepdims=True)\n",
        "\n",
        "    # Calculer les dérivées par rapport à l'état caché précédent, l'état de cellule précédent et l'entrée\n",
        "    da_prev = np.dot(parameters['Wf'][:, :n_a].T, dft) + np.dot(parameters['Wi'][:, :n_a].T, dit) + np.dot(\n",
        "        parameters['Wc'][:, :n_a].T, dcct) + np.dot(parameters['Wo'][:, :n_a].T, dot)\n",
        "    dc_prev = dc_next * ft + ot * (1 - np.square(np.tanh(c_next))) * ft * da_next\n",
        "    dxt = np.dot(parameters['Wf'][:, n_a:].T, dft) + np.dot(parameters['Wi'][:, n_a:].T, dit) + np.dot(\n",
        "        parameters['Wc'][:, n_a:].T, dcct) + np.dot(parameters['Wo'][:, n_a:].T, dot)\n",
        "\n",
        "    # Sauvegarder les gradients\n",
        "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev, \"dWf\": dWf, \"dbf\": dbf, \"dWi\": dWi, \"dbi\": dbi,\n",
        "                 \"dWc\": dWc, \"dbc\": dbc, \"dWo\": dWo, \"dbo\": dbo}\n",
        "\n",
        "    return gradients\n",
        "\n",
        "def lstm_forward(x, a0, parameters):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    x -- Données d'entrée pour chaque time-step, array de forme (n_x, m, T_x)\n",
        "    a0 -- État caché initial, array de forme (n_a, m)\n",
        "    parameters -- Dictionnaire Python des paramètres LSTM\n",
        "\n",
        "    Returns:\n",
        "    a -- États cachés pour chaque time-step, array de forme (n_a, m, T_x)\n",
        "    y -- Prédictions pour chaque time-step, array de forme (n_y, m, T_x)\n",
        "    c -- États de cellule pour chaque time-step, array de forme (n_a, m, T_x)\n",
        "    caches -- Tuple de valeurs pour la backpropagation\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialiser les caches\n",
        "    caches = []\n",
        "\n",
        "    # Récupérer les dimensions\n",
        "    n_x, m, T_x = x.shape\n",
        "    n_y, n_a = parameters[\"Wy\"].shape\n",
        "\n",
        "    # Initialiser a, c et y avec des zéros\n",
        "    a = np.zeros((n_a, m, T_x))\n",
        "    c = a.copy()\n",
        "    y = np.zeros((n_y, m, T_x))\n",
        "\n",
        "    # Initialiser a_next et c_next\n",
        "    a_next = a0\n",
        "    c_next = np.zeros(a_next.shape)\n",
        "\n",
        "    # Boucle sur tous les time-steps\n",
        "    for t in range(T_x):\n",
        "        # Mettre à jour a_next, c_next, calculer la prédiction et obtenir le cache\n",
        "        a_next, c_next, yt, cache = lstm_cell_forward(x[:, :, t], a_next, c_next, parameters)\n",
        "        # Sauvegarder les valeurs\n",
        "        a[:, :, t] = a_next\n",
        "        y[:, :, t] = yt\n",
        "        c[:, :, t] = c_next\n",
        "        # Ajouter le cache\n",
        "        caches.append(cache)\n",
        "\n",
        "    # Stocker les valeurs pour la backpropagation\n",
        "    caches = (caches, x)\n",
        "\n",
        "    return a, y, c, caches\n",
        "\n",
        "def lstm_backward(da, caches):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    da -- Gradient par rapport aux états cachés, array de forme (n_a, m, T_x)\n",
        "    caches -- Cache du forward pass\n",
        "\n",
        "    Returns:\n",
        "    gradients -- Dictionnaire Python contenant les gradients\n",
        "    \"\"\"\n",
        "\n",
        "    # Récupérer les valeurs du cache\n",
        "    (caches, x) = caches\n",
        "    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]\n",
        "\n",
        "    # Récupérer les dimensions\n",
        "    n_a, m, T_x = da.shape\n",
        "    n_x, m = x1.shape\n",
        "\n",
        "    # Initialiser les gradients\n",
        "    dx = np.zeros((n_x, m, T_x))\n",
        "    da0 = np.zeros((n_a, m))\n",
        "    da_prevt = np.zeros(da0.shape)\n",
        "    dc_prevt = np.zeros(da0.shape)\n",
        "    dWf = np.zeros((n_a, n_a + n_x))\n",
        "    dWi = np.zeros(dWf.shape)\n",
        "    dWc = np.zeros(dWf.shape)\n",
        "    dWo = np.zeros(dWf.shape)\n",
        "    dbf = np.zeros((n_a, 1))\n",
        "    dbi = np.zeros(dbf.shape)\n",
        "    dbc = np.zeros(dbf.shape)\n",
        "    dbo = np.zeros(dbf.shape)\n",
        "\n",
        "    # Boucle sur la séquence en sens inverse\n",
        "    for t in reversed(range(T_x)):\n",
        "        # Calculer tous les gradients à l'aide de lstm_cell_backward\n",
        "        gradients = lstm_cell_backward(da[:, :, t] + da_prevt, dc_prevt, caches[t])\n",
        "        # Stocker ou ajouter les gradients aux gradients de l'étape précédente\n",
        "        dx[:, :, t] = gradients[\"dxt\"]\n",
        "        dWf += gradients[\"dWf\"]\n",
        "        dWi += gradients[\"dWi\"]\n",
        "        dWc += gradients[\"dWc\"]\n",
        "        dWo += gradients[\"dWo\"]\n",
        "        dbf += gradients[\"dbf\"]\n",
        "        dbi += gradients[\"dbi\"]\n",
        "        dbc += gradients[\"dbc\"]\n",
        "        dbo += gradients[\"dbo\"]\n",
        "        da_prevt = gradients[\"da_prev\"]\n",
        "        dc_prevt = gradients[\"dc_prev\"]\n",
        "\n",
        "    # Définir le premier gradient d'activation\n",
        "    da0 = gradients[\"da_prev\"]\n",
        "\n",
        "    # Stocker les gradients dans un dictionnaire Python\n",
        "    gradients = {\"dx\": dx, \"da0\": da0, \"dWf\": dWf, \"dbf\": dbf, \"dWi\": dWi, \"dbi\": dbi,\n",
        "                 \"dWc\": dWc, \"dbc\": dbc, \"dWo\": dWo, \"dbo\": dbo}\n",
        "\n",
        "    return gradients\n",
        "\n",
        "def initialize_adam_for_lstm(parameters):\n",
        "    \"\"\"\n",
        "    Initialise v et s pour les paramètres du LSTM.\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- Dictionnaire Python contenant les paramètres du LSTM.\n",
        "\n",
        "    Returns:\n",
        "    v -- Dictionnaire Python qui contiendra la moyenne mobile exponentielle du gradient.\n",
        "    s -- Dictionnaire Python qui contiendra la moyenne mobile exponentielle du carré du gradient.\n",
        "    \"\"\"\n",
        "    v = {}\n",
        "    s = {}\n",
        "\n",
        "    # Initialiser v, s pour tous les paramètres du LSTM\n",
        "    for key in parameters.keys():\n",
        "        v[\"d\" + key] = np.zeros_like(parameters[key])\n",
        "        s[\"d\" + key] = np.zeros_like(parameters[key])\n",
        "\n",
        "    return v, s\n",
        "\n",
        "def update_parameters_with_adam_for_lstm(parameters, grads, v, s, t, learning_rate=0.01,\n",
        "                                         beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "\n",
        "    v_corrected = {}  # Estimation du premier moment corrigée du biais\n",
        "    s_corrected = {}  # Estimation du second moment corrigée du biais\n",
        "\n",
        "    # Effectuer la mise à jour Adam sur tous les paramètres\n",
        "    for key in parameters.keys():\n",
        "        # Clé correspondante dans les dictionnaires grads, v, s\n",
        "        d_key = \"d\" + key\n",
        "\n",
        "        # S'assurer que nous avons le gradient correspondant\n",
        "        if d_key not in grads:\n",
        "            continue\n",
        "\n",
        "        # Moyenne mobile des gradients\n",
        "        v[d_key] = beta1 * v[d_key] + (1 - beta1) * grads[d_key]\n",
        "\n",
        "        # Calcul de l'estimation du premier moment corrigée du biais\n",
        "        v_corrected[d_key] = v[d_key] / (1 - beta1**t)\n",
        "\n",
        "        # Moyenne mobile des carrés des gradients\n",
        "        s[d_key] = beta2 * s[d_key] + (1 - beta2) * (grads[d_key]**2)\n",
        "\n",
        "        # Calcul de l'estimation du second moment corrigée du biais\n",
        "        s_corrected[d_key] = s[d_key] / (1 - beta2**t)\n",
        "\n",
        "        # Mise à jour des paramètres\n",
        "        parameters[key] = parameters[key] - learning_rate * v_corrected[d_key] / (np.sqrt(s_corrected[d_key]) + epsilon)\n",
        "\n",
        "    return parameters, v, s\n",
        "\n",
        "def initialize_lstm_parameters(n_a, n_x, n_y):\n",
        "    \"\"\"\n",
        "    Initialise les paramètres du LSTM.\n",
        "\n",
        "    Arguments:\n",
        "    n_a -- nombre d'unités dans la couche cachée\n",
        "    n_x -- taille d'entrée\n",
        "    n_y -- taille de sortie\n",
        "\n",
        "    Returns:\n",
        "    parameters -- dictionnaire Python contenant les paramètres initialisés\n",
        "    \"\"\"\n",
        "    np.random.seed(1)\n",
        "\n",
        "    # Initialisation avec He/Xavier\n",
        "    Wf = np.random.randn(n_a, n_a + n_x) * np.sqrt(1. / (n_a + n_x))\n",
        "    bf = np.zeros((n_a, 1))\n",
        "    Wi = np.random.randn(n_a, n_a + n_x) * np.sqrt(1. / (n_a + n_x))\n",
        "    bi = np.zeros((n_a, 1))\n",
        "    Wc = np.random.randn(n_a, n_a + n_x) * np.sqrt(1. / (n_a + n_x))\n",
        "    bc = np.zeros((n_a, 1))\n",
        "    Wo = np.random.randn(n_a, n_a + n_x) * np.sqrt(1. / (n_a + n_x))\n",
        "    bo = np.zeros((n_a, 1))\n",
        "    Wy = np.random.randn(n_y, n_a) * np.sqrt(1. / n_a)\n",
        "    by = np.zeros((n_y, 1))\n",
        "\n",
        "    parameters = {\"Wf\": Wf, \"bf\": bf, \"Wi\": Wi, \"bi\": bi, \"Wc\": Wc, \"bc\": bc, \"Wo\": Wo, \"bo\": bo, \"Wy\": Wy, \"by\": by}\n",
        "\n",
        "    return parameters\n",
        "\n",
        "def train_lstm(X_train, Y_train, n_a, n_x, n_y, num_epochs=10, seed=1, learning_rate=0.01, initial_params=None):\n",
        "    \"\"\"\n",
        "    Entraîne un LSTM sur les données fournies, avec possibilité d'initialiser avec des paramètres existants.\n",
        "\n",
        "    Arguments:\n",
        "    X_train -- données d'entrée, numpy array de forme (n_x, m, T_x)\n",
        "    Y_train -- étiquettes, numpy array de forme (n_y, m, T_x)\n",
        "    n_a -- nombre d'unités dans la couche cachée\n",
        "    n_x -- taille d'entrée\n",
        "    n_y -- taille de sortie\n",
        "    num_epochs -- nombre d'époques d'entraînement\n",
        "    seed -- graine pour la reproductibilité\n",
        "    learning_rate -- taux d'apprentissage\n",
        "    initial_params -- paramètres initiaux (optionnel)\n",
        "\n",
        "    Returns:\n",
        "    parameters -- paramètres finaux\n",
        "    parameters_history -- historique des paramètres à chaque époque\n",
        "    loss_history -- historique des pertes\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Initialisation des paramètres\n",
        "    if initial_params is None:\n",
        "        parameters = initialize_lstm_parameters(n_a, n_x, n_y)\n",
        "    else:\n",
        "        parameters = copy.deepcopy(initial_params)\n",
        "\n",
        "    parameters_history = []\n",
        "    loss_history = []\n",
        "\n",
        "    # Initialiser Adam\n",
        "    v, s = initialize_adam_for_lstm(parameters)\n",
        "    t = 0  # Compteur pour Adam\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Époque {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Forward pass\n",
        "        a0 = np.zeros((n_a, X_train.shape[1]))\n",
        "        a, y_pred, c, caches = lstm_forward(X_train, a0, parameters)\n",
        "\n",
        "        # Calcul de la perte (cross-entropy)\n",
        "        loss = -np.sum(Y_train * np.log(y_pred + 1e-8)) / (Y_train.shape[1] * Y_train.shape[2])\n",
        "        loss_history.append(loss)\n",
        "        print(f\"Loss: {loss:.4f}\")\n",
        "\n",
        "        # Initialisation du gradient de sortie\n",
        "        da = np.zeros_like(a)\n",
        "\n",
        "        # Créer un dictionnaire complet pour les gradients\n",
        "        gradients = {}\n",
        "\n",
        "        # Pour chaque pas de temps, calculer le gradient\n",
        "        dWy = np.zeros_like(parameters[\"Wy\"])\n",
        "        dby = np.zeros_like(parameters[\"by\"])\n",
        "\n",
        "        for t_idx in range(Y_train.shape[2]):\n",
        "            # Gradient de la cross-entropy\n",
        "            dy = y_pred[:, :, t_idx] - Y_train[:, :, t_idx]\n",
        "            # Accumuler les gradients pour Wy et by\n",
        "            dWy += np.dot(dy, a[:, :, t_idx].T)\n",
        "            dby += np.sum(dy, axis=1, keepdims=True)\n",
        "            # Gradient par rapport à a\n",
        "            da[:, :, t_idx] = np.dot(parameters[\"Wy\"].T, dy)\n",
        "\n",
        "        # Backward pass pour le reste des paramètres LSTM\n",
        "        lstm_gradients = lstm_backward(da, caches)\n",
        "\n",
        "        # Combiner tous les gradients\n",
        "        gradients = lstm_gradients.copy()\n",
        "        gradients[\"dWy\"] = dWy\n",
        "        gradients[\"dby\"] = dby\n",
        "\n",
        "        # Mise à jour des paramètres avec Adam\n",
        "        t += 1\n",
        "        parameters, v, s = update_parameters_with_adam_for_lstm(parameters, gradients, v, s, t, learning_rate)\n",
        "\n",
        "        # Sauvegarde des paramètres après cette époque\n",
        "        parameters_history.append(copy.deepcopy(parameters))\n",
        "\n",
        "    return parameters, parameters_history, loss_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fonctions de Clustering ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (2475023392.py, line 551)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 551\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mreturn optimal_n_clusters    \"\"\"\u001b[39m\n                                 ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "def cluster_parameters(parameters_history, n_clusters=3, local_clustering=False):\n",
        "\n",
        "    \"\"\"\n",
        "    Clusterise les paramètres à chaque époque.\n",
        "    \n",
        "    Arguments:\n",
        "    parameters_history -- liste de dictionnaires ou liste de listes de dictionnaires contenant les paramètres\n",
        "    n_clusters -- nombre de clusters à former\n",
        "    local_clustering -- si True, effectue une clusterisation par client/seed\n",
        "    \n",
        "    Returns:\n",
        "    kmeans_models -- modèles KMeans pour chaque époque\n",
        "    cluster_labels -- étiquettes de cluster pour chaque graine à chaque époque\n",
        "    flat_params -- paramètres aplatis pour chaque graine à chaque époque\n",
        "    param_shapes -- formes des paramètres\n",
        "    param_sizes -- tailles des paramètres aplatis\n",
        "    \"\"\"\n",
        "    if local_clustering:\n",
        "        # Dans le cas local, parameters_history est une liste par client/seed\n",
        "        n_seeds = len(parameters_history)\n",
        "        n_epochs = len(parameters_history[0])\n",
        "    else:\n",
        "        # Dans le cas global, parameters_history est déjà aplati par seed\n",
        "        n_seeds = len(parameters_history)\n",
        "        n_epochs = len(parameters_history[0])\n",
        "\n",
        "    kmeans_models = []\n",
        "    cluster_labels = np.zeros((n_seeds, n_epochs), dtype=int)\n",
        "    flat_params = []\n",
        "\n",
        "    # Obtenir les formes et tailles des paramètres\n",
        "    _, param_shapes, param_sizes = flatten_parameters(\n",
        "        parameters_history[0][0] if local_clustering else parameters_history[0][0]\n",
        "    )\n",
        "\n",
        "    # Aplatir les paramètres pour toutes les graines et époques\n",
        "    for seed in range(n_seeds):\n",
        "        seed_params = []\n",
        "        for epoch in range(n_epochs):\n",
        "            flattened, _, _ = flatten_parameters(\n",
        "                parameters_history[seed][epoch] if local_clustering else parameters_history[seed][epoch]\n",
        "            )\n",
        "            seed_params.append(flattened)\n",
        "        flat_params.append(seed_params)\n",
        "\n",
        "    flat_params = np.array(flat_params)\n",
        "\n",
        "    # Clusteriser par époque\n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_params = flat_params[:, epoch, :]\n",
        "        kmeans = KMeans(n_clusters=min(n_clusters, n_seeds), random_state=42)\n",
        "        cluster_labels[:, epoch] = kmeans.fit_predict(epoch_params)\n",
        "        kmeans_models.append(kmeans)\n",
        "\n",
        "    return kmeans_models, cluster_labels, flat_params, param_shapes, param_sizes\n",
        "\n",
        "def cluster_trajectories_with_existing_centers(client_seeds_history, cluster_centers):\n",
        "    \"\"\"\n",
        "    Attribue des clusters aux trajectoires en utilisant des centres préexistants.\n",
        "\n",
        "    Arguments:\n",
        "    client_seeds_history -- liste d'historiques de paramètres pour différentes seeds\n",
        "    cluster_centers -- centres des clusters préétablis\n",
        "\n",
        "    Returns:\n",
        "    flat_labels -- étiquettes de cluster pour chaque seed à chaque époque\n",
        "    \"\"\"\n",
        "    n_seeds = len(client_seeds_history)\n",
        "    n_epochs = len(client_seeds_history[0])\n",
        "    n_clusters = len(cluster_centers)\n",
        "\n",
        "    # Aplatir les paramètres pour toutes les graines et époques\n",
        "    flat_labels = np.zeros((n_seeds, n_epochs), dtype=int)\n",
        "\n",
        "    for seed in range(n_seeds):\n",
        "        for epoch in range(n_epochs):\n",
        "            # Aplatir les paramètres de cette époque\n",
        "            flattened, _, _ = flatten_parameters(client_seeds_history[seed][epoch])\n",
        "\n",
        "            # Calculer les distances aux centres des clusters\n",
        "            distances = np.array([np.linalg.norm(flattened - center) for center in cluster_centers])\n",
        "\n",
        "            # Attribuer au cluster le plus proche\n",
        "            flat_labels[seed, epoch] = np.argmin(distances)\n",
        "\n",
        "    return flat_labels\n",
        "\n",
        "def cluster_local_by_client_single(client_seeds_history, n_clusters):\n",
        "    \"\"\"\n",
        "    Version simplifiée de cluster_local_by_client pour un seul client\n",
        "\n",
        "    Arguments:\n",
        "    client_seeds_history -- liste de listes de dictionnaires Python contenant les paramètres\n",
        "    n_clusters -- nombre de clusters à former\n",
        "\n",
        "    Returns:\n",
        "    cluster_centers -- centres des clusters\n",
        "    transition_matrix -- matrice de transition\n",
        "    \"\"\"\n",
        "    n_seeds = len(client_seeds_history)\n",
        "    n_epochs = len(client_seeds_history[0])\n",
        "\n",
        "    # Obtenir les formes et tailles des paramètres\n",
        "    _, param_shapes, param_sizes = flatten_parameters(client_seeds_history[0][0])\n",
        "\n",
        "    # Aplatir les paramètres pour toutes les graines et époques\n",
        "    flat_params = []\n",
        "    for seed in range(n_seeds):\n",
        "        seed_params = []\n",
        "        for epoch in range(n_epochs):\n",
        "            flattened, _, _ = flatten_parameters(client_seeds_history[seed][epoch])\n",
        "            seed_params.append(flattened)\n",
        "        flat_params.append(seed_params)\n",
        "\n",
        "    flat_params = np.array(flat_params)\n",
        "     # Clusteriser par époque\n",
        "    kmeans_models = []\n",
        "    cluster_labels = np.zeros((n_seeds, n_epochs), dtype=int)   \n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_params = flat_params[:, epoch, :]\n",
        "        \n",
        "        # Vérifier et remplacer les NaN avant clustering\n",
        "        if np.isnan(epoch_params).any():\n",
        "            print(f\"NaN trouvés dans les paramètres à l'époque {epoch}, remplacement par zéros\")\n",
        "            epoch_params = np.nan_to_num(epoch_params, nan=0.0)\n",
        "            \n",
        "        kmeans = KMeans(n_clusters=min(n_clusters, n_seeds), random_state=42)\n",
        "        cluster_labels[:, epoch] = kmeans.fit_predict(epoch_params)\n",
        "        kmeans_models.append(kmeans)\n",
        "\n",
        "\n",
        "    # Calculer la matrice de transition\n",
        "    transition_matrix = np.zeros((n_clusters, n_clusters))\n",
        "\n",
        "    # Compter les transitions\n",
        "    for seed in range(n_seeds):\n",
        "        for epoch in range(n_epochs - 1):\n",
        "            from_cluster = cluster_labels[seed, epoch]\n",
        "            to_cluster = cluster_labels[seed, epoch + 1]\n",
        "            transition_matrix[from_cluster, to_cluster] += 1\n",
        "\n",
        "    # Normaliser pour obtenir les probabilités\n",
        "    for i in range(n_clusters):\n",
        "        row_sum = np.sum(transition_matrix[i])\n",
        "        if row_sum > 0:\n",
        "            transition_matrix[i] = transition_matrix[i] / row_sum\n",
        "        else:\n",
        "            # Si aucune transition n'est observée depuis ce cluster, distribution uniforme\n",
        "            transition_matrix[i] = 1.0 / n_clusters\n",
        "\n",
        "    # Utiliser les centres de clusters du dernier modèle KMeans\n",
        "    last_kmeans = kmeans_models[-1]\n",
        "    cluster_centers = last_kmeans.cluster_centers_\n",
        "\n",
        "    return cluster_centers, transition_matrix\n",
        "\n",
        "def cluster_local_by_client(parameters_history_by_client_seed, n_clusters=3, n_steps=None):\n",
        "    \"\"\"\n",
        "    Effectue une clusterisation locale pour chaque client\n",
        "\n",
        "    Arguments:\n",
        "    parameters_history_by_client_seed -- dictionnaire contenant les historiques de paramètres pour chaque client et chaque seed\n",
        "    n_clusters -- nombre de clusters à former pour chaque client\n",
        "    n_steps -- nombre d'étapes à simuler (par défaut: même que la longueur d'origine)\n",
        "\n",
        "    Returns:\n",
        "    client_clusters -- liste des centres de clusters et matrices de transition pour chaque client\n",
        "    param_shapes -- formes des paramètres\n",
        "    param_sizes -- tailles des paramètres\n",
        "    \"\"\"\n",
        "    n_clients = len(parameters_history_by_client_seed)\n",
        "    client_clusters = []\n",
        "\n",
        "    # Pour stocker les formes et tailles des paramètres (identiques pour tous les clients)\n",
        "    param_shapes = None\n",
        "    param_sizes = None\n",
        "\n",
        "    for client_id in range(n_clients):\n",
        "        client_params_history = parameters_history_by_client_seed[client_id]\n",
        "        n_seeds = len(client_params_history)\n",
        "        n_epochs = len(client_params_history[0])\n",
        "\n",
        "        if n_steps is None:\n",
        "            n_steps = n_epochs\n",
        "\n",
        "        # Clusteriser les paramètres locaux du client\n",
        "        kmeans_models, cluster_labels, flat_params, param_shapes, param_sizes = cluster_parameters(\n",
        "            client_params_history, n_clusters=min(n_clusters, n_seeds))\n",
        "\n",
        "        # Calculer la matrice de transition locale\n",
        "        transition_matrix = compute_transition_matrix(cluster_labels)\n",
        "\n",
        "        # Extraire les centres de clusters (utiliser le dernier modèle KMeans)\n",
        "        last_kmeans = kmeans_models[-1]\n",
        "        cluster_centers = last_kmeans.cluster_centers_\n",
        "\n",
        "        # Stocker les centres et la matrice pour ce client\n",
        "        client_clusters.append((cluster_centers, transition_matrix))\n",
        "\n",
        "    return client_clusters, param_shapes, param_sizes\n",
        "\n",
        "def compute_transition_matrix(cluster_labels, n_clusters=None):\n",
        "    \"\"\"\n",
        "    Calcule la matrice de transition de Markov à partir des séquences de labels de clusters.\n",
        "    \n",
        "    Arguments:\n",
        "    cluster_labels -- étiquettes de cluster pour chaque graine à chaque époque\n",
        "    n_clusters -- nombre de clusters (optionnel, calculé automatiquement si non fourni)\n",
        "    \n",
        "    Returns:\n",
        "    transition_matrix -- matrice de transition de Markov\n",
        "    \"\"\"\n",
        "    n_seeds, n_epochs = cluster_labels.shape\n",
        "    \n",
        "    # Déterminer le nombre de clusters si non fourni\n",
        "    if n_clusters is None:\n",
        "        n_clusters = np.max(cluster_labels) + 1\n",
        "\n",
        "    transition_counts = np.zeros((n_clusters, n_clusters))\n",
        "\n",
        "    # Compter les transitions\n",
        "    for seed in range(n_seeds):\n",
        "        for epoch in range(n_epochs - 1):\n",
        "            from_cluster = cluster_labels[seed, epoch]\n",
        "            to_cluster = cluster_labels[seed, epoch + 1]\n",
        "            transition_counts[from_cluster, to_cluster] += 1\n",
        "\n",
        "    # Normaliser pour obtenir les probabilités\n",
        "    transition_matrix = np.zeros_like(transition_counts)\n",
        "    for i in range(n_clusters):\n",
        "        row_sum = np.sum(transition_counts[i])\n",
        "        if row_sum > 0:\n",
        "            transition_matrix[i] = transition_counts[i] / row_sum\n",
        "        else:\n",
        "            # Si aucune transition n'est observée depuis ce cluster, distribution uniforme\n",
        "            transition_matrix[i] = 1.0 / n_clusters\n",
        "\n",
        "    return transition_matrix\n",
        "\n",
        "def simulate_parameter_trajectory(initial_cluster, transition_matrix, n_steps, kmeans_models):\n",
        "    \"\"\"\n",
        "    Simule une trajectoire de paramètres basée sur la matrice de transition.\n",
        "\n",
        "    Arguments:\n",
        "    initial_cluster -- cluster initial\n",
        "    transition_matrix -- matrice de transition de Markov\n",
        "    n_steps -- nombre d'étapes à simuler\n",
        "    kmeans_models -- modèles KMeans pour chaque époque ou un modèle unique\n",
        "\n",
        "    Returns:\n",
        "    trajectory -- trajectoire simulée de paramètres\n",
        "    cluster_sequence -- séquence de clusters visitée\n",
        "    \"\"\"\n",
        "    n_clusters = transition_matrix.shape[0]\n",
        "    cluster_sequence = [initial_cluster]\n",
        "    current_cluster = initial_cluster\n",
        "\n",
        "    for _ in range(n_steps - 1):\n",
        "        # Échantillonner le prochain cluster\n",
        "        next_cluster = np.random.choice(n_clusters, p=transition_matrix[current_cluster])\n",
        "        cluster_sequence.append(next_cluster)\n",
        "        current_cluster = next_cluster\n",
        "\n",
        "    # Convertir la séquence de clusters en paramètres\n",
        "    trajectory = []\n",
        "    for step, cluster in enumerate(cluster_sequence):\n",
        "        # Utiliser le centre du cluster comme paramètres représentatifs\n",
        "        # Si nous avons dépassé le nombre d'époques dans kmeans_models, utiliser le dernier\n",
        "        if isinstance(kmeans_models, list):\n",
        "            model_idx = min(step, len(kmeans_models) - 1)\n",
        "            trajectory.append(kmeans_models[model_idx].cluster_centers_[cluster])\n",
        "        else:\n",
        "            # Si kmeans_models est un unique modèle, l'utiliser directement\n",
        "            trajectory.append(kmeans_models.cluster_centers_[cluster])\n",
        "\n",
        "    return trajectory, cluster_sequence\n",
        "\n",
        "def update_with_transition_matrices_only(client_transition_matrices, client_cluster_centers, param_shapes, param_sizes, n_clients):\n",
        "    \"\"\"\n",
        "    Met à jour le modèle global en utilisant uniquement les matrices de transition.\n",
        "\n",
        "    Arguments:\n",
        "    client_transition_matrices -- liste des matrices de transition de chaque client\n",
        "    client_cluster_centers -- liste des centres de clusters de chaque client (définis en phase 1)\n",
        "    param_shapes -- formes des paramètres\n",
        "    param_sizes -- tailles des paramètres aplatis\n",
        "    n_clients -- nombre de clients\n",
        "\n",
        "    Returns:\n",
        "    updated_global_params -- paramètres globaux mis à jour\n",
        "    \"\"\"\n",
        "    # Initialiser les paramètres globaux\n",
        "    aggregated_params_flat = np.zeros_like(client_cluster_centers[0][0])\n",
        "\n",
        "    # Pour chaque client\n",
        "    for client_id in range(n_clients):\n",
        "        transition_matrix = client_transition_matrices[client_id]\n",
        "        cluster_centers = client_cluster_centers[client_id]\n",
        "        n_clusters = transition_matrix.shape[0]\n",
        "\n",
        "        # Calculer la distribution stationnaire de la matrice de transition\n",
        "        pi = np.ones(n_clusters) / n_clusters  # Distribution initiale uniforme\n",
        "        for _ in range(100):  # Nombre d'itérations arbitraire pour convergence\n",
        "            pi_new = np.dot(pi, transition_matrix)\n",
        "            if np.allclose(pi, pi_new):\n",
        "                break\n",
        "            pi = pi_new\n",
        "\n",
        "        # Pondérer les centres par la distribution stationnaire\n",
        "        client_params_flat = np.zeros_like(cluster_centers[0])\n",
        "        for i in range(n_clusters):\n",
        "            client_params_flat += pi[i] * cluster_centers[i]\n",
        "\n",
        "        # Ajouter à l'agrégation globale\n",
        "        aggregated_params_flat += client_params_flat / n_clients\n",
        "\n",
        "    # Reconstruire les paramètres à leur forme d'origine\n",
        "    updated_global_params = unflatten_parameters(aggregated_params_flat, param_shapes, param_sizes)\n",
        "\n",
        "    return updated_global_params\n",
        "\n",
        "def visualize_natural_clusters_from_data(clients_data, n_epochs=50, n_clusters_range=range(2, 6), n_a=64, n_x=10, n_y=5):\n",
        "    \"\"\"\n",
        "    Visualise les clusters naturels dans les données d'entraînement des 3 villes.\n",
        "    \n",
        "    Arguments:\n",
        "    clients_data -- données des clients (villes)\n",
        "    n_epochs -- nombre d'époques d'entraînement à simuler\n",
        "    n_clusters_range -- plage de nombres de clusters à tester\n",
        "    n_a, n_x, n_y -- dimensions du modèle LSTM\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import time\n",
        "    from sklearn.decomposition import PCA\n",
        "    from sklearn.cluster import KMeans\n",
        "    from sklearn.metrics import silhouette_score\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    \n",
        "    print(\"\\n=== Analyse des clusters naturels dans les données des 3 villes ===\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # 1. Générer des trajectoires d'entraînement pour chaque ville\n",
        "    print(\"Génération des trajectoires d'entraînement...\")\n",
        "    parameters_history_by_city = []\n",
        "    \n",
        "    n_clients = len(clients_data)\n",
        "    \n",
        "    # Inspection des dimensions des données pour debug\n",
        "    print(f\"Nombre de clients: {n_clients}\")\n",
        "    for client_id, client_data in enumerate(clients_data):\n",
        "        X_c, Y_c = client_data[0]\n",
        "        print(f\"Dimensions pour client {client_id}: X_c shape={X_c.shape}, Y_c shape={Y_c.shape}\")\n",
        "        \n",
        "        # Vérifier et corriger les NaN dans les données d'entrée\n",
        "        if np.isnan(X_c).any():\n",
        "            print(f\"Client {client_id}: NaN trouvés dans X_c, remplacement par zéros\")\n",
        "            X_c = np.nan_to_num(X_c, nan=0.0)\n",
        "            clients_data[client_id][0] = (X_c, Y_c)  # Mise à jour des données\n",
        "            \n",
        "        if np.isnan(Y_c).any():\n",
        "            print(f\"Client {client_id}: NaN trouvés dans Y_c, remplacement par zéros\")\n",
        "            Y_c = np.nan_to_num(Y_c, nan=0.0)\n",
        "            clients_data[client_id][0] = (X_c, Y_c)  # Mise à jour des données\n",
        "    \n",
        "    for client_id, client_data in enumerate(clients_data):\n",
        "        X_c, Y_c = client_data[0]  # Prendre le premier seed de chaque client\n",
        "        \n",
        "        print(f\"Entraînement de la ville {client_id+1}/{n_clients}...\")\n",
        "        city_params_history = []\n",
        "        \n",
        "        # Générer 3 trajectoires différentes pour chaque ville (réduit pour accélérer)\n",
        "        for seed_id in range(3):\n",
        "            base_seed = client_id * 100 + seed_id\n",
        "            # Assurez-vous que les dimensions sont correctes ici\n",
        "            params, history, _ = train_lstm(X_c, Y_c, n_a, n_x, n_y, \n",
        "                                        num_epochs=n_epochs, seed=base_seed)\n",
        "            city_params_history.extend(history)  # Ajouter toute la trajectoire\n",
        "        \n",
        "        parameters_history_by_city.append(city_params_history)\n",
        "    \n",
        "    # 2. Aplatir les paramètres pour l'analyse\n",
        "    print(\"Préparation des données pour l'analyse de clusters...\")\n",
        "    flat_params_list = []\n",
        "    city_labels = []  # Pour tracer les villes avec des couleurs différentes\n",
        "    \n",
        "    for city_id, city_history in enumerate(parameters_history_by_city):\n",
        "        for params in city_history:\n",
        "            flattened, _, _ = flatten_parameters(params)\n",
        "            \n",
        "            # Vérifier et remplacer les NaN dans les paramètres aplatis\n",
        "            if np.isnan(flattened).any():\n",
        "                print(f\"NaN trouvés dans les paramètres aplatis pour la ville {city_id}, remplacement par zéros\")\n",
        "                flattened = np.nan_to_num(flattened, nan=0.0)\n",
        "                \n",
        "            flat_params_list.append(flattened)\n",
        "            city_labels.append(city_id)\n",
        "    \n",
        "    flat_params = np.array(flat_params_list)\n",
        "    city_labels = np.array(city_labels)\n",
        "    \n",
        "    # Vérification finale pour les NaN avant PCA\n",
        "    if np.isnan(flat_params).any():\n",
        "        print(\"ATTENTION: NaN toujours présents dans flat_params après traitement, remplacement par zéros\")\n",
        "        flat_params = np.nan_to_num(flat_params, nan=0.0)\n",
        "    \n",
        "    print(f\"Données préparées: {flat_params.shape[0]} échantillons de dimension {flat_params.shape[1]}\")\n",
        "    \n",
        "    # 3. Réduction de dimensionnalité pour visualisation\n",
        "    print(\"Réduction de dimensionnalité avec PCA...\")\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    reduced_data = pca.fit_transform(flat_params)\n",
        "    \n",
        "    explained_var = np.sum(pca.explained_variance_ratio_)\n",
        "    print(f\"PCA 2D: {explained_var:.2%} de variance expliquée\")\n",
        "    \n",
        "    # 4. Trouver le nombre optimal de clusters\n",
        "    print(\"Recherche du nombre optimal de clusters...\")\n",
        "    silhouette_scores = []\n",
        "    inertia_values = []\n",
        "    \n",
        "    for n_clusters in n_clusters_range:\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "        cluster_labels = kmeans.fit_predict(flat_params)\n",
        "        \n",
        "        # Stocker l'inertie\n",
        "        inertia_values.append(kmeans.inertia_)\n",
        "        \n",
        "        # Calculer le score silhouette\n",
        "        try:\n",
        "            silhouette = silhouette_score(flat_params, cluster_labels)\n",
        "            silhouette_scores.append(silhouette)\n",
        "            print(f\"  {n_clusters} clusters: score silhouette = {silhouette:.4f}\")\n",
        "        except:\n",
        "            silhouette_scores.append(float('-inf'))\n",
        "            print(f\"  {n_clusters} clusters: score silhouette non calculable\")\n",
        "    \n",
        "    # Déterminer le nombre optimal de clusters\n",
        "    if len(silhouette_scores) > 0:\n",
        "        best_silhouette = n_clusters_range[np.argmax(silhouette_scores)]\n",
        "    else:\n",
        "        best_silhouette = n_clusters_range[0]\n",
        "    \n",
        "    # Méthode du coude pour l'inertie\n",
        "    if len(inertia_values) > 2:\n",
        "        inertia_diff = np.diff(inertia_values)\n",
        "        inertia_diff2 = np.diff(inertia_diff)\n",
        "        elbow_idx = np.argmax(np.abs(inertia_diff2))\n",
        "        best_elbow = n_clusters_range[min(elbow_idx + 1, len(n_clusters_range) - 1)]\n",
        "    else:\n",
        "        best_elbow = n_clusters_range[0]\n",
        "    \n",
        "    print(f\"\\nMeilleur nombre de clusters:\")\n",
        "    print(f\"  Score silhouette: {best_silhouette}\")\n",
        "    print(f\"  Méthode du coude: {best_elbow}\")\n",
        "    \n",
        "    # Consensus\n",
        "    optimal_n_clusters = max([best_silhouette, best_elbow], key=[best_silhouette, best_elbow].count)\n",
        "    print(f\"Nombre optimal de clusters suggéré: {optimal_n_clusters}\")\n",
        "    \n",
        "    # 5. Créer les visualisations\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    # Graphique 1: Score silhouette\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(list(n_clusters_range), silhouette_scores, 'o-', color='blue')\n",
        "    plt.axvline(x=best_silhouette, color='red', linestyle='--')\n",
        "    plt.xlabel('Nombre de clusters')\n",
        "    plt.ylabel('Score silhouette')\n",
        "    plt.title('Score silhouette par nombre de clusters')\n",
        "    plt.grid(alpha=0.3)\n",
        "    \n",
        "    # Graphique 2: Inertie (méthode du coude)\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(list(n_clusters_range), inertia_values, 'o-', color='purple')\n",
        "    plt.axvline(x=best_elbow, color='red', linestyle='--')\n",
        "    plt.xlabel('Nombre de clusters')\n",
        "    plt.ylabel('Inertie')\n",
        "    plt.title('Méthode du coude pour l\\'inertie')\n",
        "    plt.grid(alpha=0.3)\n",
        "    \n",
        "    # Graphique 3: Visualisation par ville d'origine\n",
        "    plt.subplot(2, 2, 3)\n",
        "    city_names = [f\"Ville {i+1}\" for i in range(n_clients)]\n",
        "    city_colors = ['red', 'blue', 'green']\n",
        "    \n",
        "    for city_id in range(n_clients):\n",
        "        city_points = reduced_data[city_labels == city_id]\n",
        "        plt.scatter(city_points[:, 0], city_points[:, 1], \n",
        "                   color=city_colors[city_id], label=city_names[city_id],\n",
        "                   alpha=0.7, edgecolors='w', linewidths=0.5)\n",
        "    \n",
        "    plt.xlabel('Composante principale 1')\n",
        "    plt.ylabel('Composante principale 2')\n",
        "    plt.title(f'Visualisation par ville d\\'origine ({explained_var:.1%} var.)')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    \n",
        "    # Graphique 4: Visualisation avec clustering optimal\n",
        "    plt.subplot(2, 2, 4)\n",
        "    \n",
        "    # Effectuer le clustering avec le nombre optimal\n",
        "    kmeans = KMeans(n_clusters=optimal_n_clusters, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans.fit_predict(flat_params)\n",
        "    \n",
        "    # Colormap pour les clusters\n",
        "    cmap = plt.cm.get_cmap('tab10', optimal_n_clusters)\n",
        "    \n",
        "    for cluster in range(optimal_n_clusters):\n",
        "        cluster_points = reduced_data[cluster_labels == cluster]\n",
        "        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], \n",
        "                   c=[cmap(cluster)], label=f'Cluster {cluster+1}',\n",
        "                   alpha=0.7, edgecolors='w', linewidths=0.5)\n",
        "    \n",
        "    plt.xlabel('Composante principale 1')\n",
        "    plt.ylabel('Composante principale 2')\n",
        "    plt.title(f'Visualisation des {optimal_n_clusters} clusters naturels')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('clusters_naturels_villes.png')\n",
        "    plt.show()\n",
        "    \n",
        "    # Figure supplémentaire: Distribution des villes dans les clusters\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    # Créer une matrice de distribution ville x cluster\n",
        "    distribution = np.zeros((n_clients, optimal_n_clusters))\n",
        "    \n",
        "    for i in range(len(city_labels)):\n",
        "        city = city_labels[i]\n",
        "        cluster = cluster_labels[i]\n",
        "        distribution[city, cluster] += 1\n",
        "    \n",
        "    # Normaliser par ligne (pourcentage par ville)\n",
        "    for i in range(n_clients):\n",
        "        if np.sum(distribution[i]) > 0:\n",
        "            distribution[i] = distribution[i] / np.sum(distribution[i]) * 100\n",
        "    \n",
        "    # Tracer la heatmap\n",
        "    sns.heatmap(distribution, annot=True, fmt='.1f', cmap='viridis',\n",
        "               xticklabels=[f'Cluster {i+1}' for i in range(optimal_n_clusters)],\n",
        "               yticklabels=city_names,\n",
        "               cbar_kws={'label': 'Pourcentage (%)'})\n",
        "    \n",
        "    plt.xlabel('Cluster')\n",
        "    plt.ylabel('Ville')\n",
        "    plt.title('Distribution des villes dans les clusters naturels')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('distribution_villes_clusters.png')\n",
        "    plt.show()\n",
        "    \n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"Analyse complète en {total_time:.2f} secondes\")\n",
        "    \n",
        "    return optimal_n_clusters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fonctions utilitaires ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def flatten_parameters(parameters):\n",
        "    \"\"\"\n",
        "    Aplatit les paramètres d'un LSTM en un seul vecteur.\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- dictionnaire Python contenant les paramètres\n",
        "\n",
        "    Returns:\n",
        "    flattened -- vecteur aplati des paramètres\n",
        "    param_shapes -- formes originales des paramètres\n",
        "    param_sizes -- tailles des paramètres aplatis\n",
        "    \"\"\"\n",
        "    flattened = []\n",
        "    param_shapes = {}\n",
        "    param_sizes = {}\n",
        "\n",
        "    for key in [\"Wf\", \"bf\", \"Wi\", \"bi\", \"Wc\", \"bc\", \"Wo\", \"bo\", \"Wy\", \"by\"]:\n",
        "        param = parameters[key]\n",
        "        param_shapes[key] = param.shape\n",
        "        flattened.append(param.flatten())\n",
        "        param_sizes[key] = param.size\n",
        "\n",
        "    return np.concatenate(flattened), param_shapes, param_sizes\n",
        "\n",
        "def unflatten_parameters(flattened, param_shapes, param_sizes):\n",
        "    \"\"\"\n",
        "    Restaure un vecteur aplati de paramètres à leur forme originale.\n",
        "\n",
        "    Arguments:\n",
        "    flattened -- vecteur aplati des paramètres\n",
        "    param_shapes -- formes originales des paramètres\n",
        "    param_sizes -- tailles des paramètres aplatis\n",
        "\n",
        "    Returns:\n",
        "    parameters -- dictionnaire Python contenant les paramètres\n",
        "    \"\"\"\n",
        "    parameters = {}\n",
        "    start = 0\n",
        "\n",
        "    for key in [\"Wf\", \"bf\", \"Wi\", \"bi\", \"Wc\", \"bc\", \"Wo\", \"bo\", \"Wy\", \"by\"]:\n",
        "        size = param_sizes[key]\n",
        "        parameters[key] = flattened[start:start+size].reshape(param_shapes[key])\n",
        "        start += size\n",
        "\n",
        "    return parameters\n",
        "\n",
        "def calculate_transmission_size(parameters=None, n_clusters=None, transition_matrix=None, client_centers=None):\n",
        "    \"\"\"\n",
        "    Calcule la taille de transmission en octets.\n",
        "\n",
        "    Args:\n",
        "        parameters: Dictionnaire ou liste des paramètres originaux (arrays numpy)\n",
        "        n_clusters: Nombre de clusters utilisés\n",
        "        transition_matrix: Matrice de transition entre clusters\n",
        "        client_centers: Centres des clusters par client (liste de tuples (centers, transition_matrix))\n",
        "\n",
        "    Returns:\n",
        "        Taille totale en octets\n",
        "    \"\"\"\n",
        "    total_size = 0\n",
        "\n",
        "    # Cas 1: Paramètres originaux (dictionnaire)\n",
        "    if parameters is not None and isinstance(parameters, dict):\n",
        "        for key in parameters:\n",
        "            if hasattr(parameters[key], 'size'):\n",
        "                # Chaque nombre flottant occupe 4 octets (float32)\n",
        "                total_size += parameters[key].size * 4\n",
        "        return total_size\n",
        "\n",
        "    # Cas 2: Paramètres originaux (liste)\n",
        "    elif parameters is not None and isinstance(parameters, list) and all(hasattr(p, 'size') for p in parameters if p is not None):\n",
        "        for param in parameters:\n",
        "            if param is not None and hasattr(param, 'size'):\n",
        "                # Chaque nombre flottant occupe 4 octets (float32)\n",
        "                total_size += param.size * 4\n",
        "        return total_size\n",
        "\n",
        "    # Cas 3: Clusterisation globale avec n_clusters et transition_matrix\n",
        "    elif n_clusters is not None and transition_matrix is not None:\n",
        "        # Taille de la matrice de transition\n",
        "        if hasattr(transition_matrix, 'size'):\n",
        "            total_size += transition_matrix.size * 4\n",
        "\n",
        "        # Si nous avons des centres de clusters explicites\n",
        "        if client_centers is not None and not isinstance(client_centers, list):\n",
        "            if hasattr(client_centers, 'size'):\n",
        "                total_size += client_centers.size * 4\n",
        "\n",
        "        return total_size\n",
        "\n",
        "    # Cas 4: Clusterisation locale avec client_centers\n",
        "    elif client_centers is not None and isinstance(client_centers, list):\n",
        "        for client_data in client_centers:\n",
        "            # Dans la clusterisation locale, client_data est un tuple (cluster_centers, transition_matrix)\n",
        "            if isinstance(client_data, tuple) and len(client_data) == 2:\n",
        "                centers, trans_matrix = client_data\n",
        "\n",
        "                # Taille des centres de clusters\n",
        "                if hasattr(centers, 'size'):\n",
        "                    total_size += centers.size * 4\n",
        "\n",
        "                # Taille de la matrice de transition\n",
        "                if hasattr(trans_matrix, 'size'):\n",
        "                    total_size += trans_matrix.size * 4\n",
        "\n",
        "        return total_size\n",
        "\n",
        "    # En cas d'erreur ou de paramètres non valides\n",
        "    raise ValueError(\"Entrée invalide pour le calcul de la taille de transmission\")\n",
        "\n",
        "def aggregate_client_clusters(client_clusters, param_shapes, param_sizes, n_clients, weight_by_client=None):\n",
        "    \"\"\"\n",
        "    Agrège les clusters de paramètres de plusieurs clients\n",
        "\n",
        "    Arguments:\n",
        "    client_clusters -- liste des centres de clusters et matrices de transition pour chaque client\n",
        "    param_shapes -- formes des paramètres\n",
        "    param_sizes -- tailles des paramètres\n",
        "    n_clients -- nombre de clients\n",
        "    weight_by_client -- poids pour l'agrégation de chaque client (optionnel)\n",
        "\n",
        "    Returns:\n",
        "    aggregated_params -- paramètres agrégés à partir des clusters des clients\n",
        "    \"\"\"\n",
        "    # Si les poids ne sont pas fournis, utiliser un poids uniforme\n",
        "    if weight_by_client is None:\n",
        "        weight_by_client = np.ones(n_clients) / n_clients\n",
        "\n",
        "    # Initialiser les paramètres agrégés\n",
        "    all_flattened_params = []\n",
        "\n",
        "    # Pour chaque client, simuler une trajectoire à partir de ses clusters\n",
        "    for client_id, (cluster_centers, transition_matrix) in enumerate(client_clusters):\n",
        "        # Choisir un cluster de départ (utiliser le cluster le plus fréquent ou le premier)\n",
        "        initial_cluster = 0\n",
        "\n",
        "        # Obtenir un ensemble de paramètres représentatifs de ce client\n",
        "        # Simplement moyenne des centres de clusters pondérée par la probabilité stationnaire\n",
        "        n_clusters = transition_matrix.shape[0]\n",
        "\n",
        "        # Calculer la distribution stationnaire (au lieu de simuler une trajectoire)\n",
        "        # Méthode simple: itérer la matrice de transition jusqu'à convergence\n",
        "        pi = np.ones(n_clusters) / n_clusters  # Distribution initiale uniforme\n",
        "        for _ in range(100):  # Nombre d'itérations arbitraire, devrait converger rapidement\n",
        "            pi_new = np.dot(pi, transition_matrix)\n",
        "            if np.allclose(pi, pi_new):\n",
        "                break\n",
        "            pi = pi_new\n",
        "\n",
        "        # Pondérer les centres par la distribution stationnaire\n",
        "        client_params = np.zeros_like(cluster_centers[0])\n",
        "        for i in range(n_clusters):\n",
        "            client_params += pi[i] * cluster_centers[i]\n",
        "\n",
        "        # Ajouter à la liste des paramètres aplatis\n",
        "        all_flattened_params.append(client_params)\n",
        "\n",
        "    # Combiner tous les paramètres avec les poids des clients\n",
        "    aggregated_flattened = np.zeros_like(all_flattened_params[0])\n",
        "    for client_id, flattened in enumerate(all_flattened_params):\n",
        "        aggregated_flattened += weight_by_client[client_id] * flattened\n",
        "\n",
        "    # Reconstruire les paramètres à leur forme d'origine\n",
        "    aggregated_params = unflatten_parameters(aggregated_flattened, param_shapes, param_sizes)\n",
        "\n",
        "    return aggregated_params\n",
        "\n",
        "def update_with_transition_matrices(client_transition_matrices, global_kmeans_model, initial_global_params, param_shapes, param_sizes):\n",
        "    \"\"\"\n",
        "    Met à jour le modèle global en utilisant uniquement les matrices de transition des clients.\n",
        "\n",
        "    Arguments:\n",
        "    client_transition_matrices -- liste des matrices de transition de chaque client\n",
        "    global_kmeans_model -- modèle KMeans global utilisé pour la clusterisation\n",
        "    initial_global_params -- paramètres initiaux obtenus par FedAvg\n",
        "    param_shapes -- formes des paramètres\n",
        "    param_sizes -- tailles des paramètres aplatis\n",
        "\n",
        "    Returns:\n",
        "    updated_global_params -- paramètres globaux mis à jour\n",
        "    \"\"\"\n",
        "    n_clients = len(client_transition_matrices)\n",
        "    n_clusters = global_kmeans_model.cluster_centers_.shape[0]\n",
        "\n",
        "    # Agréger les matrices de transition des clients\n",
        "    global_transition_matrix = np.zeros((n_clusters, n_clusters))\n",
        "    for client_id in range(n_clients):\n",
        "        global_transition_matrix += client_transition_matrices[client_id]\n",
        "\n",
        "    # Normaliser la matrice de transition globale\n",
        "    for i in range(n_clusters):\n",
        "        row_sum = np.sum(global_transition_matrix[i])\n",
        "        if row_sum > 0:\n",
        "            global_transition_matrix[i] = global_transition_matrix[i] / row_sum\n",
        "        else:\n",
        "            global_transition_matrix[i] = np.ones(n_clusters) / n_clusters\n",
        "\n",
        "    # Calculer la distribution stationnaire de la matrice de transition\n",
        "    pi = np.ones(n_clusters) / n_clusters  # Distribution initiale uniforme\n",
        "    for _ in range(100):  # Nombre d'itérations arbitraire pour convergence\n",
        "        pi_new = np.dot(pi, global_transition_matrix)\n",
        "        if np.allclose(pi, pi_new):\n",
        "            break\n",
        "        pi = pi_new\n",
        "\n",
        "    # Créer le nouveau modèle global en pondérant les centres par la distribution stationnaire\n",
        "    weighted_centers = np.zeros_like(global_kmeans_model.cluster_centers_[0])\n",
        "    for i in range(n_clusters):\n",
        "        weighted_centers += pi[i] * global_kmeans_model.cluster_centers_[i]\n",
        "\n",
        "    # Convertir les paramètres aplatis en structure de dictionnaire\n",
        "    updated_global_params = unflatten_parameters(weighted_centers, param_shapes, param_sizes)\n",
        "\n",
        "    return updated_global_params\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fonctions de données ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "weather_switerland_month_1 = pd.read_csv('Switzerland\\weather_switerland_month1.csv')\n",
        "weather_switerland_month_2 = pd.read_csv('Switzerland\\weather_switerland_month2.csv')\n",
        "weather_switerland_month_3 = pd.read_csv('Switzerland\\weather_switerland_month3.csv')\n",
        "weather_switerland_month_4 = pd.read_csv('Switzerland\\weather_switerland_month4.csv')\n",
        "weather_switerland_month_5 = pd.read_csv('Switzerland\\weather_switerland_month5.csv')\n",
        "weather_switerland_month_6 = pd.read_csv('Switzerland\\weather_switerland_month6.csv')\n",
        "weather_switerland_month_7 = pd.read_csv('Switzerland\\weather_switerland_month7.csv')\n",
        "weather_switerland_month_8 = pd.read_csv('Switzerland\\weather_switerland_month8.csv')\n",
        "weather_switerland_month_9 = pd.read_csv('Switzerland\\weather_switerland_month9.csv')\n",
        "weather_switerland_month_10 = pd.read_csv('Switzerland\\weather_switerland_month10.csv')\n",
        "weather_switerland_month_11 = pd.read_csv('Switzerland\\weather_switerland_month11.csv')\n",
        "weather_switerland_month_12 = pd.read_csv('Switzerland\\weather_switerland_month12.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "weather_norway_month_1 = pd.read_csv('Norway\\weather_norway_month1.csv')\n",
        "weather_norway_month_2 = pd.read_csv('Norway\\weather_norway_month2.csv')\n",
        "weather_norway_month_3 = pd.read_csv('Norway\\weather_norway_month3.csv')\n",
        "weather_norway_month_4 = pd.read_csv('Norway\\weather_norway_month4.csv')\n",
        "weather_norway_month_5 = pd.read_csv('Norway\\weather_norway_month5.csv')\n",
        "weather_norway_month_6 = pd.read_csv('Norway\\weather_norway_month6.csv')\n",
        "weather_norway_month_7 = pd.read_csv('Norway\\weather_norway_month7.csv')\n",
        "weather_norway_month_8 = pd.read_csv('Norway\\weather_norway_month8.csv')\n",
        "weather_norway_month_9 = pd.read_csv('Norway\\weather_norway_month9.csv')\n",
        "weather_norway_month_10 = pd.read_csv('Norway\\weather_norway_month10.csv')\n",
        "weather_norway_month_11 = pd.read_csv('Norway\\weather_norway_month11.csv')\n",
        "weather_norway_month_12 = pd.read_csv('Norway\\weather_norway_month12.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "weather_estonia_month_1 = pd.read_csv('Estonia\\weather_estonia_month1.csv')\n",
        "weather_estonia_month_2 = pd.read_csv('Estonia\\weather_estonia_month2.csv')\n",
        "weather_estonia_month_3 = pd.read_csv('Estonia\\weather_estonia_month3.csv')\n",
        "weather_estonia_month_4 = pd.read_csv('Estonia\\weather_estonia_month4.csv')\n",
        "weather_estonia_month_5 = pd.read_csv('Estonia\\weather_estonia_month5.csv')\n",
        "weather_estonia_month_6 = pd.read_csv('Estonia\\weather_estonia_month6.csv')\n",
        "weather_estonia_month_7 = pd.read_csv('Estonia\\weather_estonia_month7.csv')\n",
        "weather_estonia_month_8 = pd.read_csv('Estonia\\weather_estonia_month8.csv')\n",
        "weather_estonia_month_9 = pd.read_csv('Estonia\\weather_estonia_month9.csv')\n",
        "weather_estonia_month_10 = pd.read_csv('Estonia\\weather_estonia_month10.csv')\n",
        "weather_estonia_month_11 = pd.read_csv('Estonia\\weather_estonia_month11.csv')\n",
        "weather_estonia_month_12 = pd.read_csv('Estonia\\weather_estonia_month12.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_realistic_consumption(weather_df, country):\n",
        "    # Créer une série temporelle pour la consommation\n",
        "    consumption = np.zeros(len(weather_df))\n",
        "    \n",
        "    # Base de consommation par pays (MW)\n",
        "    base_consumption = {\n",
        "        \"Switzerland\": 6000,\n",
        "        \"Norway\": 8000,\n",
        "        \"Estonia\": 2000\n",
        "    }\n",
        "    base = base_consumption.get(country, 5000)\n",
        "    \n",
        "    # Facteurs d'influence\n",
        "    for i in range(len(weather_df)):\n",
        "        # Heure de la journée (cycle quotidien)\n",
        "        hour = pd.to_datetime(weather_df['datetime'].iloc[i]).hour\n",
        "        day_factor = 1.0 + 0.2 * np.sin((hour - 8) * np.pi / 12)  # Pic à 8h et 20h\n",
        "        \n",
        "        # Température (plus il fait froid, plus la consommation est élevée)\n",
        "        temp = weather_df['temp'].iloc[i]\n",
        "        temp_factor = 1.0 + max(0, (15 - temp) / 15)  # Augmente quand temp < 15°C\n",
        "        \n",
        "        # Vent (plus il y a de vent, plus la consommation est élevée - chauffage)\n",
        "        wind = weather_df['windspeed'].iloc[i]\n",
        "        wind_factor = 1.0 + wind / 50  # Effet modéré\n",
        "        \n",
        "        # Visibilité/nuages (plus il fait sombre, plus l'éclairage est utilisé)\n",
        "        cloud = weather_df['cloudcover'].iloc[i] if 'cloudcover' in weather_df else 50\n",
        "        cloud_factor = 1.0 + cloud / 200  # Effet faible\n",
        "        \n",
        "        # Jour de la semaine (weekend vs jour ouvrable)\n",
        "        weekday = pd.to_datetime(weather_df['datetime'].iloc[i]).weekday()\n",
        "        weekday_factor = 0.9 if weekday >= 5 else 1.0  # Baisse le weekend\n",
        "        \n",
        "        # Calculer la consommation pour cette heure\n",
        "        consumption[i] = base * day_factor * temp_factor * wind_factor * cloud_factor * weekday_factor\n",
        "        \n",
        "        # Ajouter un bruit aléatoire (±5%)\n",
        "        consumption[i] *= np.random.uniform(0.95, 1.05)\n",
        "    \n",
        "    return pd.DataFrame({'Consumption': consumption})\n",
        "\n",
        "def load_real_data(n_clients, n_seeds_per_client, n_x, n_y, sequence_length):\n",
        "    \"\"\"\n",
        "    Charge les données réelles des pays depuis leur structure de dossiers.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    \n",
        "    # Répertoire de travail\n",
        "    current_dir = os.getcwd()\n",
        "    print(f\"Répertoire de travail actuel: {current_dir}\")\n",
        "    print(f\"Contenu du répertoire: {os.listdir()}\")\n",
        "    \n",
        "    # Liste des pays disponibles (dossiers)\n",
        "    countries = [\"Switzerland\", \"Norway\", \"Estonia\"]\n",
        "    available_countries = []\n",
        "    \n",
        "    # Vérifier quels pays ont des données\n",
        "    for country in countries:\n",
        "        if os.path.exists(country) and os.path.isdir(country):\n",
        "            files = os.listdir(country)\n",
        "            csv_files = [f for f in files if f.endswith('.csv')]\n",
        "            if csv_files:\n",
        "                available_countries.append(country)\n",
        "                print(f\"Pays {country} trouvé avec {len(csv_files)} fichiers CSV\")\n",
        "            else:\n",
        "                print(f\"Aucun fichier CSV trouvé pour {country}\")\n",
        "        else:\n",
        "            print(f\"Dossier {country} introuvable\")\n",
        "    \n",
        "    if not available_countries:\n",
        "        raise ValueError(\"Aucun fichier de données météo trouvé pour les pays spécifiés.\")\n",
        "    \n",
        "    print(f\"Pays disponibles: {available_countries}\")\n",
        "    \n",
        "    # Limiter le nombre de clients aux pays disponibles\n",
        "    n_clients = min(n_clients, len(available_countries))\n",
        "    clients_data = []\n",
        "    \n",
        "    for client_id in range(n_clients):\n",
        "        country = available_countries[client_id]\n",
        "        client_seeds_data = []\n",
        "        \n",
        "        print(f\"Chargement des données pour {country} (client {client_id+1})...\")\n",
        "        \n",
        "        # Charger les données météo pour ce pays\n",
        "        weather_data = []\n",
        "        csv_files = [f for f in os.listdir(country) if f.endswith('.csv')]\n",
        "        \n",
        "        for file_name in csv_files:\n",
        "            file_path = os.path.join(country, file_name)\n",
        "            try:\n",
        "                df_month = pd.read_csv(file_path)\n",
        "                weather_data.append(df_month)\n",
        "                print(f\"Chargé {file_name}, {df_month.shape[0]} lignes\")\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur lors du chargement de {file_path}: {e}\")\n",
        "        \n",
        "        if not weather_data:\n",
        "            print(f\"Aucune donnée n'a pu être chargée pour {country}. Ce client sera ignoré.\")\n",
        "            continue\n",
        "        \n",
        "        # Concaténer les données météo\n",
        "        weather_df = pd.concat(weather_data, ignore_index=True)\n",
        "        print(f\"Données chargées pour {country}: {len(weather_df)} enregistrements\")\n",
        "        print(f\"Colonnes disponibles: {weather_df.columns.tolist()}\")\n",
        "        \n",
        "        # Générer des données de consommation\n",
        "        consumption_df = generate_consumption_from_weather(weather_df, country)\n",
        "        \n",
        "        # Vérifier et préparer les données\n",
        "        if 'datetime' not in weather_df.columns:\n",
        "            weather_df['datetime'] = pd.date_range(start='2023-01-01', periods=len(weather_df), freq='H')\n",
        "        \n",
        "        # Sélectionner les features\n",
        "        features = select_weather_features(weather_df, n_x)\n",
        "        print(f\"Features sélectionnées pour {country}: {features}\")\n",
        "        \n",
        "        # Préparer les données\n",
        "        X_data = weather_df[features].values\n",
        "        Y_data = consumption_df['Consumption'].values.reshape(-1, 1)\n",
        "        \n",
        "        # Normaliser\n",
        "        scaler_X = StandardScaler()\n",
        "        scaler_Y = StandardScaler()\n",
        "        X_normalized = scaler_X.fit_transform(X_data)\n",
        "        Y_normalized = scaler_Y.fit_transform(Y_data)\n",
        "        \n",
        "        # Pour chaque seed, créer différentes séquences d'entraînement\n",
        "        for seed in range(n_seeds_per_client):\n",
        "            # Créer des séquences\n",
        "            X_sequences, Y_sequences = create_sequences(\n",
        "                X_normalized, Y_normalized, sequence_length, offset=seed\n",
        "            )\n",
        "            \n",
        "            if len(X_sequences) == 0:\n",
        "                print(f\"Pas assez de données pour créer des séquences avec la seed {seed}\")\n",
        "                continue\n",
        "                \n",
        "            # Reformater pour LSTM (n_x, batch_size, sequence_length)\n",
        "            X_train_lstm = np.transpose(X_sequences, (2, 0, 1))  # (n_x, batch_size, sequence_length)\n",
        "            Y_train_lstm = np.transpose(Y_sequences, (1, 0, 2))  # (n_y, batch_size, sequence_length)\n",
        "            \n",
        "            client_seeds_data.append((X_train_lstm, Y_train_lstm))\n",
        "        \n",
        "        if client_seeds_data:\n",
        "            clients_data.append(client_seeds_data)\n",
        "        else:\n",
        "            print(f\"Aucune séquence valide n'a pu être créée pour {country}\")\n",
        "    \n",
        "    if not clients_data:\n",
        "        raise ValueError(\"Aucune donnée n'a pu être chargée pour aucun client\")\n",
        "    \n",
        "    return clients_data\n",
        "\n",
        "def generate_consumption_from_weather(weather_df, country):\n",
        "    \"\"\"\n",
        "    Génère des données de consommation réalistes basées sur les données météorologiques.\n",
        "    \n",
        "    Arguments:\n",
        "    weather_df -- DataFrame pandas contenant les données météo\n",
        "    country -- nom du pays pour adapter les facteurs de consommation\n",
        "    \n",
        "    Returns:\n",
        "    consumption_df -- DataFrame pandas contenant une colonne 'Consumption'\n",
        "    \"\"\"\n",
        "    # Facteurs spécifiques par pays\n",
        "    country_factors = {\n",
        "        \"Switzerland\": {\"base\": 6000, \"temp_factor\": 1.0, \"day_factor\": 1.0},\n",
        "        \"Norway\": {\"base\": 8000, \"temp_factor\": 1.2, \"day_factor\": 0.9},\n",
        "        \"Estonia\": {\"base\": 3000, \"temp_factor\": 1.1, \"day_factor\": 0.95}\n",
        "    }\n",
        "    \n",
        "    factors = country_factors.get(country, {\"base\": 5000, \"temp_factor\": 1.0, \"day_factor\": 1.0})\n",
        "    \n",
        "    # Vérifier la disponibilité des colonnes nécessaires\n",
        "    datetime_col = None\n",
        "    for col in [\"datetime\", \"date\", \"time\"]:\n",
        "        if col in weather_df.columns:\n",
        "            datetime_col = col\n",
        "            break\n",
        "    \n",
        "    if not datetime_col:\n",
        "        # Créer une colonne datetime si absente\n",
        "        weather_df[\"datetime\"] = pd.date_range(start=\"2023-01-01\", periods=len(weather_df), freq=\"H\")\n",
        "        datetime_col = \"datetime\"\n",
        "    \n",
        "    # Convertir datetime en objet datetime si ce n'est pas déjà le cas\n",
        "    if not pd.api.types.is_datetime64_any_dtype(weather_df[datetime_col]):\n",
        "        try:\n",
        "            weather_df[datetime_col] = pd.to_datetime(weather_df[datetime_col])\n",
        "        except:\n",
        "            # Si conversion impossible, créer une nouvelle colonne\n",
        "            weather_df[\"datetime\"] = pd.date_range(start=\"2023-01-01\", periods=len(weather_df), freq=\"H\")\n",
        "            datetime_col = \"datetime\"\n",
        "    \n",
        "    # Extraire l'heure du jour et le jour de la semaine\n",
        "    hours = weather_df[datetime_col].dt.hour\n",
        "    weekdays = weather_df[datetime_col].dt.dayofweek\n",
        "    \n",
        "    # Identifier les colonnes pertinentes\n",
        "    temp_col = next((col for col in weather_df.columns if col in [\"temp\", \"temperature\"]), None)\n",
        "    humidity_col = next((col for col in weather_df.columns if \"humid\" in col.lower()), None)\n",
        "    wind_col = next((col for col in weather_df.columns if \"wind\" in col.lower() and \"dir\" not in col.lower()), None)\n",
        "    cloud_col = next((col for col in weather_df.columns if \"cloud\" in col.lower()), None)\n",
        "    \n",
        "    # Générer la consommation\n",
        "    consumption = np.zeros(len(weather_df))\n",
        "    \n",
        "    for i in range(len(weather_df)):\n",
        "        # Base de consommation\n",
        "        base = factors[\"base\"]\n",
        "        \n",
        "        # Effet de l'heure (pic matin et soir)\n",
        "        hour = hours[i]\n",
        "        hour_factor = 1.0 + 0.3 * np.sin((hour - 8) * np.pi / 12) * factors[\"day_factor\"]\n",
        "        \n",
        "        # Effet du jour de la semaine (baisse le weekend)\n",
        "        weekday = weekdays[i]\n",
        "        weekday_factor = 0.8 if weekday >= 5 else 1.0\n",
        "        \n",
        "        # Effet de la température\n",
        "        temp_effect = 1.0\n",
        "        if temp_col:\n",
        "            temp = weather_df[temp_col].iloc[i]\n",
        "            temp_effect = 1.0 + factors[\"temp_factor\"] * max(0, (15 - temp) / 20)\n",
        "        \n",
        "        # Effets supplémentaires si disponibles\n",
        "        additional_factor = 1.0\n",
        "        \n",
        "        if wind_col:\n",
        "            wind = weather_df[wind_col].iloc[i]\n",
        "            additional_factor *= 1.0 + wind / 100\n",
        "        \n",
        "        if humidity_col:\n",
        "            humidity = weather_df[humidity_col].iloc[i]\n",
        "            additional_factor *= 1.0 + humidity / 500\n",
        "        \n",
        "        if cloud_col:\n",
        "            cloud = weather_df[cloud_col].iloc[i]\n",
        "            additional_factor *= 1.0 + cloud / 300\n",
        "        \n",
        "        # Calculer la consommation finale avec bruit aléatoire\n",
        "        consumption[i] = base * hour_factor * weekday_factor * temp_effect * additional_factor\n",
        "        consumption[i] *= np.random.uniform(0.95, 1.05)  # Ajouter 5% de bruit aléatoire\n",
        "    \n",
        "    return pd.DataFrame({\"Consumption\": consumption})\n",
        "\n",
        "def select_weather_features(weather_df, n_x):\n",
        "    \"\"\"\n",
        "    Sélectionne les n_x meilleures features météorologiques pour l'entrée du modèle.\n",
        "    \n",
        "    Arguments:\n",
        "    weather_df -- DataFrame pandas avec les données météo\n",
        "    n_x -- nombre de features à sélectionner\n",
        "    \n",
        "    Returns:\n",
        "    selected_features -- liste des noms de colonnes sélectionnées\n",
        "    \"\"\"\n",
        "    # Liste des features par ordre de priorité\n",
        "    priority_features = [\n",
        "        \"temp\", \"feelslike\", \"humidity\", \"windspeed\", \"dew\",\n",
        "        \"precip\", \"cloudcover\", \"visibility\", \"solarradiation\", \n",
        "        \"solarenergy\", \"uvindex\", \"windgust\", \"sealevelpressure\",\n",
        "        \"snowdepth\", \"precipprob\"\n",
        "    ]\n",
        "    \n",
        "    # Colonnes à exclure\n",
        "    exclude_cols = [\"name\", \"datetime\", \"date\", \"time\", \"preciptype\", \n",
        "                    \"stations\", \"icon\", \"conditions\", \"severerisk\",\n",
        "                    \"winddir\", \"snow\", \"Consumption\"]\n",
        "    \n",
        "    # Trouver les features disponibles dans le DataFrame\n",
        "    available_features = []\n",
        "    \n",
        "    # D'abord essayer les features prioritaires\n",
        "    for feature in priority_features:\n",
        "        if feature in weather_df.columns and feature not in exclude_cols:\n",
        "            available_features.append(feature)\n",
        "    \n",
        "    # Si pas assez, ajouter d'autres colonnes numériques\n",
        "    if len(available_features) < n_x:\n",
        "        numeric_cols = weather_df.select_dtypes(include=np.number).columns\n",
        "        for col in numeric_cols:\n",
        "            if col not in available_features and col not in exclude_cols:\n",
        "                available_features.append(col)\n",
        "    \n",
        "    # Limiter au nombre demandé\n",
        "    selected_features = available_features[:n_x]\n",
        "    \n",
        "    # S'il n'y a toujours pas assez de features, lever une erreur\n",
        "    if len(selected_features) < n_x:\n",
        "        raise ValueError(f\"Pas assez de features disponibles: {len(selected_features)} < {n_x}\")\n",
        "    \n",
        "    return selected_features\n",
        "\n",
        "def create_test_dataset(clients_data, batch_size, n_x, n_y, sequence_length):\n",
        "    \"\"\"\n",
        "    Crée un jeu de données de test en utilisant une partie des données de chaque client.\n",
        "    \"\"\"\n",
        "    test_samples_X = []\n",
        "    test_samples_Y = []\n",
        "\n",
        "    for client_id, client_seeds_data in enumerate(clients_data):\n",
        "        # Vérifier la structure des données\n",
        "        print(f\"Structure des données pour le client {client_id}:\")\n",
        "        print(f\"Type: {type(client_seeds_data)}\")\n",
        "        \n",
        "        # Si c'est un tuple, c'est directement (X, Y)\n",
        "        if isinstance(client_seeds_data, tuple) and len(client_seeds_data) == 2:\n",
        "            X, Y = client_seeds_data\n",
        "            print(f\"Forme de X: {X.shape}, Forme de Y: {Y.shape}\")\n",
        "        # Si c'est une liste, c'est une liste de seeds, prendre la première\n",
        "        elif isinstance(client_seeds_data, list) and len(client_seeds_data) > 0:\n",
        "            if isinstance(client_seeds_data[0], tuple) and len(client_seeds_data[0]) == 2:\n",
        "                X, Y = client_seeds_data[0]\n",
        "                print(f\"Forme de X: {X.shape}, Forme de Y: {Y.shape}\")\n",
        "            else:\n",
        "                print(f\"Structure inattendue pour client_seeds_data[0]: {type(client_seeds_data[0])}\")\n",
        "                continue\n",
        "        else:\n",
        "            print(f\"Structure inattendue pour client_seeds_data: {type(client_seeds_data)}\")\n",
        "            continue\n",
        "        \n",
        "        # Nombre d'échantillons à utiliser pour le test\n",
        "        n_samples = X.shape[1]\n",
        "        n_test = min(batch_size // len(clients_data), n_samples // 3)  # Environ 1/3 des données\n",
        "        \n",
        "        # Sélectionner les échantillons de test\n",
        "        X_test = X[:, :n_test, :]\n",
        "        Y_test = Y[:, :n_test, :]\n",
        "        \n",
        "        test_samples_X.append(X_test)\n",
        "        test_samples_Y.append(Y_test)\n",
        "    \n",
        "    if not test_samples_X:\n",
        "        raise ValueError(\"Aucun échantillon de test n'a pu être extrait.\")\n",
        "    \n",
        "    # Concaténer les échantillons\n",
        "    X_test = np.concatenate(test_samples_X, axis=1)\n",
        "    Y_test = np.concatenate(test_samples_Y, axis=1)\n",
        "    \n",
        "    # S'assurer que la taille du batch est correcte\n",
        "    if X_test.shape[1] > batch_size:\n",
        "        X_test = X_test[:, :batch_size, :]\n",
        "        Y_test = Y_test[:, :batch_size, :]\n",
        "    \n",
        "    print(f\"Forme finale de X_test: {X_test.shape}, Y_test: {Y_test.shape}\")\n",
        "    return X_test, Y_test\n",
        "\n",
        "def create_sequences(X, Y, sequence_length, offset=0):\n",
        "    \"\"\"\n",
        "    Crée des séquences temporelles à partir des données X et Y.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- données d'entrée, numpy array de forme (n_features, n_samples)\n",
        "    Y -- données cibles, numpy array de forme (n_targets, n_samples)\n",
        "    sequence_length -- longueur de chaque séquence\n",
        "    offset -- décalage pour commencer les séquences (défaut: 0)\n",
        "    \n",
        "    Returns:\n",
        "    X_sequences -- séquences d'entrée, numpy array de forme (n_samples - sequence_length - offset + 1, sequence_length, n_features)\n",
        "    Y_sequences -- séquences cibles, numpy array de forme (n_samples - sequence_length - offset + 1, n_targets, sequence_length)\n",
        "    \"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    \n",
        "    # Vérifier s'il y a suffisamment d'échantillons pour créer au moins une séquence\n",
        "    if n_samples < sequence_length + offset:\n",
        "        return np.array([]), np.array([])\n",
        "    \n",
        "    # Nombre de séquences qui seront créées\n",
        "    n_sequences = n_samples - sequence_length - offset + 1\n",
        "    \n",
        "    # Initialiser les tableaux de sortie\n",
        "    X_sequences = []\n",
        "    Y_sequences = []\n",
        "    \n",
        "    # Créer les séquences\n",
        "    for i in range(n_sequences):\n",
        "        start_idx = i + offset\n",
        "        end_idx = start_idx + sequence_length\n",
        "        \n",
        "        # Extraire les séquences X et Y\n",
        "        X_seq = X[start_idx:end_idx, :]\n",
        "        Y_seq = Y[start_idx:end_idx, :]\n",
        "        \n",
        "        X_sequences.append(X_seq)\n",
        "        Y_sequences.append(Y_seq)\n",
        "    \n",
        "    # Convertir en tableaux numpy\n",
        "    X_sequences = np.array(X_sequences)\n",
        "    Y_sequences = np.array(Y_sequences)\n",
        "    \n",
        "    return X_sequences, Y_sequences\n",
        "    \"\"\"\n",
        "    Crée un jeu de données de test en utilisant une partie des données de chaque client.\n",
        "    \"\"\"\n",
        "    test_samples_X = []\n",
        "    test_samples_Y = []\n",
        "\n",
        "    for client_id, client_seeds_data in enumerate(clients_data):\n",
        "        # Prendre la première seed\n",
        "        X, Y = client_seeds_data[0]\n",
        "        \n",
        "        # Nombre d'échantillons à utiliser pour le test\n",
        "        n_samples = X.shape[1]\n",
        "        n_test = min(batch_size // len(clients_data), n_samples // 3)  # Environ 1/3 des données\n",
        "        \n",
        "        # Sélectionner les échantillons de test\n",
        "        X_test = X[:, :n_test, :]\n",
        "        Y_test = Y[:, :n_test, :]\n",
        "        \n",
        "        test_samples_X.append(X_test)\n",
        "        test_samples_Y.append(Y_test)\n",
        "    \n",
        "    # Concaténer les échantillons\n",
        "    X_test = np.concatenate(test_samples_X, axis=1)\n",
        "    Y_test = np.concatenate(test_samples_Y, axis=1)\n",
        "    \n",
        "    # S'assurer que la taille du batch est correcte\n",
        "    if X_test.shape[1] > batch_size:\n",
        "        X_test = X_test[:, :batch_size, :]\n",
        "        Y_test = Y_test[:, :batch_size, :]\n",
        "    \n",
        "    return X_test, Y_test\n",
        "    \"\"\"\n",
        "    Crée un jeu de données de test à partir des mêmes fichiers.\n",
        "    Utilise les 30% réservés pour le test.\n",
        "    \n",
        "    Arguments:\n",
        "    clients_data -- liste des données clients (non utilisée directement)\n",
        "    batch_size -- taille du batch pour le test\n",
        "    n_x -- dimension d'entrée\n",
        "    n_y -- dimension de sortie\n",
        "    sequence_length -- longueur de la séquence temporelle\n",
        "    \n",
        "    Returns:\n",
        "    X_test -- données de test (n_x, batch_size, sequence_length)\n",
        "    Y_test -- étiquettes de test (n_y, batch_size, sequence_length)\n",
        "    \"\"\"\n",
        "    countries = [\"Switzerland\", \"Norway\", \"Estonia\"]\n",
        "    test_samples_X = []\n",
        "    test_samples_Y = []\n",
        "    \n",
        "    # Récupérer les pays disponibles\n",
        "    available_countries = []\n",
        "    for country in countries:\n",
        "        file_prefix = \"weather_switerland_month\" if country == \"Switzerland\" else f\"weather_{country.lower()}_month\"\n",
        "        country_files = [f for f in os.listdir() if f.startswith(file_prefix)]\n",
        "        if country_files:\n",
        "            available_countries.append(country)\n",
        "    \n",
        "    if not available_countries:\n",
        "        raise ValueError(\"Aucun fichier de données météo trouvé pour les pays spécifiés.\")\n",
        "    \n",
        "    for country in available_countries:\n",
        "        # Charger les données météo pour ce pays\n",
        "        weather_data = []\n",
        "        file_prefix = \"weather_switerland_month\" if country == \"Switzerland\" else f\"weather_{country.lower()}_month\"\n",
        "        \n",
        "        for month in range(1, 13):\n",
        "            file_name = f\"{file_prefix}{month}.csv\"\n",
        "            \n",
        "            try:\n",
        "                if os.path.exists(file_name):\n",
        "                    df_month = pd.read_csv(file_name)\n",
        "                    weather_data.append(df_month)\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur lors du chargement de {file_name}: {e}\")\n",
        "        \n",
        "        if not weather_data:\n",
        "            continue\n",
        "        \n",
        "        # Concaténer les données météo\n",
        "        weather_df = pd.concat(weather_data, ignore_index=True)\n",
        "        \n",
        "        # Générer des données de consommation\n",
        "        consumption_df = generate_consumption_from_weather(weather_df, country)\n",
        "        \n",
        "        # Sélectionner les features\n",
        "        features = select_weather_features(weather_df, n_x)\n",
        "        \n",
        "        # Préparer les données X et Y\n",
        "        X_data = weather_df[features].values\n",
        "        Y_data = consumption_df.values\n",
        "        \n",
        "        # Normaliser\n",
        "        scaler_X = StandardScaler()\n",
        "        scaler_Y = StandardScaler()\n",
        "        X_normalized = scaler_X.fit_transform(X_data)\n",
        "        Y_normalized = scaler_Y.fit_transform(Y_data)\n",
        "        \n",
        "        # Diviser - ne prendre que les 30% de test\n",
        "        _, X_test, _, Y_test = train_test_split(\n",
        "            X_normalized, Y_normalized, test_size=0.3, random_state=42\n",
        "        )\n",
        "        \n",
        "        # Créer des séquences\n",
        "        X_sequences, Y_sequences = create_sequences(X_test, Y_test, sequence_length)\n",
        "        \n",
        "        if len(X_sequences) == 0:\n",
        "            continue\n",
        "            \n",
        "        # Reformater pour LSTM\n",
        "        X_test_lstm = np.transpose(X_sequences, (2, 0, 1))\n",
        "        Y_test_lstm = np.transpose(Y_sequences, (1, 0, 2))\n",
        "        \n",
        "        # Limiter le nombre d'échantillons pour ce pays\n",
        "        max_samples = batch_size // len(available_countries)\n",
        "        if X_test_lstm.shape[1] > max_samples:\n",
        "            X_test_lstm = X_test_lstm[:, :max_samples, :]\n",
        "            Y_test_lstm = Y_test_lstm[:, :max_samples, :]\n",
        "        \n",
        "        test_samples_X.append(X_test_lstm)\n",
        "        test_samples_Y.append(Y_test_lstm)\n",
        "    \n",
        "    if not test_samples_X:\n",
        "        raise ValueError(\"Aucune donnée de test n'a pu être préparée\")\n",
        "    \n",
        "    # Concaténer les échantillons\n",
        "    X_test = np.concatenate(test_samples_X, axis=1)\n",
        "    Y_test = np.concatenate(test_samples_Y, axis=1)\n",
        "    \n",
        "    # Ajuster la taille du batch si nécessaire\n",
        "    if X_test.shape[1] > batch_size:\n",
        "        X_test = X_test[:, :batch_size, :]\n",
        "        Y_test = Y_test[:, :batch_size, :]\n",
        "    \n",
        "    return X_test, Y_test\n",
        "\n",
        "    \"\"\"\n",
        "    Crée un jeu de données de test à partir d'une fraction des données de chaque client.\n",
        "\n",
        "    Arguments:\n",
        "    clients_data -- données de tous les clients\n",
        "    batch_size -- taille du batch pour le test\n",
        "    n_x -- dimension d'entrée\n",
        "    n_y -- dimension de sortie\n",
        "    sequence_length -- longueur de la séquence temporelle\n",
        "\n",
        "    Returns:\n",
        "    X_test -- données de test, array de forme (n_x, batch_size, sequence_length)\n",
        "    Y_test -- étiquettes de test, array de forme (n_y, batch_size, sequence_length)\n",
        "    \"\"\"\n",
        "    # Extraire un échantillon de chaque client pour le test\n",
        "    test_samples_X = []\n",
        "    test_samples_Y = []\n",
        "\n",
        "    for client_data in clients_data:\n",
        "        # Prendre la première seed de chaque client\n",
        "        X, Y = client_data[0]\n",
        "\n",
        "        # Prendre 20% des échantillons pour le test\n",
        "        n_samples = X.shape[1]\n",
        "        n_test = min(batch_size // len(clients_data), n_samples // 5)\n",
        "\n",
        "        test_samples_X.append(X[:, :n_test, :])\n",
        "        test_samples_Y.append(Y[:, :n_test, :])\n",
        "\n",
        "    # Concaténer les échantillons de test\n",
        "    X_test = np.concatenate(test_samples_X, axis=1)\n",
        "    Y_test = np.concatenate(test_samples_Y, axis=1)\n",
        "\n",
        "    # S'assurer que la taille du batch est correcte\n",
        "    if X_test.shape[1] > batch_size:\n",
        "        X_test = X_test[:, :batch_size, :]\n",
        "        Y_test = Y_test[:, :batch_size, :]\n",
        "\n",
        "    return X_test, Y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fonctions d'évaluation ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_lstm(X_test, Y_test, parameters):\n",
        "    \"\"\"\n",
        "    Évalue les performances d'un LSTM pour la régression.\n",
        "\n",
        "    Arguments:\n",
        "    X_test -- données de test, numpy array de forme (n_x, m, T_x)\n",
        "    Y_test -- étiquettes de test, numpy array de forme (n_y, m, T_x)\n",
        "    parameters -- dictionnaire Python contenant les paramètres du LSTM\n",
        "\n",
        "    Returns:\n",
        "    rmse_val -- erreur quadratique moyenne\n",
        "    loss -- perte du modèle\n",
        "    \"\"\"\n",
        "    n_a = parameters[\"Wf\"].shape[0]\n",
        "\n",
        "    # Forward pass\n",
        "    a0 = np.zeros((n_a, X_test.shape[1]))\n",
        "    _, y_pred, _, _ = lstm_forward(X_test, a0, parameters)\n",
        "\n",
        "    # Calculer RMSE\n",
        "    rmse_val = np.sqrt(np.mean((y_pred - Y_test) ** 2))\n",
        "\n",
        "    # Calcul de la perte (MSE)\n",
        "    loss = np.mean((y_pred - Y_test) ** 2)\n",
        "\n",
        "    return rmse_val, loss\n",
        "\n",
        "def rmse(predictions, targets):\n",
        "    \"\"\"\n",
        "    Calcule la racine de l'erreur quadratique moyenne (RMSE)\n",
        "\n",
        "    Arguments:\n",
        "    predictions -- prédictions du modèle, array numpy\n",
        "    targets -- valeurs cibles, array numpy\n",
        "\n",
        "    Returns:\n",
        "    rmse_value -- valeur RMSE (float)\n",
        "    \"\"\"\n",
        "    return np.sqrt(np.mean((predictions - targets) ** 2))\n",
        "\n",
        "def analyze_computational_complexity_local(n_clients, n_seeds_per_client, n_a, n_x, n_y, n_epochs,\n",
        "                                          n_clusters, sequence_length, batch_size):\n",
        "    \"\"\"\n",
        "    Analyse la complexité computationnelle du processus d'apprentissage fédéré avec clusterisation locale.\n",
        "    \"\"\"\n",
        "    complexity_dict = {}\n",
        "\n",
        "    # Dimension des paramètres LSTM\n",
        "    d = n_a**2 + n_a*n_x + n_a*n_y + n_a + n_y*n_a + n_y  # Dimension totale des paramètres\n",
        "\n",
        "    # 1. Complexité de l'entraînement local sur un client pour chaque seed (par époque)\n",
        "    forward_complexity = batch_size * sequence_length * (n_a**2 + n_a*n_x + n_a*n_y)\n",
        "    backward_complexity = batch_size * sequence_length * (n_a**2 + n_a*n_x + n_a*n_y)\n",
        "    client_seed_training_complexity = n_epochs * (forward_complexity + backward_complexity)\n",
        "    client_training_complexity = n_seeds_per_client * client_seed_training_complexity\n",
        "\n",
        "    complexity_dict[\"Local Training (per client)\"] = client_training_complexity\n",
        "    complexity_dict[\"Local Training (per seed)\"] = client_seed_training_complexity\n",
        "    complexity_dict[\"Local Training (all clients)\"] = n_clients * client_training_complexity\n",
        "\n",
        "    # 2. Complexité de la clusterisation locale\n",
        "    kmeans_iterations = 100  # Hypothèse pour le nombre d'itérations K-means\n",
        "    n_samples = n_seeds_per_client * n_epochs  # Nombre d'échantillons à clusteriser\n",
        "\n",
        "    # Complexité de K-means pour un client\n",
        "    clustering_complexity_per_client = kmeans_iterations * n_samples * n_clusters * d\n",
        "    clustering_complexity_all_clients = n_clients * clustering_complexity_per_client\n",
        "\n",
        "    complexity_dict[\"Local Clustering (per client)\"] = clustering_complexity_per_client\n",
        "    complexity_dict[\"Local Clustering (all clients)\"] = clustering_complexity_all_clients\n",
        "\n",
        "    # 3. Complexité du calcul des matrices de transition locales\n",
        "    transition_matrix_complexity_per_client = n_seeds_per_client * n_epochs * n_clusters**2\n",
        "    transition_matrix_complexity_all_clients = n_clients * transition_matrix_complexity_per_client\n",
        "\n",
        "    complexity_dict[\"Transition Matrix Computation (per client)\"] = transition_matrix_complexity_per_client\n",
        "    complexity_dict[\"Transition Matrix Computation (all clients)\"] = transition_matrix_complexity_all_clients\n",
        "\n",
        "    # 4. Complexité de la communication client-serveur\n",
        "    cluster_centers_size = n_clusters * d  # Taille des centres de clusters\n",
        "    transition_matrix_size = n_clusters * n_clusters  # Taille de la matrice de transition\n",
        "\n",
        "    communication_complexity_per_client = cluster_centers_size + transition_matrix_size\n",
        "    communication_complexity_all_clients = n_clients * communication_complexity_per_client\n",
        "\n",
        "    traditional_communication_per_client = d\n",
        "    traditional_communication_all_clients = n_clients * traditional_communication_per_client\n",
        "\n",
        "    complexity_dict[\"Communication (proposed, per client)\"] = communication_complexity_per_client\n",
        "    complexity_dict[\"Communication (proposed, all clients)\"] = communication_complexity_all_clients\n",
        "    complexity_dict[\"Communication (traditional, per client)\"] = traditional_communication_per_client\n",
        "    complexity_dict[\"Communication (traditional, all clients)\"] = traditional_communication_all_clients\n",
        "    complexity_dict[\"Communication Reduction Ratio\"] = traditional_communication_all_clients / communication_complexity_all_clients\n",
        "\n",
        "    # 5. Complexité de l'agrégation au serveur\n",
        "    distribution_stationary_complexity = n_clients * n_clusters * n_clusters * 100\n",
        "    aggregation_complexity_proposed = distribution_stationary_complexity + n_clients * d\n",
        "\n",
        "    aggregation_complexity_traditional = n_clients * d\n",
        "\n",
        "    complexity_dict[\"Server Aggregation (proposed)\"] = aggregation_complexity_proposed\n",
        "    complexity_dict[\"Server Aggregation (traditional)\"] = aggregation_complexity_traditional\n",
        "\n",
        "    # 6. Complexité du transfer learning\n",
        "    transfer_epochs = 5  # Hypothèse\n",
        "    transfer_complexity = transfer_epochs * (forward_complexity + backward_complexity)\n",
        "\n",
        "    complexity_dict[\"Transfer Learning\"] = transfer_complexity\n",
        "\n",
        "    # 7. Complexité totale des approches\n",
        "    proposed_approach_complexity = (\n",
        "        n_clients * client_training_complexity +  # Entraînement local\n",
        "        clustering_complexity_all_clients +       # Clusterisation locale\n",
        "        transition_matrix_complexity_all_clients + # Calcul des matrices de transition\n",
        "        communication_complexity_all_clients +    # Communication client-serveur\n",
        "        aggregation_complexity_proposed +         # Agrégation au serveur\n",
        "        transfer_complexity                       # Transfer learning\n",
        "    )\n",
        "\n",
        "    traditional_approach_complexity = (\n",
        "        n_clients * client_training_complexity +  # Entraînement local (même coût)\n",
        "        traditional_communication_all_clients +   # Communication client-serveur\n",
        "        aggregation_complexity_traditional +      # Agrégation au serveur\n",
        "        transfer_complexity                       # Transfer learning (même coût)\n",
        "    )\n",
        "\n",
        "    complexity_dict[\"Total (Proposed Approach)\"] = proposed_approach_complexity\n",
        "    complexity_dict[\"Total (Traditional Approach)\"] = traditional_approach_complexity\n",
        "\n",
        "    efficiency_ratio = traditional_approach_complexity / proposed_approach_complexity\n",
        "    complexity_dict[\"Efficiency Ratio\"] = efficiency_ratio\n",
        "\n",
        "    # 8. Analyse par composante (pourcentage du temps total)\n",
        "    total_proposed = proposed_approach_complexity\n",
        "\n",
        "    complexity_dict[\"Local Training (% of total)\"] = n_clients * client_training_complexity / total_proposed * 100\n",
        "    complexity_dict[\"Local Clustering (% of total)\"] = clustering_complexity_all_clients / total_proposed * 100\n",
        "    complexity_dict[\"Transition Matrix (% of total)\"] = transition_matrix_complexity_all_clients / total_proposed * 100\n",
        "    complexity_dict[\"Communication (% of total)\"] = communication_complexity_all_clients / total_proposed * 100\n",
        "    complexity_dict[\"Server Aggregation (% of total)\"] = aggregation_complexity_proposed / total_proposed * 100\n",
        "    complexity_dict[\"Transfer Learning (% of total)\"] = transfer_complexity / total_proposed * 100\n",
        "\n",
        "    return complexity_dict\n",
        "\n",
        "def transfer_learning(X_train, Y_train, X_test, Y_test, source_parameters, n_a, n_x, n_y, num_epochs=5, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Effectue un transfert d'apprentissage à partir des paramètres source.\n",
        "    \"\"\"\n",
        "    # Utiliser les paramètres source comme initialisation\n",
        "    parameters = copy.deepcopy(source_parameters)\n",
        "\n",
        "    # Initialiser Adam\n",
        "    v, s = initialize_adam_for_lstm(parameters)\n",
        "    t = 0  # Compteur pour Adam\n",
        "\n",
        "    loss_history = []\n",
        "    accuracy_history = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Époque de transfert {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Forward pass\n",
        "        a0 = np.zeros((n_a, X_train.shape[1]))\n",
        "        a, y_pred, c, caches = lstm_forward(X_train, a0, parameters)\n",
        "\n",
        "        # Calcul de la perte (cross-entropy)\n",
        "        loss = -np.sum(Y_train * np.log(y_pred + 1e-8)) / (Y_train.shape[1] * Y_train.shape[2])\n",
        "\n",
        "        # Initialisation du gradient de sortie\n",
        "        da = np.zeros_like(a)\n",
        "\n",
        "        # Calculer les gradients pour Wy et by\n",
        "        dWy = np.zeros_like(parameters[\"Wy\"])\n",
        "        dby = np.zeros_like(parameters[\"by\"])\n",
        "\n",
        "        # Pour chaque pas de temps, calculer le gradient\n",
        "        for t_idx in range(Y_train.shape[2]):\n",
        "            # Gradient de la cross-entropy\n",
        "            dy = y_pred[:, :, t_idx] - Y_train[:, :, t_idx]\n",
        "            # Accumuler les gradients pour Wy et by\n",
        "            dWy += np.dot(dy, a[:, :, t_idx].T)\n",
        "            dby += np.sum(dy, axis=1, keepdims=True)\n",
        "            # Gradient par rapport à a\n",
        "            da[:, :, t_idx] = np.dot(parameters[\"Wy\"].T, dy)\n",
        "\n",
        "        # Backward pass pour les autres paramètres\n",
        "        lstm_gradients = lstm_backward(da, caches)\n",
        "\n",
        "        # Combiner tous les gradients\n",
        "        gradients = lstm_gradients.copy()\n",
        "        gradients[\"dWy\"] = dWy\n",
        "        gradients[\"dby\"] = dby\n",
        "\n",
        "        # Mise à jour des paramètres avec Adam\n",
        "        t += 1\n",
        "        parameters, v, s = update_parameters_with_adam_for_lstm(parameters, gradients, v, s, t, learning_rate)\n",
        "\n",
        "        # Évaluer sur l'ensemble de test\n",
        "        accuracy, test_loss = evaluate_lstm(X_test, Y_test, parameters)\n",
        "        loss_history.append(test_loss)\n",
        "        accuracy_history.append(accuracy)\n",
        "\n",
        "        print(f\"Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    return parameters, loss_history, accuracy_history\n",
        "\n",
        "def simulate_full_parameter_transmission(parameters_history_by_client, X_test, Y_test, n_a, n_x, n_y, num_transfer_epochs, learning_rate):\n",
        "    \"\"\"\n",
        "    Simule la méthode traditionnelle de transmission complète des paramètres.\n",
        "\n",
        "    Arguments:\n",
        "    parameters_history_by_client -- historique des paramètres pour chaque client\n",
        "    X_test -- données de test\n",
        "    Y_test -- étiquettes de test\n",
        "    n_a -- nombre d'unités cachées\n",
        "    n_x -- dimension d'entrée\n",
        "    n_y -- dimension de sortie\n",
        "    num_transfer_epochs -- nombre d'époques pour le transfer learning\n",
        "    learning_rate -- taux d'apprentissage\n",
        "\n",
        "    Returns:\n",
        "    full_params_avg -- paramètres moyennés sans transfer\n",
        "    full_params_transfer -- paramètres après transfer\n",
        "    full_params_acc -- précision après transfer\n",
        "    full_params_loss -- perte après transfer\n",
        "    full_params_size -- taille de transmission (en octets)\n",
        "    \"\"\"\n",
        "    # Extraire les derniers paramètres pour chaque client\n",
        "    last_params = [client_history[-1] for client_history in parameters_history_by_client]\n",
        "\n",
        "    # Calculer la moyenne des paramètres (FedAvg)\n",
        "    full_params_avg = {}\n",
        "    for key in last_params[0].keys():\n",
        "        full_params_avg[key] = np.mean([params[key] for params in last_params], axis=0)\n",
        "\n",
        "    # Calculer la taille de transmission\n",
        "    full_params_size = calculate_transmission_size(full_params_avg)\n",
        "\n",
        "    # Réentraîner avec transfer learning\n",
        "    full_params_transfer, loss_history, acc_history = transfer_learning(\n",
        "        X_train=X_test, Y_train=Y_test,\n",
        "        X_test=X_test, Y_test=Y_test,\n",
        "        source_parameters=full_params_avg,\n",
        "        n_a=n_a, n_x=n_x, n_y=n_y,\n",
        "        num_epochs=num_transfer_epochs,\n",
        "        learning_rate=learning_rate\n",
        "    )\n",
        "\n",
        "    # Récupérer les performances finales\n",
        "    full_params_acc = acc_history[-1]\n",
        "    full_params_loss = loss_history[-1]\n",
        "\n",
        "    return full_params_avg, full_params_transfer, full_params_acc, full_params_loss, full_params_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualisation ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_results(original_acc, compressed_acc, transfer_acc, fedavg_acc=None,\n",
        "                                   original_loss=None, compressed_loss=None, transfer_loss=None, fedavg_loss=None,\n",
        "                                   title_suffix=\"\"):\n",
        "    \"\"\"\n",
        "    Visualise les résultats de l'expérience avec comparaison des méthodes.\n",
        "    \n",
        "    Arguments:\n",
        "    original_acc -- précision originale\n",
        "    compressed_acc -- précision avec compression\n",
        "    transfer_acc -- précision après transfer learning\n",
        "    fedavg_acc -- précision avec FedAvg (optionnel)\n",
        "    original_loss, compressed_loss, transfer_loss, fedavg_loss -- pertes correspondantes (optionnel)\n",
        "    title_suffix -- suffixe pour le titre (optionnel)\n",
        "    \"\"\"\n",
        "    # Déterminer combien de métriques visualiser (précision et/ou perte)\n",
        "    show_loss = all(x is not None for x in [original_loss, compressed_loss, transfer_loss])\n",
        "    n_plots = 1 + int(show_loss)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, n_plots, figsize=(15 * n_plots // 2, 5))\n",
        "    \n",
        "    # Convertir axes en liste si un seul graphique\n",
        "    if n_plots == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    # Graphique des précisions\n",
        "    if fedavg_acc is not None:\n",
        "        labels = ['Original', 'Compressé', 'Transfert', 'FedAvg']\n",
        "        accuracies = [original_acc, compressed_acc, transfer_acc, fedavg_acc]\n",
        "        colors = ['blue', 'orange', 'green', 'red']\n",
        "    else:\n",
        "        labels = ['Original', 'Compressé', 'Transfert']\n",
        "        accuracies = [original_acc, compressed_acc, transfer_acc]\n",
        "        colors = ['blue', 'orange', 'green']\n",
        "        \n",
        "    axes[0].bar(labels, accuracies, color=colors)\n",
        "    axes[0].set_ylabel('Précision')\n",
        "    axes[0].set_title(f'Comparaison des précisions{title_suffix}')\n",
        "    \n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for i, v in enumerate(accuracies):\n",
        "        axes[0].text(i, v + 0.01, f\"{v:.3f}\", ha='center')\n",
        "\n",
        "    # Graphique des pertes si disponible\n",
        "    if show_loss:\n",
        "        if fedavg_loss is not None:\n",
        "            losses = [original_loss, compressed_loss, transfer_loss, fedavg_loss]\n",
        "        else:\n",
        "            losses = [original_loss, compressed_loss, transfer_loss]\n",
        "            \n",
        "        axes[1].bar(labels, losses, color=colors)\n",
        "        axes[1].set_ylabel('Perte')\n",
        "        axes[1].set_title(f'Comparaison des pertes{title_suffix}')\n",
        "        \n",
        "        # Ajouter les valeurs sur les barres\n",
        "        for i, v in enumerate(losses):\n",
        "            axes[1].text(i, v + 0.01, f\"{v:.3f}\", ha='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'resultats_compression{title_suffix.replace(\" \", \"_\").lower()}.png')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_transition_matrices_local(client_clusters):\n",
        "    \"\"\"\n",
        "    Visualise les matrices de transition de chaque client.\n",
        "    \"\"\"\n",
        "    n_clients = len(client_clusters)\n",
        "    \n",
        "    # Vérifier s'il y a des clients à visualiser\n",
        "    if n_clients == 0:\n",
        "        print(\"Aucune matrice de transition à visualiser (n_clients = 0).\")\n",
        "        return\n",
        "    \n",
        "    # Déterminer la disposition optimale des sous-figures\n",
        "    n_cols = min(3, n_clients)\n",
        "    n_rows = (n_clients + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
        "    \n",
        "    # Gérer le cas d'un seul client (axes n'est pas un tableau)\n",
        "    if n_clients == 1:\n",
        "        axes = np.array([axes])\n",
        "    \n",
        "    # Aplatir le tableau d'axes pour itération facile\n",
        "    if n_rows * n_cols > 1:\n",
        "        axes = axes.flatten()\n",
        "    \n",
        "    for i, client_data in enumerate(client_clusters):\n",
        "        if i < len(axes):\n",
        "            # Extraire la matrice de transition (adapté selon votre structure de données)\n",
        "            if isinstance(client_data, tuple) and len(client_data) == 2:\n",
        "                # Si client_data est un tuple (cluster_centers, transition_matrix)\n",
        "                transition_matrix = client_data[1]\n",
        "            else:\n",
        "                # Si client_data est directement la matrice de transition\n",
        "                transition_matrix = client_data\n",
        "            \n",
        "            ax = axes[i]\n",
        "            im = ax.imshow(transition_matrix, cmap='viridis', interpolation='none')\n",
        "            ax.set_title(f'Client {i+1}')\n",
        "            ax.set_xlabel('Cluster de destination')\n",
        "            ax.set_ylabel('Cluster de départ')\n",
        "            \n",
        "            # Ajouter les valeurs sur la figure\n",
        "            for row in range(transition_matrix.shape[0]):\n",
        "                for col in range(transition_matrix.shape[1]):\n",
        "                    ax.text(col, row, f'{transition_matrix[row, col]:.2f}',\n",
        "                             ha='center', va='center',\n",
        "                             color='white' if transition_matrix[row, col] > 0.5 else 'black')\n",
        "    \n",
        "    # Masquer les axes supplémentaires s'il y en a\n",
        "    for i in range(n_clients, len(axes)):\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    # Ajouter une barre de couleur commune\n",
        "    fig.colorbar(im, ax=axes.tolist(), label='Probabilité de transition')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('matrices_transition_locales.png')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_computational_complexity_local(complexity_dict):\n",
        "    \"\"\"\n",
        "    Visualise la complexité computationnelle pour l'approche de clusterisation locale.\n",
        "    \"\"\"\n",
        "    # Extraction des composantes principales pour la visualisation\n",
        "    components = [\"Local Training (per client)\", \"Local Clustering (per client)\",\n",
        "                  \"Transition Matrix Computation (per client)\",\n",
        "                  \"Server Aggregation (proposed)\", \"Transfer Learning\"]\n",
        "\n",
        "    # Vérifier que toutes les clés existent\n",
        "    for comp in components:\n",
        "        if comp not in complexity_dict:\n",
        "            print(f\"Attention: Clé '{comp}' non trouvée dans complexity_dict\")\n",
        "            # Remplacer par une valeur par défaut pour éviter l'erreur\n",
        "            complexity_dict[comp] = 0\n",
        "\n",
        "    values = [complexity_dict[comp] for comp in components]\n",
        "\n",
        "    # Normalisation pour une meilleure visualisation\n",
        "    total = sum(values)\n",
        "    if total > 0:\n",
        "        normalized_values = np.array(values) / total * 100\n",
        "    else:\n",
        "        normalized_values = np.zeros_like(values)\n",
        "\n",
        "    # Création du graphique\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    bars = plt.bar(components, normalized_values, color=['blue', 'green', 'orange', 'red', 'purple'])\n",
        "\n",
        "    # Ajout des valeurs en pourcentage\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "                 f'{height:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "    plt.title('Décomposition de la complexité computationnelle (Clusterisation Locale)')\n",
        "    plt.ylabel('Pourcentage du temps de calcul total')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('decomposition_complexite_locale.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Comparaison des approches\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    ratio = complexity_dict.get(\"Efficiency Ratio\", 1.0)  # Valeur par défaut de 1.0\n",
        "    plt.bar(['Approche traditionnelle (FedAvg)', 'Approche proposée (Locale)'],\n",
        "            [100, 100/ratio if ratio > 0 else 0],\n",
        "            color=['gray', 'green'])\n",
        "    plt.title(f'Comparaison de l\\'efficacité (Traditionnel / Proposé = {ratio:.2f}x)')\n",
        "    plt.ylabel('Complexité relative (%)')\n",
        "    plt.ylim(0, max(100, 100/ratio if ratio > 0 else 0) * 1.1)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('comparaison_complexite_locale.png')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_bandwidth_savings(original_size, compressed_size):\n",
        "    \"\"\"\n",
        "    Visualise l'économie de bande passante.\n",
        "    \"\"\"\n",
        "    # Vérifier que original_size n'est pas zéro pour éviter division par zéro\n",
        "    if original_size <= 0:\n",
        "        print(\"AVERTISSEMENT: La taille originale est nulle ou négative, impossible de calculer le pourcentage d'économie.\")\n",
        "        saved_percentage = 0\n",
        "    else:\n",
        "        saved_percentage = (1 - compressed_size / original_size) * 100\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Barres de taille\n",
        "    bars = plt.bar(['Paramètres originaux', 'Paramètres compressés'],\n",
        "                   [original_size/1024 if original_size > 0 else 0, compressed_size/1024],\n",
        "                   color=['blue', 'green'])\n",
        "\n",
        "    # Ajouter valeurs numériques\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                 f'{height:.2f} KB', ha='center', va='bottom')\n",
        "\n",
        "    if original_size > 0:\n",
        "        plt.title(f'Économie de bande passante: {saved_percentage:.2f}%')\n",
        "    else:\n",
        "        plt.title('Économie de bande passante: Non calculable (taille originale nulle)')\n",
        "    plt.ylabel('Taille (KB)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('economies_bande_passante.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Diagramme circulaire pour visualiser les proportions - uniquement si les valeurs sont valides\n",
        "    if original_size > 0 and compressed_size < original_size:  # Cas normal: compression réduit la taille\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.pie([compressed_size, original_size - compressed_size],\n",
        "                labels=['Utilisé', 'Économisé'],\n",
        "                colors=['green', 'lightgray'],\n",
        "                autopct='%1.1f%%',\n",
        "                startangle=90,\n",
        "                explode=(0, 0.1))\n",
        "        plt.axis('equal')\n",
        "        plt.title(f'Économie de bande passante: {saved_percentage:.2f}%')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('pourcentage_economie.png')\n",
        "        plt.show()\n",
        "    elif original_size > 0 and compressed_size > original_size:  # Cas où la compression augmente la taille\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.pie([original_size, compressed_size - original_size],\n",
        "                labels=['Taille originale', 'Surcoût de compression'],\n",
        "                colors=['blue', 'red'],\n",
        "                autopct='%1.1f%%',\n",
        "                startangle=90,\n",
        "                explode=(0, 0.1))\n",
        "        plt.axis('equal')\n",
        "        plt.title(f'Augmentation de la taille: {-saved_percentage:.2f}%')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('pourcentage_surcout.png')\n",
        "        plt.show()\n",
        "    # Pas de diagramme si original_size est nul ou négatif\n",
        "\n",
        "def visualize_communication_rounds_comparison(communication_results):\n",
        "    \"\"\"\n",
        "    Visualise la comparaison des deux méthodes pour chaque cycle de communication.\n",
        "\n",
        "    Arguments:\n",
        "    communication_results -- liste de dictionnaires contenant les résultats pour chaque cycle\n",
        "    \"\"\"\n",
        "    n_rounds = len(communication_results)\n",
        "    cycles = [result[\"cycle\"] + 1 for result in communication_results]  # +1 pour commencer à 1\n",
        "\n",
        "    # Extraction des données\n",
        "    proposed_acc = [result[\"proposed_accuracy\"] for result in communication_results]\n",
        "    proposed_loss = [result[\"proposed_loss\"] for result in communication_results]\n",
        "    fedavg_acc = [result[\"fedavg_accuracy\"] for result in communication_results]\n",
        "    fedavg_loss = [result[\"fedavg_loss\"] for result in communication_results]\n",
        "    traditional_bw = [result[\"traditional_bandwidth\"]/1024 for result in communication_results]  # KB\n",
        "    proposed_bw = [result[\"proposed_bandwidth\"]/1024 for result in communication_results]  # KB\n",
        "\n",
        "    # Créer une figure avec 3 sous-graphiques\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(12, 15))\n",
        "\n",
        "    # Graphique de précision\n",
        "    ax1 = axes[0]\n",
        "    ax1.set_xlabel('Cycle de communication')\n",
        "    ax1.set_ylabel('Précision')\n",
        "    ax1.plot(cycles, proposed_acc, 'o-', color='blue', label='Méthode proposée')\n",
        "    ax1.plot(cycles, fedavg_acc, 's-', color='red', label='FedAvg traditionnel')\n",
        "\n",
        "    # Ajouter les valeurs sur les points\n",
        "    for i, (acc_p, acc_f) in enumerate(zip(proposed_acc, fedavg_acc)):\n",
        "        ax1.text(cycles[i], acc_p + 0.01, f'{acc_p:.3f}', ha='center')\n",
        "        ax1.text(cycles[i], acc_f - 0.02, f'{acc_f:.3f}', ha='center')\n",
        "\n",
        "    ax1.set_title('Comparaison de la précision')\n",
        "    ax1.legend()\n",
        "    ax1.grid(alpha=0.3)\n",
        "\n",
        "    # Graphique de perte\n",
        "    ax2 = axes[1]\n",
        "    ax2.set_xlabel('Cycle de communication')\n",
        "    ax2.set_ylabel('Perte')\n",
        "    ax2.plot(cycles, proposed_loss, 'o-', color='blue', label='Méthode proposée')\n",
        "    ax2.plot(cycles, fedavg_loss, 's-', color='red', label='FedAvg traditionnel')\n",
        "\n",
        "    # Ajouter les valeurs sur les points\n",
        "    for i, (loss_p, loss_f) in enumerate(zip(proposed_loss, fedavg_loss)):\n",
        "        ax2.text(cycles[i], loss_p + 0.02, f'{loss_p:.3f}', ha='center')\n",
        "        ax2.text(cycles[i], loss_f - 0.04, f'{loss_f:.3f}', ha='center')\n",
        "\n",
        "    ax2.set_title('Comparaison de la perte')\n",
        "    ax2.legend()\n",
        "    ax2.grid(alpha=0.3)\n",
        "\n",
        "    # Graphique de la bande passante\n",
        "    ax3 = axes[2]\n",
        "    ax3.set_xlabel('Cycle de communication')\n",
        "    ax3.set_ylabel('Bande passante (KB)')\n",
        "\n",
        "    # Largeur des barres\n",
        "    width = 0.35\n",
        "    x = np.array(cycles)\n",
        "\n",
        "    # Barres\n",
        "    ax3.bar(x - width/2, traditional_bw, width, color='red', label='FedAvg traditionnel')\n",
        "    ax3.bar(x + width/2, proposed_bw, width, color='blue', label='Méthode proposée')\n",
        "\n",
        "    # Ajouter les chiffres sur les barres\n",
        "    for i, (trad, prop) in enumerate(zip(traditional_bw, proposed_bw)):\n",
        "        ax3.text(cycles[i] - width/2, trad + max(traditional_bw)/40, f'{trad:.1f}', ha='center')\n",
        "        ax3.text(cycles[i] + width/2, prop + max(traditional_bw)/40, f'{prop:.1f}', ha='center')\n",
        "\n",
        "    # Ajouter pourcentage d'économie pour les cycles > 0\n",
        "    for i in range(1, n_rounds):\n",
        "        saving = (1 - proposed_bw[i]/traditional_bw[i]) * 100\n",
        "        ax3.text(cycles[i], max(traditional_bw[i], proposed_bw[i]) * 1.1,\n",
        "                 f'Économie: {saving:.1f}%', ha='center', fontweight='bold')\n",
        "\n",
        "    ax3.set_title('Comparaison de la bande passante')\n",
        "    ax3.legend()\n",
        "\n",
        "    # Ajuster la mise en page\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('resultats_comparaison_federated.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Créer un second graphique pour une comparaison directe des performances\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Pour la précision\n",
        "    plt.subplot(1, 2, 1)\n",
        "    x = np.arange(n_rounds)\n",
        "    width = 0.35\n",
        "    plt.bar(x - width/2, proposed_acc, width, label='Méthode proposée', color='blue')\n",
        "    plt.bar(x + width/2, fedavg_acc, width, label='FedAvg traditionnel', color='red')\n",
        "\n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for i, (acc_p, acc_f) in enumerate(zip(proposed_acc, fedavg_acc)):\n",
        "        plt.text(i - width/2, acc_p + 0.01, f'{acc_p:.3f}', ha='center')\n",
        "        plt.text(i + width/2, acc_f + 0.01, f'{acc_f:.3f}', ha='center')\n",
        "\n",
        "    plt.xlabel('Cycle de communication')\n",
        "    plt.ylabel('Précision')\n",
        "    plt.title('Comparaison de la précision')\n",
        "    plt.xticks(x, [f'Cycle {i+1}' for i in range(n_rounds)])\n",
        "    plt.legend()\n",
        "\n",
        "    # Pour la perte\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.bar(x - width/2, proposed_loss, width, label='Méthode proposée', color='blue')\n",
        "    plt.bar(x + width/2, fedavg_loss, width, label='FedAvg traditionnel', color='red')\n",
        "\n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for i, (loss_p, loss_f) in enumerate(zip(proposed_loss, fedavg_loss)):\n",
        "        plt.text(i - width/2, loss_p + 0.02, f'{loss_p:.3f}', ha='center')\n",
        "        plt.text(i + width/2, loss_f + 0.02, f'{loss_f:.3f}', ha='center')\n",
        "\n",
        "    plt.xlabel('Cycle de communication')\n",
        "    plt.ylabel('Perte')\n",
        "    plt.title('Comparaison de la perte')\n",
        "    plt.xticks(x, [f'Cycle {i+1}' for i in range(n_rounds)])\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('resultats_performances_comparaison.png')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_transition_matrix(transition_matrix):\n",
        "    \"\"\"\n",
        "    Visualise la matrice de transition.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(transition_matrix, cmap='viridis', interpolation='none')\n",
        "    plt.colorbar(label='Probabilité de transition')\n",
        "    plt.title('Matrice de transition de Markov')\n",
        "    plt.xlabel('Cluster de destination')\n",
        "    plt.ylabel('Cluster de départ')\n",
        "\n",
        "    # Ajouter les valeurs sur la figure\n",
        "    for i in range(transition_matrix.shape[0]):\n",
        "        for j in range(transition_matrix.shape[1]):\n",
        "            plt.text(j, i, f'{transition_matrix[i, j]:.2f}',\n",
        "                     ha='center', va='center',\n",
        "                     color='white' if transition_matrix[i, j] > 0.5 else 'black')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('matrice_transition_markov.png')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiments ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YmBVDrCpw2CL",
        "outputId": "7a83ccb8-eee8-41eb-a503-a2bf89b26782"
      },
      "outputs": [],
      "source": [
        "def federated_main_two_phase_local(\n",
        "    n_clients=3, n_epochs=50, n_clusters=3,\n",
        "    n_a=64, n_x=10, n_y=5,\n",
        "    batch_size=32, sequence_length=10,\n",
        "    num_transfer_epochs=5, learning_rate=0.01,\n",
        "    n_communication_rounds=6,  # Nombre total de cycles de communication\n",
        "    use_synthetic_data=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Pipeline complet de Federated Learning avec approche en deux phases et clusterisation locale:\n",
        "    Phase 1: Initialisation avec FedAvg + centres de clusters\n",
        "    Phase 2: Mises à jour avec matrices de transition uniquement\n",
        "    Compare aussi avec la méthode traditionnelle FedAvg à chaque cycle\n",
        "    \"\"\"\n",
        "    print(\"=== Lancement du Federated Learning en deux phases avec clusterisation locale ===\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    final_transition_matrices = []\n",
        "    # Code pour les données réelles\n",
        "    print(\"Chargement des données réelles...\")\n",
        "    clients_data = load_real_data(n_clients, 1, n_x, n_y, sequence_length)\n",
        "    # Extraire seulement la première seed pour chaque client\n",
        "    clients_data = [[client_seeds[0]] for client_seeds in clients_data]\n",
        "    # Utilisez les variables locales au lieu de params et passez les dimensions\n",
        "    optimal_n_clusters = visualize_natural_clusters_from_data(clients_data, n_epochs=n_epochs,n_a=n_a,n_x=n_x,n_y=n_y)\n",
        "    # Ajuster le nombre de clusters en fonction de l'analyse\n",
        "    print(f\"Ajustement du nombre de clusters de {n_clusters} à {optimal_n_clusters} basé sur l'analyse\")\n",
        "    n_clusters = optimal_n_clusters  # Mettez à jour la variable locale\n",
        "    \n",
        "    # Créer des données de test\n",
        "    X_test, Y_test = create_test_dataset(clients_data, batch_size//2, n_x, n_y, sequence_length)\n",
        "\n",
        "    # 1. Vérifier et remplacer les NaN dans les données chargées\n",
        "    for client_id, client_data in enumerate(clients_data):\n",
        "        X_c, Y_c = client_data[0]\n",
        "        \n",
        "        # Vérifier pour les NaN\n",
        "        if np.isnan(X_c).any():\n",
        "            print(f\"Client {client_id}: NaN trouvés dans X_c, remplacement par zéros\")\n",
        "            X_c = np.nan_to_num(X_c, nan=0.0)\n",
        "            \n",
        "        if np.isnan(Y_c).any():\n",
        "            print(f\"Client {client_id}: NaN trouvés dans Y_c, remplacement par zéros\")\n",
        "            Y_c = np.nan_to_num(Y_c, nan=0.0)\n",
        "            \n",
        "        # Remplacer dans clients_data\n",
        "        clients_data[client_id][0] = (X_c, Y_c)\n",
        "    \n",
        "    # Même chose pour X_test et Y_test\n",
        "    if np.isnan(X_test).any():\n",
        "        print(\"NaN trouvés dans X_test, remplacement par zéros\")\n",
        "        X_test = np.nan_to_num(X_test, nan=0.0)\n",
        "        \n",
        "    if np.isnan(Y_test).any():\n",
        "        print(\"NaN trouvés dans Y_test, remplacement par zéros\")\n",
        "        Y_test = np.nan_to_num(Y_test, nan=0.0)\n",
        "    # Résultats pour chaque cycle de communication\n",
        "    communication_results = []\n",
        "    global_params = None\n",
        "    fedavg_params = None  # Pour garder une trace des paramètres FedAvg à chaque cycle\n",
        "    client_cluster_centers = []  # Stockage des centres de clusters pour chaque client\n",
        "    param_shapes = None\n",
        "    param_sizes = None\n",
        "\n",
        "    # Pour le calcul de la bande passante\n",
        "    traditional_bandwidth = []\n",
        "    proposed_bandwidth = []\n",
        "\n",
        "    # 2 Itérer sur les cycles de communication\n",
        "    for comm_round in range(n_communication_rounds):\n",
        "        print(f\"\\n=== Cycle de communication {comm_round+1}/{n_communication_rounds} ===\")\n",
        "\n",
        "        # Phase 1: Premier cycle - FedAvg complet et centres de clusters\n",
        "        if comm_round == 0:\n",
        "            print(\"Phase 1: Initialisation avec FedAvg et création des centres de clusters\")\n",
        "\n",
        "            # Entraînement local sur chaque client\n",
        "            parameters_by_client = []\n",
        "            parameters_history_by_client = []\n",
        "\n",
        "            for client_id, client_data in enumerate(clients_data):\n",
        "                # Extraire X et Y du premier élément de client_data\n",
        "                X_c, Y_c = client_data[0]\n",
        "                print(f\"Entraînement du client {client_id+1}/{n_clients}\")\n",
        "\n",
        "                # Plusieurs seeds pour chaque client (pour la clusterisation locale)\n",
        "                client_seeds_history = []\n",
        "                for seed_id in range(5):  # Utiliser 5 seeds différentes par client\n",
        "                    base_seed = client_id * 100 + seed_id\n",
        "                    params, history, _ = train_lstm(X_c, Y_c, n_a, n_x, n_y,\n",
        "                                                num_epochs=n_epochs, seed=base_seed)\n",
        "                    client_seeds_history.append(history)\n",
        "\n",
        "                # Stocker la dernière itération des paramètres pour FedAvg initial\n",
        "                parameters_by_client.append(client_seeds_history[0][-1])\n",
        "                parameters_history_by_client.append(client_seeds_history)\n",
        "\n",
        "            # FedAvg: Moyenne des paramètres des clients\n",
        "            global_params = {}\n",
        "            for key in parameters_by_client[0].keys():\n",
        "                global_params[key] = np.mean([params[key] for params in parameters_by_client], axis=0)\n",
        "\n",
        "\n",
        "            # Garder également ces paramètres comme référence FedAvg\n",
        "            fedavg_params = copy.deepcopy(global_params)\n",
        "\n",
        "            # Réaliser la clusterisation locale au niveau de chaque client\n",
        "            client_results = []\n",
        "\n",
        "            for client_id in range(n_clients):\n",
        "                # Clusteriser les paramètres de ce client\n",
        "                cluster_centers, transition_matrix = cluster_local_by_client_single(\n",
        "                    parameters_history_by_client[client_id], n_clusters=n_clusters\n",
        "                )\n",
        "\n",
        "                # Conserver les formes et tailles des paramètres (identiques pour tous les clients)\n",
        "                if client_id == 0:\n",
        "                    _, param_shapes, param_sizes = flatten_parameters(parameters_history_by_client[0][0][0])\n",
        "\n",
        "                # Stocker les centres et matrices pour chaque client\n",
        "                client_results.append((cluster_centers, transition_matrix))\n",
        "\n",
        "                # Stocker uniquement les centres pour utilisation future\n",
        "                client_cluster_centers.append(cluster_centers)\n",
        "\n",
        "            # Calcul de la bande passante\n",
        "            # Pour la phase 1, les deux approches envoient les paramètres complets\n",
        "            initial_bandwidth = calculate_transmission_size(global_params) * n_clients\n",
        "\n",
        "            # Pour la méthode proposée, on envoie aussi les clusters\n",
        "            clusters_bandwidth = calculate_transmission_size(parameters=None, client_centers=client_results)\n",
        "            proposed_initial_bandwidth = initial_bandwidth + clusters_bandwidth\n",
        "\n",
        "            traditional_bandwidth.append(initial_bandwidth)\n",
        "            proposed_bandwidth.append(proposed_initial_bandwidth)\n",
        "\n",
        "        # Phase 2: Cycles suivants - Mises à jour par matrices de transition uniquement\n",
        "        else:\n",
        "            print(\"Phase 2: Mise à jour avec matrices de transition uniquement\")\n",
        "\n",
        "            # MÉTHODE PROPOSÉE: Matrices de transition uniquement\n",
        "            client_transition_matrices = []\n",
        "\n",
        "            # MÉTHODE TRADITIONNELLE: FedAvg standard\n",
        "            fedavg_parameters_by_client = []\n",
        "\n",
        "            for client_id, client_data in enumerate(clients_data):\n",
        "                # Extraire X et Y\n",
        "                X_c, Y_c = client_data[0]\n",
        "                print(f\"Client {client_id+1}/{n_clients}\")\n",
        "\n",
        "                # 1. Entraînement pour FedAvg traditionnel\n",
        "                params_fedavg, _, _ = train_lstm(X_c, Y_c, n_a, n_x, n_y,\n",
        "                                            num_epochs=n_epochs,\n",
        "                                            seed=client_id+comm_round*100,\n",
        "                                            initial_params=fedavg_params)\n",
        "                fedavg_parameters_by_client.append(params_fedavg)\n",
        "\n",
        "                # 2. Pour la méthode proposée: générer plusieurs séquences d'entraînement\n",
        "                client_seeds_history = []\n",
        "                for seed_id in range(5):  # Utiliser 5 seeds différentes\n",
        "                    base_seed = client_id * 100 + seed_id + comm_round * 1000\n",
        "                    params, history, _ = train_lstm(X_c, Y_c, n_a, n_x, n_y,\n",
        "                                                num_epochs=n_epochs,\n",
        "                                                seed=base_seed,\n",
        "                                                initial_params=global_params)\n",
        "                    client_seeds_history.append(history)\n",
        "\n",
        "\n",
        "                # Transformer les trajectoires d'entraînement en séquences de clusters\n",
        "                # en utilisant les centres de la Phase 1\n",
        "                flat_labels = cluster_trajectories_with_existing_centers(\n",
        "                    client_seeds_history, client_cluster_centers[client_id]\n",
        "                )\n",
        "\n",
        "                # Calculer uniquement la matrice de transition à partir de ces labels\n",
        "                transition_matrix = compute_transition_matrix(flat_labels, n_clusters)\n",
        "\n",
        "                # Stocker la matrice de transition pour ce client\n",
        "                client_transition_matrices.append(transition_matrix)\n",
        "\n",
        "            # Mettre à jour le modèle global en utilisant uniquement les matrices de transition\n",
        "            global_params = update_with_transition_matrices_only(\n",
        "                client_transition_matrices, client_cluster_centers, param_shapes, param_sizes, n_clients\n",
        "            )\n",
        "            final_transition_matrices.extend(client_transition_matrices)\n",
        "\n",
        "            # Mettre à jour le modèle FedAvg traditionnel\n",
        "            fedavg_params = {}\n",
        "            for key in fedavg_parameters_by_client[0].keys():\n",
        "                fedavg_params[key] = np.mean([params[key] for params in fedavg_parameters_by_client], axis=0)\n",
        "\n",
        "            # Calcul de la bande passante\n",
        "            # Pour FedAvg, tous les clients envoient tous leurs paramètres\n",
        "            traditional_size = calculate_transmission_size(fedavg_params) * n_clients\n",
        "            traditional_bandwidth.append(traditional_size)\n",
        "\n",
        "            # Pour la méthode proposée, seulement les matrices de transition\n",
        "            # Taille d'une matrice de transition: n_clusters x n_clusters x 4 octets\n",
        "            proposed_size = n_clusters * n_clusters * 4 * n_clients\n",
        "            proposed_bandwidth.append(proposed_size)\n",
        "\n",
        "        # Évaluation des deux modèles pour ce cycle\n",
        "        proposed_acc, proposed_loss = evaluate_lstm(X_test, Y_test, global_params)\n",
        "        fedavg_acc, fedavg_loss = evaluate_lstm(X_test, Y_test, fedavg_params)\n",
        "\n",
        "        # Stockage et affichage des résultats comme avant\n",
        "        cycle_result = {\n",
        "            \"cycle\": comm_round,\n",
        "            \"proposed_accuracy\": proposed_acc,\n",
        "            \"proposed_loss\": proposed_loss,\n",
        "            \"fedavg_accuracy\": fedavg_acc,\n",
        "            \"fedavg_loss\": fedavg_loss,\n",
        "            \"traditional_bandwidth\": traditional_bandwidth[-1],\n",
        "            \"proposed_bandwidth\": proposed_bandwidth[-1]\n",
        "        }\n",
        "        communication_results.append(cycle_result)\n",
        "\n",
        "        print(f\"Résultats du cycle {comm_round+1}:\")\n",
        "        print(f\"  Méthode proposée - Accuracy: {proposed_acc:.4f}, Loss: {proposed_loss:.4f}\")\n",
        "        print(f\"  FedAvg traditionnel - Accuracy: {fedavg_acc:.4f}, Loss: {fedavg_loss:.4f}\")\n",
        "        print(f\"  Bande passante traditionnelle: {traditional_bandwidth[-1]/1024:.2f} KB\")\n",
        "        print(f\"  Bande passante proposée: {proposed_bandwidth[-1]/1024:.2f} KB\")\n",
        "        if comm_round > 0:\n",
        "            saving = (1 - proposed_bandwidth[-1]/traditional_bandwidth[-1]) * 100\n",
        "            print(f\"  Économie de bande passante: {saving:.2f}%\")\n",
        "\n",
        "    # Calcul du temps total d'exécution et affichage des résultats comme avant\n",
        "    total_time = time.time() - start_time\n",
        "    visualize_transition_matrices_local(final_transition_matrices)\n",
        "    # Visualisation des résultats\n",
        "    visualize_communication_rounds_comparison(communication_results)\n",
        "\n",
        "    # Résumé des résultats\n",
        "    print(\"\\n=== Résumé de l'expérience en deux phases avec clusterisation locale ===\")\n",
        "    print(f\"Nombre de cycles de communication: {n_communication_rounds}\")\n",
        "    print(f\"Temps total d'exécution: {total_time:.2f} secondes\")\n",
        "\n",
        "    # Calcul des économies de bande passante cumulées\n",
        "    total_traditional = sum(traditional_bandwidth)\n",
        "    total_proposed = sum(proposed_bandwidth)\n",
        "    total_saving = (1 - total_proposed/total_traditional) * 100\n",
        "\n",
        "    print(f\"Bande passante traditionnelle totale: {total_traditional/1024:.2f} KB\")\n",
        "    print(f\"Bande passante proposée totale: {total_proposed/1024:.2f} KB\")\n",
        "    print(f\"Économie de bande passante totale: {total_saving:.2f}%\")\n",
        "\n",
        "    return communication_results, global_params, fedavg_params\n",
        "\n",
        "def compare_approaches(results_global, results_local):\n",
        "    \"\"\"\n",
        "    Compare et visualise les résultats des deux approches.\n",
        "\n",
        "    Arguments:\n",
        "    results_global -- résultats de l'approche globale\n",
        "    results_local -- résultats de l'approche locale\n",
        "    \"\"\"\n",
        "    # Fonction utilitaire pour récupérer en toute sécurité les valeurs des dictionnaires\n",
        "    def safe_get(results, key, default=0):\n",
        "        value = results.get(key, default)\n",
        "        if not isinstance(value, (int, float)) or not np.isfinite(value):\n",
        "            return default\n",
        "        return value\n",
        "\n",
        "    print(\"\\n\\n\" + \"=\" * 80)\n",
        "    print(\"COMPARAISON DES APPROCHES\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # 1. Comparaison des performances (précision)\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # Précision\n",
        "    acc_labels = ['Original', 'Compressé', 'Après transfert']\n",
        "    acc_global = [results_global['original_acc'], results_global['compressed_acc'], results_global['transfer_acc']]\n",
        "    acc_local = [results_local['original_acc'], results_local['local_compressed_acc'], results_local['local_transfer_acc']]\n",
        "\n",
        "    x = np.arange(len(acc_labels))\n",
        "    width = 0.35\n",
        "\n",
        "    axes[0, 0].bar(x - width/2, acc_global, width, label='Approche globale')\n",
        "    axes[0, 0].bar(x + width/2, acc_local, width, label='Approche locale')\n",
        "    axes[0, 0].set_ylabel('Précision')\n",
        "    axes[0, 0].set_title('Comparaison des précisions')\n",
        "    axes[0, 0].set_xticks(x)\n",
        "    axes[0, 0].set_xticklabels(acc_labels)\n",
        "    axes[0, 0].legend()\n",
        "\n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for i, v in enumerate(acc_global):\n",
        "        axes[0, 0].text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center')\n",
        "    for i, v in enumerate(acc_local):\n",
        "        axes[0, 0].text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center')\n",
        "\n",
        "    # Perte\n",
        "    loss_labels = ['Original', 'Compressé', 'Après transfert']\n",
        "    loss_global = [results_global['original_loss'], results_global['compressed_loss'], results_global['transfer_loss']]\n",
        "    loss_local = [results_local['original_loss'], results_local['local_compressed_loss'], results_local['local_transfer_loss']]\n",
        "\n",
        "    axes[0, 1].bar(x - width/2, loss_global, width, label='Approche globale')\n",
        "    axes[0, 1].bar(x + width/2, loss_local, width, label='Approche locale')\n",
        "    axes[0, 1].set_ylabel('Perte')\n",
        "    axes[0, 1].set_title('Comparaison des pertes')\n",
        "    axes[0, 1].set_xticks(x)\n",
        "    axes[0, 1].set_xticklabels(loss_labels)\n",
        "    axes[0, 1].legend()\n",
        "\n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for i, v in enumerate(loss_global):\n",
        "        axes[0, 1].text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center')\n",
        "    for i, v in enumerate(loss_local):\n",
        "        axes[0, 1].text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center')\n",
        "\n",
        "    # 2. Comparaison des tailles de transmission\n",
        "    size_labels = ['Original', 'Compressé']\n",
        "    size_global = [results_global['original_size']/1024, results_global['compressed_size']/1024] # en KB\n",
        "    size_local = [results_local['original_size']/1024, results_local['local_compressed_size']/1024] # en KB\n",
        "\n",
        "    axes[1, 0].bar(x[:2] - width/2, size_global, width, label='Approche globale')\n",
        "    axes[1, 0].bar(x[:2] + width/2, size_local, width, label='Approche locale')\n",
        "    axes[1, 0].set_ylabel('Taille (KB)')\n",
        "    axes[1, 0].set_title('Comparaison des tailles de transmission')\n",
        "    axes[1, 0].set_xticks(x[:2])\n",
        "    axes[1, 0].set_xticklabels(size_labels)\n",
        "    axes[1, 0].legend()\n",
        "\n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for i, v in enumerate(size_global):\n",
        "        axes[1, 0].text(i - width/2, v + 0.5, f\"{v:.1f}\", ha='center')\n",
        "    for i, v in enumerate(size_local):\n",
        "        axes[1, 0].text(i + width/2, v + 0.5, f\"{v:.1f}\", ha='center')\n",
        "\n",
        "    # 3. Comparaison des temps d'exécution\n",
        "    time_labels = ['Entraînement', 'Compression', 'Transfer']\n",
        "    time_global = [results_global['local_training_time'], results_global['compression_time'], results_global['transfer_time']]\n",
        "    time_local = [results_local['local_training_time'], results_local['clustering_time'] + results_local['aggregation_time'], results_local['transfer_time']]\n",
        "\n",
        "    axes[1, 1].bar(x[:3] - width/2, time_global, width, label='Approche globale')\n",
        "    axes[1, 1].bar(x[:3] + width/2, time_local, width, label='Approche locale')\n",
        "    axes[1, 1].set_ylabel('Temps (secondes)')\n",
        "    axes[1, 1].set_title('Comparaison des temps d\\'exécution')\n",
        "    axes[1, 1].set_xticks(x[:3])\n",
        "    axes[1, 1].set_xticklabels(time_labels)\n",
        "    axes[1, 1].legend()\n",
        "\n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for i, v in enumerate(time_global):\n",
        "        axes[1, 1].text(i - width/2, v + 0.5, f\"{v:.1f}\", ha='center')\n",
        "    for i, v in enumerate(time_local):\n",
        "        axes[1, 1].text(i + width/2, v + 0.5, f\"{v:.1f}\", ha='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('comparaison_approches.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Tableau récapitulatif\n",
        "    print(\"\\nRÉSUMÉ COMPARATIF :\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Métrique':<30} | {'Approche globale':<20} | {'Approche locale':<20}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Vérification des valeurs avant affichage pour éviter les erreurs de formatage\n",
        "    # dues à des valeurs NaN ou inf qui résulteraient de divisions par zéro\n",
        "\n",
        "    # Taux de compression\n",
        "    global_compression = safe_get(results_global, 'compression_ratio', 0)\n",
        "    local_compression = safe_get(results_local, 'compression_ratio', 0)\n",
        "    if not isinstance(global_compression, (int, float)) or not np.isfinite(global_compression):\n",
        "        global_compression = 0\n",
        "    if not isinstance(local_compression, (int, float)) or not np.isfinite(local_compression):\n",
        "        local_compression = 0\n",
        "    print(f\"{'Taux de compression':<30} | {global_compression:<20.2f} | {local_compression:<20.2f}\")\n",
        "\n",
        "    # Économie de bande passante\n",
        "    global_saving = safe_get(results_global, 'bandwidth_saving', 0)\n",
        "    local_saving = safe_get(results_local, 'bandwidth_saving', 0)\n",
        "    if not isinstance(global_saving, (int, float)) or not np.isfinite(global_saving):\n",
        "        global_saving = 0\n",
        "    if not isinstance(local_saving, (int, float)) or not np.isfinite(local_saving):\n",
        "        local_saving = 0\n",
        "    print(f\"{'Économie de bande passante (%)':<30} | {global_saving:<20.2f} | {local_saving:<20.2f}\")\n",
        "\n",
        "    # Perte relative de performance\n",
        "    global_loss = safe_get(results_global, 'performance_loss', 0)\n",
        "    local_loss = safe_get(results_local, 'performance_loss', 0)\n",
        "    if not isinstance(global_loss, (int, float)) or not np.isfinite(global_loss):\n",
        "        global_loss = 0\n",
        "    if not isinstance(local_loss, (int, float)) or not np.isfinite(local_loss):\n",
        "        local_loss = 0\n",
        "    print(f\"{'Perte relative de perf. (%)':<30} | {global_loss:<20.2f} | {local_loss:<20.2f}\")\n",
        "\n",
        "    # Récupération après transfert\n",
        "    global_recovery = safe_get(results_global, 'performance_recovery', 0)\n",
        "    local_recovery = safe_get(results_local, 'performance_recovery', 0)\n",
        "    if not isinstance(global_recovery, (int, float)) or not np.isfinite(global_recovery):\n",
        "        global_recovery = 0\n",
        "    if not isinstance(local_recovery, (int, float)) or not np.isfinite(local_recovery):\n",
        "        local_recovery = 0\n",
        "    print(f\"{'Récupération après transfert (%)':<30} | {global_recovery:<20.2f} | {local_recovery:<20.2f}\")\n",
        "\n",
        "    # Efficacité computationnelle\n",
        "    global_efficiency = safe_get(results_global, 'computational_efficiency', 0)\n",
        "    local_efficiency = safe_get(results_local, 'computational_efficiency', 0)\n",
        "    if not isinstance(global_efficiency, (int, float)) or not np.isfinite(global_efficiency):\n",
        "        global_efficiency = 0\n",
        "    if not isinstance(local_efficiency, (int, float)) or not np.isfinite(local_efficiency):\n",
        "        local_efficiency = 0\n",
        "    print(f\"{'Efficacité computationnelle':<30} | {global_efficiency:<20.2f} | {local_efficiency:<20.2f}\")\n",
        "\n",
        "    # Temps d'exécution\n",
        "    global_time = safe_get(results_global, 'total_time', 0)\n",
        "    local_time = safe_get(results_local, 'total_time', 0)\n",
        "    if not isinstance(global_time, (int, float)) or not np.isfinite(global_time):\n",
        "        global_time = 0\n",
        "    if not isinstance(local_time, (int, float)) or not np.isfinite(local_time):\n",
        "        local_time = 0\n",
        "    print(f\"{'Temps total d exécution (s)':<30} | {global_time:<20.2f} | {local_time:<20.2f}\")\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Analyse des résultats\n",
        "    print(\"\\nANALYSE DES RÉSULTATS :\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Récupérer les valeurs avec une gestion sécurisée\n",
        "    def safe_compare(local_val, global_val, better_if_higher=True):\n",
        "        # Vérifie si les deux valeurs sont des nombres valides\n",
        "        if not isinstance(local_val, (int, float)) or not np.isfinite(local_val) or \\\n",
        "           not isinstance(global_val, (int, float)) or not np.isfinite(global_val):\n",
        "            return None\n",
        "\n",
        "        # Si les deux valeurs sont 0, on ne peut pas vraiment comparer\n",
        "        if local_val == 0 and global_val == 0:\n",
        "            return None\n",
        "\n",
        "        # Comparaison selon le critère spécifié\n",
        "        if better_if_higher:\n",
        "            return local_val > global_val\n",
        "        else:\n",
        "            return local_val < global_val\n",
        "\n",
        "    # Compression (plus élevé = mieux)\n",
        "    comp_result = safe_compare(\n",
        "        safe_get(results_local, 'compression_ratio'),\n",
        "        safe_get(results_global, 'compression_ratio'),\n",
        "        True\n",
        "    )\n",
        "    if comp_result is not None:\n",
        "        if comp_result:\n",
        "            diff = safe_get(results_local, 'compression_ratio') - safe_get(results_global, 'compression_ratio')\n",
        "            print(f\"✓ L'approche locale offre un meilleur taux de compression (+{diff:.2f}x)\")\n",
        "        else:\n",
        "            diff = safe_get(results_global, 'compression_ratio') - safe_get(results_local, 'compression_ratio')\n",
        "            print(f\"✗ L'approche locale offre un moins bon taux de compression (-{diff:.2f}x)\")\n",
        "    else:\n",
        "        print(\"⚠ Impossible de comparer les taux de compression (valeurs non valides ou nulles)\")\n",
        "\n",
        "    # Performance\n",
        "    perf_result = safe_compare(\n",
        "        safe_get(results_local, 'performance_loss'),\n",
        "        safe_get(results_global, 'performance_loss'),\n",
        "        False\n",
        "    )\n",
        "    if perf_result is not None:\n",
        "        if perf_result:\n",
        "            diff = safe_get(results_global, 'performance_loss') - safe_get(results_local, 'performance_loss')\n",
        "            print(f\"✓ L'approche locale préserve mieux les performances (-{diff:.2f}% de perte)\")\n",
        "        else:\n",
        "            diff = safe_get(results_local, 'performance_loss') - safe_get(results_global, 'performance_loss')\n",
        "            print(f\"✗ L'approche locale préserve moins bien les performances (+{diff:.2f}% de perte)\")\n",
        "    else:\n",
        "        print(\"⚠ Impossible de comparer les pertes de performance (valeurs égales ou non valides)\")\n",
        "\n",
        "    # Récupération\n",
        "    recovery_result = safe_compare(\n",
        "        safe_get(results_local, 'performance_recovery'),\n",
        "        safe_get(results_global, 'performance_recovery'),\n",
        "        True\n",
        "    )\n",
        "    if recovery_result is not None:\n",
        "        if recovery_result:\n",
        "            diff = safe_get(results_local, 'performance_recovery') - safe_get(results_global, 'performance_recovery')\n",
        "            print(f\"✓ L'approche locale montre une meilleure récupération après transfert (+{diff:.2f}%)\")\n",
        "        else:\n",
        "            diff = safe_get(results_global, 'performance_recovery') - safe_get(results_local, 'performance_recovery')\n",
        "            print(f\"✗ L'approche locale montre une moins bonne récupération après transfert (-{diff:.2f}%)\")\n",
        "    else:\n",
        "        print(\"⚠ Impossible de comparer la récupération après transfert (valeurs égales ou non valides)\")\n",
        "\n",
        "    # Efficacité\n",
        "    efficiency_result = safe_compare(\n",
        "        safe_get(results_local, 'computational_efficiency'),\n",
        "        safe_get(results_global, 'computational_efficiency'),\n",
        "        True\n",
        "    )\n",
        "    if efficiency_result is not None:\n",
        "        if efficiency_result:\n",
        "            diff = safe_get(results_local, 'computational_efficiency') - safe_get(results_global, 'computational_efficiency')\n",
        "            print(f\"✓ L'approche locale est plus efficace computationnellement (+{diff:.2f}x)\")\n",
        "        else:\n",
        "            diff = safe_get(results_global, 'computational_efficiency') - safe_get(results_local, 'computational_efficiency')\n",
        "            print(f\"✗ L'approche locale est moins efficace computationnellement (-{diff:.2f}x)\")\n",
        "    else:\n",
        "        print(\"⚠ Impossible de comparer l'efficacité computationnelle (valeurs non valides ou nulles)\")\n",
        "\n",
        "    # Temps d'exécution\n",
        "    time_result = safe_compare(\n",
        "        safe_get(results_local, 'total_time'),\n",
        "        safe_get(results_global, 'total_time'),\n",
        "        False\n",
        "    )\n",
        "    if time_result is not None:\n",
        "        if time_result:\n",
        "            diff = safe_get(results_global, 'total_time') - safe_get(results_local, 'total_time')\n",
        "            print(f\"✓ L'approche locale est plus rapide (-{diff:.2f} secondes)\")\n",
        "        else:\n",
        "            diff = safe_get(results_local, 'total_time') - safe_get(results_global, 'total_time')\n",
        "            print(f\"✗ L'approche locale est plus lente (+{diff:.2f} secondes)\")\n",
        "    else:\n",
        "        print(\"⚠ Impossible de comparer les temps d'exécution (valeurs non valides ou nulles)\")\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Conclusion\n",
        "    advantages_local = 0\n",
        "    advantages_analyzed = 0\n",
        "\n",
        "    # Compression (plus élevé = mieux)\n",
        "    comp_result = safe_compare(\n",
        "        safe_get(results_local, 'compression_ratio'),\n",
        "        safe_get(results_global, 'compression_ratio'),\n",
        "        True\n",
        "    )\n",
        "    if comp_result is not None:\n",
        "        advantages_analyzed += 1\n",
        "        if comp_result:\n",
        "            advantages_local += 1\n",
        "\n",
        "    # Performance loss (plus bas = mieux)\n",
        "    perf_result = safe_compare(\n",
        "        safe_get(results_local, 'performance_loss'),\n",
        "        safe_get(results_global, 'performance_loss'),\n",
        "        False\n",
        "    )\n",
        "    if perf_result is not None:\n",
        "        advantages_analyzed += 1\n",
        "        if perf_result:\n",
        "            advantages_local += 1\n",
        "\n",
        "    # Recovery (plus élevé = mieux)\n",
        "    recovery_result = safe_compare(\n",
        "        safe_get(results_local, 'performance_recovery'),\n",
        "        safe_get(results_global, 'performance_recovery'),\n",
        "        True\n",
        "    )\n",
        "    if recovery_result is not None:\n",
        "        advantages_analyzed += 1\n",
        "        if recovery_result:\n",
        "            advantages_local += 1\n",
        "\n",
        "    # Efficiency (plus élevé = mieux)\n",
        "    efficiency_result = safe_compare(\n",
        "        safe_get(results_local, 'computational_efficiency'),\n",
        "        safe_get(results_global, 'computational_efficiency'),\n",
        "        True\n",
        "    )\n",
        "    if efficiency_result is not None:\n",
        "        advantages_analyzed += 1\n",
        "        if efficiency_result:\n",
        "            advantages_local += 1\n",
        "\n",
        "    # Time (plus bas = mieux)\n",
        "    time_result = safe_compare(\n",
        "        safe_get(results_local, 'total_time'),\n",
        "        safe_get(results_global, 'total_time'),\n",
        "        False\n",
        "    )\n",
        "    if time_result is not None:\n",
        "        advantages_analyzed += 1\n",
        "        if time_result:\n",
        "            advantages_local += 1\n",
        "\n",
        "    print(\"\\nCONCLUSION :\")\n",
        "    if advantages_analyzed == 0:\n",
        "        print(\"⚠ Impossible de conclure (données insuffisantes pour la comparaison)\")\n",
        "    elif advantages_local > advantages_analyzed / 2:\n",
        "        print(f\"✅ L'approche de clusterisation locale est globalement meilleure ({advantages_local}/{advantages_analyzed} avantages)\")\n",
        "    else:\n",
        "        print(f\"❌ L'approche de clusterisation globale reste préférable ({advantages_analyzed-advantages_local}/{advantages_analyzed} avantages)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EXPÉRIENCE FEDERATED LEARNING AVEC APPROCHE EN DEUX PHASES\n",
            "================================================================================\n",
            "=== Lancement du Federated Learning en deux phases avec clusterisation locale ===\n",
            "Chargement des données réelles...\n",
            "Répertoire de travail actuel: c:\\Users\\ikram\\cassiope_42\n",
            "Contenu du répertoire: ['.git', 'Estonia', 'LSTM2.ipynb', 'matrices_transition_locales.png', 'Norway', 'README.md', 'resultats_comparaison_federated.png', 'resultats_performances_comparaison.png', 'Switzerland']\n",
            "Pays Switzerland trouvé avec 12 fichiers CSV\n",
            "Pays Norway trouvé avec 12 fichiers CSV\n",
            "Pays Estonia trouvé avec 12 fichiers CSV\n",
            "Pays disponibles: ['Switzerland', 'Norway', 'Estonia']\n",
            "Chargement des données pour Switzerland (client 1)...\n",
            "Chargé weather_switerland_month1.csv, 744 lignes\n",
            "Chargé weather_switerland_month10.csv, 745 lignes\n",
            "Chargé weather_switerland_month11.csv, 720 lignes\n",
            "Chargé weather_switerland_month12.csv, 744 lignes\n",
            "Chargé weather_switerland_month2.csv, 672 lignes\n",
            "Chargé weather_switerland_month3.csv, 743 lignes\n",
            "Chargé weather_switerland_month4.csv, 720 lignes\n",
            "Chargé weather_switerland_month5.csv, 744 lignes\n",
            "Chargé weather_switerland_month6.csv, 720 lignes\n",
            "Chargé weather_switerland_month7.csv, 744 lignes\n",
            "Chargé weather_switerland_month8.csv, 744 lignes\n",
            "Chargé weather_switerland_month9.csv, 720 lignes\n",
            "Données chargées pour Switzerland: 8760 enregistrements\n",
            "Colonnes disponibles: ['name', 'datetime', 'temp', 'feelslike', 'dew', 'humidity', 'precip', 'precipprob', 'preciptype', 'snow', 'snowdepth', 'windgust', 'windspeed', 'winddir', 'sealevelpressure', 'cloudcover', 'visibility', 'solarradiation', 'solarenergy', 'uvindex', 'severerisk', 'conditions', 'icon', 'stations']\n",
            "Features sélectionnées pour Switzerland: ['temp', 'feelslike', 'humidity', 'windspeed', 'dew', 'precip', 'cloudcover', 'visibility']\n",
            "Chargement des données pour Norway (client 2)...\n",
            "Chargé weather_norway_month1.csv, 744 lignes\n",
            "Chargé weather_norway_month10.csv, 745 lignes\n",
            "Chargé weather_norway_month11.csv, 720 lignes\n",
            "Chargé weather_norway_month12.csv, 744 lignes\n",
            "Chargé weather_norway_month2.csv, 696 lignes\n",
            "Chargé weather_norway_month3.csv, 743 lignes\n",
            "Chargé weather_norway_month4.csv, 720 lignes\n",
            "Chargé weather_norway_month5.csv, 744 lignes\n",
            "Chargé weather_norway_month6.csv, 720 lignes\n",
            "Chargé weather_norway_month7.csv, 744 lignes\n",
            "Chargé weather_norway_month8.csv, 744 lignes\n",
            "Chargé weather_norway_month9.csv, 720 lignes\n",
            "Données chargées pour Norway: 8784 enregistrements\n",
            "Colonnes disponibles: ['name', 'datetime', 'temp', 'feelslike', 'dew', 'humidity', 'precip', 'precipprob', 'preciptype', 'snow', 'snowdepth', 'windgust', 'windspeed', 'winddir', 'sealevelpressure', 'cloudcover', 'visibility', 'solarradiation', 'solarenergy', 'uvindex', 'severerisk', 'conditions', 'icon', 'stations']\n",
            "Features sélectionnées pour Norway: ['temp', 'feelslike', 'humidity', 'windspeed', 'dew', 'precip', 'cloudcover', 'visibility']\n",
            "Chargement des données pour Estonia (client 3)...\n",
            "Chargé weather_estonia_month1.csv, 744 lignes\n",
            "Chargé weather_estonia_month10.csv, 745 lignes\n",
            "Chargé weather_estonia_month11.csv, 720 lignes\n",
            "Chargé weather_estonia_month12.csv, 744 lignes\n",
            "Chargé weather_estonia_month2.csv, 672 lignes\n",
            "Chargé weather_estonia_month3.csv, 743 lignes\n",
            "Chargé weather_estonia_month4.csv, 720 lignes\n",
            "Chargé weather_estonia_month5.csv, 744 lignes\n",
            "Chargé weather_estonia_month6.csv, 720 lignes\n",
            "Chargé weather_estonia_month7.csv, 744 lignes\n",
            "Chargé weather_estonia_month8.csv, 744 lignes\n",
            "Chargé weather_estonia_month9.csv, 720 lignes\n",
            "Données chargées pour Estonia: 8760 enregistrements\n",
            "Colonnes disponibles: ['name', 'datetime', 'temp', 'feelslike', 'dew', 'humidity', 'precip', 'precipprob', 'preciptype', 'snow', 'snowdepth', 'windgust', 'windspeed', 'winddir', 'sealevelpressure', 'cloudcover', 'visibility', 'solarradiation', 'solarenergy', 'uvindex', 'severerisk', 'conditions', 'icon', 'stations']\n",
            "Features sélectionnées pour Estonia: ['temp', 'feelslike', 'humidity', 'windspeed', 'dew', 'precip', 'cloudcover', 'visibility']\n",
            "\n",
            "=== Analyse des clusters naturels dans les données des 3 villes ===\n",
            "Génération des trajectoires d'entraînement...\n",
            "Nombre de clients: 3\n",
            "Dimensions pour client 0: X_c shape=(8, 8751, 10), Y_c shape=(10, 8751, 1)\n",
            "Dimensions pour client 1: X_c shape=(8, 8775, 10), Y_c shape=(10, 8775, 1)\n",
            "Dimensions pour client 2: X_c shape=(8, 8751, 10), Y_c shape=(10, 8751, 1)\n",
            "Entraînement de la ville 1/3...\n",
            "Époque 1/10\n",
            "Loss: nan\n",
            "Époque 2/10\n",
            "Loss: nan\n",
            "Époque 3/10\n",
            "Loss: nan\n",
            "Époque 4/10\n",
            "Loss: nan\n",
            "Époque 5/10\n",
            "Loss: nan\n",
            "Époque 6/10\n",
            "Loss: nan\n",
            "Époque 7/10\n",
            "Loss: nan\n",
            "Époque 8/10\n",
            "Loss: nan\n",
            "Époque 9/10\n",
            "Loss: nan\n",
            "Époque 10/10\n",
            "Loss: nan\n",
            "Époque 1/10\n",
            "Loss: nan\n",
            "Époque 2/10\n",
            "Loss: nan\n",
            "Époque 3/10\n",
            "Loss: nan\n",
            "Époque 4/10\n",
            "Loss: nan\n",
            "Époque 5/10\n",
            "Loss: nan\n",
            "Époque 6/10\n",
            "Loss: nan\n",
            "Époque 7/10\n",
            "Loss: nan\n",
            "Époque 8/10\n",
            "Loss: nan\n",
            "Époque 9/10\n",
            "Loss: nan\n",
            "Époque 10/10\n",
            "Loss: nan\n",
            "Époque 1/10\n",
            "Loss: nan\n",
            "Époque 2/10\n",
            "Loss: nan\n",
            "Époque 3/10\n",
            "Loss: nan\n",
            "Époque 4/10\n",
            "Loss: nan\n",
            "Époque 5/10\n",
            "Loss: nan\n",
            "Époque 6/10\n",
            "Loss: nan\n",
            "Époque 7/10\n",
            "Loss: nan\n",
            "Époque 8/10\n",
            "Loss: nan\n",
            "Époque 9/10\n",
            "Loss: nan\n",
            "Époque 10/10\n",
            "Loss: nan\n",
            "Entraînement de la ville 2/3...\n",
            "Époque 1/10\n",
            "Loss: nan\n",
            "Époque 2/10\n",
            "Loss: nan\n",
            "Époque 3/10\n",
            "Loss: nan\n",
            "Époque 4/10\n",
            "Loss: nan\n",
            "Époque 5/10\n",
            "Loss: nan\n",
            "Époque 6/10\n",
            "Loss: nan\n",
            "Époque 7/10\n",
            "Loss: nan\n",
            "Époque 8/10\n",
            "Loss: nan\n",
            "Époque 9/10\n",
            "Loss: nan\n",
            "Époque 10/10\n",
            "Loss: nan\n",
            "Époque 1/10\n",
            "Loss: nan\n",
            "Époque 2/10\n",
            "Loss: nan\n",
            "Époque 3/10\n",
            "Loss: nan\n",
            "Époque 4/10\n",
            "Loss: nan\n",
            "Époque 5/10\n",
            "Loss: nan\n",
            "Époque 6/10\n",
            "Loss: nan\n",
            "Époque 7/10\n",
            "Loss: nan\n",
            "Époque 8/10\n",
            "Loss: nan\n",
            "Époque 9/10\n",
            "Loss: nan\n",
            "Époque 10/10\n",
            "Loss: nan\n",
            "Époque 1/10\n",
            "Loss: nan\n",
            "Époque 2/10\n",
            "Loss: nan\n",
            "Époque 3/10\n",
            "Loss: nan\n",
            "Époque 4/10\n",
            "Loss: nan\n",
            "Époque 5/10\n",
            "Loss: nan\n",
            "Époque 6/10\n",
            "Loss: nan\n",
            "Époque 7/10\n",
            "Loss: nan\n",
            "Époque 8/10\n",
            "Loss: nan\n",
            "Époque 9/10\n",
            "Loss: nan\n",
            "Époque 10/10\n",
            "Loss: nan\n",
            "Entraînement de la ville 3/3...\n",
            "Époque 1/10\n",
            "Loss: nan\n",
            "Époque 2/10\n",
            "Loss: nan\n",
            "Époque 3/10\n",
            "Loss: nan\n",
            "Époque 4/10\n",
            "Loss: nan\n",
            "Époque 5/10\n",
            "Loss: nan\n",
            "Époque 6/10\n",
            "Loss: nan\n",
            "Époque 7/10\n",
            "Loss: nan\n",
            "Époque 8/10\n",
            "Loss: nan\n",
            "Époque 9/10\n",
            "Loss: nan\n",
            "Époque 10/10\n",
            "Loss: nan\n",
            "Époque 1/10\n",
            "Loss: nan\n",
            "Époque 2/10\n",
            "Loss: nan\n",
            "Époque 3/10\n",
            "Loss: nan\n",
            "Époque 4/10\n",
            "Loss: nan\n",
            "Époque 5/10\n",
            "Loss: nan\n",
            "Époque 6/10\n",
            "Loss: nan\n",
            "Époque 7/10\n",
            "Loss: nan\n",
            "Époque 8/10\n",
            "Loss: nan\n",
            "Époque 9/10\n",
            "Loss: nan\n",
            "Époque 10/10\n",
            "Loss: nan\n",
            "Époque 1/10\n",
            "Loss: nan\n",
            "Époque 2/10\n",
            "Loss: nan\n",
            "Époque 3/10\n",
            "Loss: nan\n",
            "Époque 4/10\n",
            "Loss: nan\n",
            "Époque 5/10\n",
            "Loss: nan\n",
            "Époque 6/10\n",
            "Loss: nan\n",
            "Époque 7/10\n",
            "Loss: nan\n",
            "Époque 8/10\n",
            "Loss: nan\n",
            "Époque 9/10\n",
            "Loss: nan\n",
            "Époque 10/10\n",
            "Loss: nan\n",
            "Préparation des données pour l'analyse de clusters...\n",
            "Données préparées: 90 échantillons de dimension 5578\n",
            "Réduction de dimensionnalité avec PCA...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 114\u001b[39m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results, final_model_proposed, final_model_fedavg\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[43mmain_two_phase\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mmain_two_phase\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     10\u001b[39m params = {\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mn_clients\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3\u001b[39m,            \u001b[38;5;66;03m# Nombre de clients\u001b[39;00m\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mn_epochs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m10\u001b[39m,             \u001b[38;5;66;03m# Nombre d'époques pour l'entraînement local\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33muse_synthetic_data\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Utiliser des données synthétiques\u001b[39;00m\n\u001b[32m     23\u001b[39m }\n\u001b[32m     26\u001b[39m  \u001b[38;5;66;03m# Exécuter l'expérience\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m results, final_model_proposed, final_model_fedavg = \u001b[43mfederated_main_two_phase_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Afficher un tableau récapitulatif des performances\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRÉCAPITULATIF DES PERFORMANCES :\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mfederated_main_two_phase_local\u001b[39m\u001b[34m(n_clients, n_epochs, n_clusters, n_a, n_x, n_y, batch_size, sequence_length, num_transfer_epochs, learning_rate, n_communication_rounds, use_synthetic_data)\u001b[39m\n\u001b[32m     23\u001b[39m clients_data = [[client_seeds[\u001b[32m0\u001b[39m]] \u001b[38;5;28;01mfor\u001b[39;00m client_seeds \u001b[38;5;129;01min\u001b[39;00m clients_data]\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Utilisez les variables locales au lieu de params et passez les dimensions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m optimal_n_clusters = \u001b[43mvisualize_natural_clusters_from_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclients_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_a\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_x\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_y\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Ajuster le nombre de clusters en fonction de l'analyse\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAjustement du nombre de clusters de \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_clusters\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m à \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimal_n_clusters\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m basé sur l\u001b[39m\u001b[33m'\u001b[39m\u001b[33manalyse\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 389\u001b[39m, in \u001b[36mvisualize_natural_clusters_from_data\u001b[39m\u001b[34m(clients_data, n_epochs, n_clusters_range, n_a, n_x, n_y)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRéduction de dimensionnalité avec PCA...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    388\u001b[39m pca = PCA(n_components=\u001b[32m2\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m reduced_data = \u001b[43mpca\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    391\u001b[39m explained_var = np.sum(pca.explained_variance_ratio_)\n\u001b[32m    392\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPCA 2D: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexplained_var\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m de variance expliquée\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\decomposition\\_pca.py:468\u001b[39m, in \u001b[36mPCA.fit_transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    447\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[32m    448\u001b[39m \n\u001b[32m    449\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    466\u001b[39m \u001b[33;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     U, S, _, X, x_is_centered, xp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    470\u001b[39m         U = U[:, : \u001b[38;5;28mself\u001b[39m.n_components_]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\decomposition\\_pca.py:505\u001b[39m, in \u001b[36mPCA._fit\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    495\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    496\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPCA with svd_solver=\u001b[39m\u001b[33m'\u001b[39m\u001b[33marpack\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is not supported for Array API inputs.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    497\u001b[39m     )\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# Validate the data, without ever forcing a copy as any solver that\u001b[39;00m\n\u001b[32m    500\u001b[39m \u001b[38;5;66;03m# supports sparse input data and the `covariance_eigh` solver are\u001b[39;00m\n\u001b[32m    501\u001b[39m \u001b[38;5;66;03m# written in a way to avoid the need for any inplace modification of\u001b[39;00m\n\u001b[32m    502\u001b[39m \u001b[38;5;66;03m# the input data contrary to the other solvers.\u001b[39;00m\n\u001b[32m    503\u001b[39m \u001b[38;5;66;03m# The copy will happen\u001b[39;00m\n\u001b[32m    504\u001b[39m \u001b[38;5;66;03m# later, only if needed, once the solver negotiation below is done.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[38;5;28mself\u001b[39m._fit_svd_solver = \u001b[38;5;28mself\u001b[39m.svd_solver\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_svd_solver == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2942\u001b[39m         out = X, y\n\u001b[32m   2943\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2944\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2945\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2946\u001b[39m     out = _check_y(y, **check_params)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m expected <= 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1103\u001b[39m         % (array.ndim, estimator_name)\n\u001b[32m   1104\u001b[39m     )\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1116\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
            "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ],
      "source": [
        "def main_two_phase():\n",
        "    \"\"\"\n",
        "    Fonction principale pour l'approche en deux phases.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"EXPÉRIENCE FEDERATED LEARNING AVEC APPROCHE EN DEUX PHASES\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Paramètres\n",
        "    params = {\n",
        "        \"n_clients\": 3,            # Nombre de clients\n",
        "        \"n_epochs\": 10,             # Nombre d'époques pour l'entraînement local\n",
        "        \"n_clusters\": 3,           # Nombre de clusters pour la compression\n",
        "        \"n_a\": 32,                 # Dimension cachée\n",
        "        \"n_x\": 8,                  # Dimension d'entrée\n",
        "        \"n_y\": 10,                  # Nombre de classes\n",
        "        \"batch_size\": 32,          # Taille du batch\n",
        "        \"sequence_length\": 10,     # Longueur de séquence\n",
        "        \"num_transfer_epochs\": 3,  # Époques pour le transfer learning\n",
        "        \"learning_rate\": 0.01,     # Taux d'apprentissage\n",
        "        \"n_communication_rounds\": 3, # Nombre de cycles de communication\n",
        "        \"use_synthetic_data\": False  # Utiliser des données synthétiques\n",
        "    }\n",
        "\n",
        "\n",
        "     # Exécuter l'expérience\n",
        "    results, final_model_proposed, final_model_fedavg = federated_main_two_phase_local(**params)\n",
        "\n",
        "\n",
        "    # Afficher un tableau récapitulatif des performances\n",
        "    print(\"\\nRÉCAPITULATIF DES PERFORMANCES :\")\n",
        "    print(\"-\" * 100)\n",
        "    print(\"| Cycle | Proposée Acc | FedAvg Acc | Diff Acc (%) | Proposée Loss | FedAvg Loss | Diff Loss (%) |\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    for result in results:\n",
        "        cycle = result[\"cycle\"] + 1\n",
        "        prop_acc = result[\"proposed_accuracy\"]\n",
        "        fedavg_acc = result[\"fedavg_accuracy\"]\n",
        "        prop_loss = result[\"proposed_loss\"]\n",
        "        fedavg_loss = result[\"fedavg_loss\"]\n",
        "\n",
        "        # Calcul des différences en pourcentage\n",
        "        if fedavg_acc > 0:\n",
        "            acc_diff = ((prop_acc - fedavg_acc) / fedavg_acc) * 100\n",
        "        else:\n",
        "            acc_diff = float('inf')\n",
        "\n",
        "        if fedavg_loss > 0:\n",
        "            loss_diff = ((prop_loss - fedavg_loss) / fedavg_loss) * 100\n",
        "        else:\n",
        "            loss_diff = float('inf')\n",
        "\n",
        "        # Signe pour indiquer si c'est mieux (+) ou moins bien (-)\n",
        "        acc_sign = \"+\" if acc_diff > 0 else \"\"\n",
        "        loss_sign = \"+\" if loss_diff > 0 else \"\"\n",
        "\n",
        "        print(f\"| {cycle:5d} | {prop_acc:11.4f} | {fedavg_acc:9.4f} | {acc_sign}{acc_diff:10.2f} | {prop_loss:12.4f} | {fedavg_loss:10.4f} | {loss_sign}{loss_diff:11.2f} |\")\n",
        "\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    # Afficher un récapitulatif des économies de bande passante\n",
        "    print(\"\\nRÉCAPITULATIF DES ÉCONOMIES DE BANDE PASSANTE :\")\n",
        "    print(\"-\" * 80)\n",
        "    print(\"| Cycle | Traditionnelle (KB) | Proposée (KB) | Économie (%) |\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    total_trad = 0\n",
        "    total_prop = 0\n",
        "\n",
        "    for result in results:\n",
        "        cycle = result[\"cycle\"] + 1\n",
        "        trad_kb = result[\"traditional_bandwidth\"] / 1024\n",
        "        prop_kb = result[\"proposed_bandwidth\"] / 1024\n",
        "\n",
        "        if cycle > 1:  # Phase 2\n",
        "            saving = (1 - prop_kb/trad_kb) * 100\n",
        "            print(f\"| {cycle:5d} | {trad_kb:18.2f} | {prop_kb:13.2f} | {saving:11.2f} |\")\n",
        "        else:  # Phase 1\n",
        "            print(f\"| {cycle:5d} | {trad_kb:18.2f} | {prop_kb:13.2f} | {'N/A':11s} |\")\n",
        "\n",
        "        total_trad += result[\"traditional_bandwidth\"]\n",
        "        total_prop += result[\"proposed_bandwidth\"]\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Économie totale\n",
        "    total_trad_kb = total_trad / 1024\n",
        "    total_prop_kb = total_prop / 1024\n",
        "    total_saving = (1 - total_prop/total_trad) * 100\n",
        "\n",
        "    print(f\"| Total | {total_trad_kb:18.2f} | {total_prop_kb:13.2f} | {total_saving:11.2f} |\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Conclusion\n",
        "    print(\"\\nCONCLUSION:\")\n",
        "    avg_acc_diff = sum([(r[\"proposed_accuracy\"] - r[\"fedavg_accuracy\"]) / r[\"fedavg_accuracy\"] * 100 if r[\"fedavg_accuracy\"] > 0 else 0 for r in results]) / len(results)\n",
        "    avg_loss_diff = sum([(r[\"proposed_loss\"] - r[\"fedavg_loss\"]) / r[\"fedavg_loss\"] * 100 if r[\"fedavg_loss\"] > 0 else 0 for r in results]) / len(results)\n",
        "\n",
        "    print(f\"Différence moyenne de précision: {avg_acc_diff:.2f}%\")\n",
        "    print(f\"Différence moyenne de perte: {avg_loss_diff:.2f}%\")\n",
        "    print(f\"Économie moyenne de bande passante (cycles 2-3): {total_saving:.2f}%\")\n",
        "\n",
        "    if avg_acc_diff > -1 and total_saving > 50:  # Seuils arbitraires pour la conclusion\n",
        "        print(\"VERDICT: La méthode proposée permet d'économiser significativement de la bande passante tout en maintenant des performances comparables à FedAvg.\")\n",
        "    elif avg_acc_diff < -5:\n",
        "        print(\"VERDICT: La méthode proposée économise de la bande passante mais au détriment d'une baisse notable des performances.\")\n",
        "    else:\n",
        "        print(\"VERDICT: Compromis modéré entre économie de bande passante et performances.\")\n",
        "\n",
        "    return results, final_model_proposed, final_model_fedavg\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_two_phase()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
